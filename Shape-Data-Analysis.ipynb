{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, struct\n",
    "import matplotlib as plt\n",
    "from array import array as pyarray\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from pylab import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NMNIST(dataset=\"training\", digits=range(10), path=r'E:\\Users\\Shashi\\OneDrive\\Datasets\\Shapes'):\n",
    "    \n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'Shapes_1_1_Train_Features.dat')\n",
    "        fname_lbl = os.path.join(path, 'Shapes_1_1_Train_Labels.dat')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 'Shapes_1_1_Test_Features.dat')\n",
    "        fname_lbl = os.path.join(path, 'Shapes_1_1_Test_Labels.dat')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    lbl = np.fromfile(flbl, dtype=np.uint8)\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    img = np.fromfile(fimg, dtype=np.uint8)\n",
    "    fimg.close()\n",
    "\n",
    "    size=len(lbl)\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "\n",
    "    rows=28;cols=28;\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectortoimg(v,show=True):\n",
    "    plt.imshow(v.reshape(28, 28),interpolation='None', cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niR9C/Q2H4kPRU0/tVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1732e1810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path= os.path.join(os.path.curdir, 'data')\n",
    "images, labels = load_NMNIST('training', digits=[0,1,2], path=path)\n",
    "\n",
    "flatimages = list()\n",
    "for i in images:\n",
    "    flatimages.append(i.ravel())\n",
    "x_train = asarray(flatimages) # X now contains 60000 feature vectors, each of dimension 784\n",
    "y_train=labels # T contains class labels with 0->Triangle, 1->Square, 2->Pizza\n",
    "vectortoimg(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Checking multiple training vectors by plotting images.\\nBe patient:\")\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "nrows=15\n",
    "ncols=15\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        plt.subplot(nrows, ncols, row*ncols+col + 1)\n",
    "        vectortoimg(x_train[np.random.randint(len(y_train))],show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_NMNIST('testing', digits=[0,1,2], path=path)\n",
    "flatimages = list()\n",
    "for i in images:\n",
    "    flatimages.append(i.ravel())\n",
    "x_test = asarray(flatimages) # X now contains 60000 feature vectors, each of dimension 784\n",
    "y_test = labels # T contains class labels with 0->Triangle, 1->Square, 2->Pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single fully-connected neural layer as encoder and as decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 3 floats -> 3 floats represents 3 classes\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's also create a separate encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Crossentropy loss, and Adadelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.4017 - val_loss: 0.3013\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.3153 - val_loss: 0.2930\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.3051 - val_loss: 0.2811\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2885 - val_loss: 0.2617\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2771 - val_loss: 0.2469\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2689 - val_loss: 0.2324\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2630 - val_loss: 0.2229\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2590 - val_loss: 0.2169\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2555 - val_loss: 0.2113\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2515 - val_loss: 0.2076\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2471 - val_loss: 0.2045\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2422 - val_loss: 0.1993\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2372 - val_loss: 0.1948\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2323 - val_loss: 0.1905\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2275 - val_loss: 0.1865\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2228 - val_loss: 0.1828\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2183 - val_loss: 0.1786\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2139 - val_loss: 0.1752\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2097 - val_loss: 0.1721\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2057 - val_loss: 0.1691\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2019 - val_loss: 0.1659\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1982 - val_loss: 0.1632\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1947 - val_loss: 0.1607\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1913 - val_loss: 0.1581\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1882 - val_loss: 0.1562\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1852 - val_loss: 0.1538\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1823 - val_loss: 0.1519\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1797 - val_loss: 0.1502\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1772 - val_loss: 0.1487\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1749 - val_loss: 0.1471\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1728 - val_loss: 0.1456\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1708 - val_loss: 0.1440\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1689 - val_loss: 0.1431\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1672 - val_loss: 0.1419\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1657 - val_loss: 0.1405\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1642 - val_loss: 0.1398\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1628 - val_loss: 0.1391\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1615 - val_loss: 0.1380\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1604 - val_loss: 0.1371\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1592 - val_loss: 0.1368\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1582 - val_loss: 0.1359\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1572 - val_loss: 0.1354\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1563 - val_loss: 0.1349\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1554 - val_loss: 0.1346\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1546 - val_loss: 0.1336\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1538 - val_loss: 0.1330\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1530 - val_loss: 0.1327\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1523 - val_loss: 0.1325\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1516 - val_loss: 0.1317\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1510 - val_loss: 0.1318\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1504 - val_loss: 0.1311\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1498 - val_loss: 0.1307\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1493 - val_loss: 0.1304\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1487 - val_loss: 0.1294\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1482 - val_loss: 0.1300\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1478 - val_loss: 0.1292\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1473 - val_loss: 0.1293\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1468 - val_loss: 0.1283\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1464 - val_loss: 0.1282\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1460 - val_loss: 0.1281\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1456 - val_loss: 0.1281\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1452 - val_loss: 0.1278\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1448 - val_loss: 0.1273\n",
      "Epoch 64/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1444 - val_loss: 0.1273\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1440 - val_loss: 0.1268\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1436 - val_loss: 0.1267\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1432 - val_loss: 0.1263\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1429 - val_loss: 0.1264\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1425 - val_loss: 0.1261\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1421 - val_loss: 0.1261\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1417 - val_loss: 0.1254\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1414 - val_loss: 0.1254\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1410 - val_loss: 0.1252\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1407 - val_loss: 0.1244\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1403 - val_loss: 0.1248\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1400 - val_loss: 0.1241\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1396 - val_loss: 0.1242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1393 - val_loss: 0.1240\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1390 - val_loss: 0.1237\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1387 - val_loss: 0.1233\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1384 - val_loss: 0.1231\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1381 - val_loss: 0.1232\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1378 - val_loss: 0.1225\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1375 - val_loss: 0.1231\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1373 - val_loss: 0.1221\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1370 - val_loss: 0.1219\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1368 - val_loss: 0.1221\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1365 - val_loss: 0.1222\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1363 - val_loss: 0.1219\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1360 - val_loss: 0.1219\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1358 - val_loss: 0.1214\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1356 - val_loss: 0.1214\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1354 - val_loss: 0.1211\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1352 - val_loss: 0.1207\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1350 - val_loss: 0.1204\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1347 - val_loss: 0.1211\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1346 - val_loss: 0.1210\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1344 - val_loss: 0.1203\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1342 - val_loss: 0.1203\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1340 - val_loss: 0.1200\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1338 - val_loss: 0.1203\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1336 - val_loss: 0.1207\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1335 - val_loss: 0.1201\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1333 - val_loss: 0.1197\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1332 - val_loss: 0.1199\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1330 - val_loss: 0.1198\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1329 - val_loss: 0.1199\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1327 - val_loss: 0.1196\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1326 - val_loss: 0.1194\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1324 - val_loss: 0.1192\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1323 - val_loss: 0.1190\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1322 - val_loss: 0.1192\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1321 - val_loss: 0.1194\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1319 - val_loss: 0.1189\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1318 - val_loss: 0.1188\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1317 - val_loss: 0.1188\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1316 - val_loss: 0.1185\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1315 - val_loss: 0.1185\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1314 - val_loss: 0.1186\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1313 - val_loss: 0.1180\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1312 - val_loss: 0.1181\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1311 - val_loss: 0.1187\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1310 - val_loss: 0.1178\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1309 - val_loss: 0.1179\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1308 - val_loss: 0.1178\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1307 - val_loss: 0.1182\n",
      "Epoch 127/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1307 - val_loss: 0.1181\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1306 - val_loss: 0.1182\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1305 - val_loss: 0.1179\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1304 - val_loss: 0.1181\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1303 - val_loss: 0.1179\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1302 - val_loss: 0.1180\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1302 - val_loss: 0.1178\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1301 - val_loss: 0.1177\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1300 - val_loss: 0.1180\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1300 - val_loss: 0.1179\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1299 - val_loss: 0.1175\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1298 - val_loss: 0.1177\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1298 - val_loss: 0.1176\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1297 - val_loss: 0.1172\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1296 - val_loss: 0.1175\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1296 - val_loss: 0.1179\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1295 - val_loss: 0.1175\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1294 - val_loss: 0.1177\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1294 - val_loss: 0.1176\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1293 - val_loss: 0.1178\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1293 - val_loss: 0.1171\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1292 - val_loss: 0.1177\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1291 - val_loss: 0.1173\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1291 - val_loss: 0.1173\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1290 - val_loss: 0.1168\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1290 - val_loss: 0.1172\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1289 - val_loss: 0.1172\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1289 - val_loss: 0.1173\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1288 - val_loss: 0.1172\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1288 - val_loss: 0.1173\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1287 - val_loss: 0.1173\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1287 - val_loss: 0.1168\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1286 - val_loss: 0.1170\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1286 - val_loss: 0.1169\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1285 - val_loss: 0.1169\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1285 - val_loss: 0.1167\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1284 - val_loss: 0.1173\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1284 - val_loss: 0.1167\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1283 - val_loss: 0.1168\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1283 - val_loss: 0.1169\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1282 - val_loss: 0.1168\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1282 - val_loss: 0.1169\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1281 - val_loss: 0.1169\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1281 - val_loss: 0.1165\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1280 - val_loss: 0.1170\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1280 - val_loss: 0.1170\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1279 - val_loss: 0.1166\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1279 - val_loss: 0.1170\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1278 - val_loss: 0.1163\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1277 - val_loss: 0.1163\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1277 - val_loss: 0.1162\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1276 - val_loss: 0.1165\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1276 - val_loss: 0.1169\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1275 - val_loss: 0.1166\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1275 - val_loss: 0.1164\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1274 - val_loss: 0.1166\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1274 - val_loss: 0.1166\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1273 - val_loss: 0.1164\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1272 - val_loss: 0.1162\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1272 - val_loss: 0.1165\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1271 - val_loss: 0.1163\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1270 - val_loss: 0.1165\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1270 - val_loss: 0.1166\n",
      "Epoch 190/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1269 - val_loss: 0.1161\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1268 - val_loss: 0.1166\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1268 - val_loss: 0.1159\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1267 - val_loss: 0.1161\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1266 - val_loss: 0.1158\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1266 - val_loss: 0.1159\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1265 - val_loss: 0.1158\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1264 - val_loss: 0.1158\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1263 - val_loss: 0.1161\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1263 - val_loss: 0.1162\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1262 - val_loss: 0.1161\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1261 - val_loss: 0.1163\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1261 - val_loss: 0.1157\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1260 - val_loss: 0.1159\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1259 - val_loss: 0.1158\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1259 - val_loss: 0.1158\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1258 - val_loss: 0.1155\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1257 - val_loss: 0.1159\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1257 - val_loss: 0.1159\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1256 - val_loss: 0.1154\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1255 - val_loss: 0.1156\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1255 - val_loss: 0.1160\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1254 - val_loss: 0.1157\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.1254 - val_loss: 0.1154\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1253 - val_loss: 0.1156\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1253 - val_loss: 0.1158\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1252 - val_loss: 0.1155\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1252 - val_loss: 0.1157\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1251 - val_loss: 0.1160\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1251 - val_loss: 0.1158\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1250 - val_loss: 0.1156\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1250 - val_loss: 0.1156\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1250 - val_loss: 0.1158\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1249 - val_loss: 0.1157\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1249 - val_loss: 0.1158\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1248 - val_loss: 0.1154\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1248 - val_loss: 0.1159\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1248 - val_loss: 0.1159\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1247 - val_loss: 0.1156\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1247 - val_loss: 0.1156\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1247 - val_loss: 0.1157\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1246 - val_loss: 0.1159\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1246 - val_loss: 0.1155\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1246 - val_loss: 0.1153\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1245 - val_loss: 0.1154\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1245 - val_loss: 0.1156\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1245 - val_loss: 0.1157\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1244 - val_loss: 0.1152\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1244 - val_loss: 0.1155\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1244 - val_loss: 0.1160\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1243 - val_loss: 0.1158\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1243 - val_loss: 0.1156\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1243 - val_loss: 0.1161\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1243 - val_loss: 0.1159\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1242 - val_loss: 0.1157\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1242 - val_loss: 0.1163\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1242 - val_loss: 0.1158\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1242 - val_loss: 0.1154\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1241 - val_loss: 0.1160\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1241 - val_loss: 0.1155\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1241 - val_loss: 0.1159\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1241 - val_loss: 0.1157\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1240 - val_loss: 0.1157\n",
      "Epoch 253/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1240 - val_loss: 0.1160\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1240 - val_loss: 0.1160\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1240 - val_loss: 0.1160\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1239 - val_loss: 0.1162\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1239 - val_loss: 0.1161\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1239 - val_loss: 0.1160\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1239 - val_loss: 0.1157\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1238 - val_loss: 0.1157\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1238 - val_loss: 0.1159\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1238 - val_loss: 0.1159\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1238 - val_loss: 0.1161\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1238 - val_loss: 0.1158\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1237 - val_loss: 0.1159\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1237 - val_loss: 0.1165\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1237 - val_loss: 0.1160\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1237 - val_loss: 0.1157\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1237 - val_loss: 0.1156\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1236 - val_loss: 0.1160\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1236 - val_loss: 0.1163\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1236 - val_loss: 0.1164\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1236 - val_loss: 0.1161\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1236 - val_loss: 0.1164\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1235 - val_loss: 0.1162\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1235 - val_loss: 0.1163\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1235 - val_loss: 0.1162\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1235 - val_loss: 0.1161\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1235 - val_loss: 0.1165\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1166\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1162\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1164\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1162\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1166\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1234 - val_loss: 0.1163\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1167\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1164\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1164\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1166\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1164\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1166\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1170\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1164\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1167\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1167\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1163\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1232 - val_loss: 0.1168\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1231 - val_loss: 0.1167\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1231 - val_loss: 0.1166\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1231 - val_loss: 0.1169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1730c6190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=300,\n",
    "                batch_size=512,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd172ec2110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    print(D)\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    print(w)\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    print([w[i, j] for i, j in ind])\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[ 877 1073  737]\n",
      " [1593 1092  752]\n",
      " [ 920 1128 1828]]\n",
      "[1073, 1593, 1828]\n",
      "0.4494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "result = np.zeros((len(y_test), 2))\n",
    "for index, item in enumerate(y_test):\n",
    "    result[index] = (y_test[index], np.max(encoded_imgs[index]))\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "y_pred = kmeans.fit_predict((encoded_imgs))\n",
    "print(acc(y_test, y_pred))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3391 - val_loss: 0.2950\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2960 - val_loss: 0.2773\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2780 - val_loss: 0.2562\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2686 - val_loss: 0.2573\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2649 - val_loss: 0.2398\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2628 - val_loss: 0.2360\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2610 - val_loss: 0.2293\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2592 - val_loss: 0.2190\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2573 - val_loss: 0.2187\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2554 - val_loss: 0.2159\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2516 - val_loss: 0.2118\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2460 - val_loss: 0.2036\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2423 - val_loss: 0.2072\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2390 - val_loss: 0.1951\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2321 - val_loss: 0.1981\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2246 - val_loss: 0.1846\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2184 - val_loss: 0.1832\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2138 - val_loss: 0.1805\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2088 - val_loss: 0.1827\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2038 - val_loss: 0.1836\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1986 - val_loss: 0.1811\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1931 - val_loss: 0.1771\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1881 - val_loss: 0.1742\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1833 - val_loss: 0.1730\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1786 - val_loss: 0.1641\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1742 - val_loss: 0.1682\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1693 - val_loss: 0.1614\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1648 - val_loss: 0.1611\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1598 - val_loss: 0.1589\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1554 - val_loss: 0.1593\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1517 - val_loss: 0.1577\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1481 - val_loss: 0.1646\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1446 - val_loss: 0.1581\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1413 - val_loss: 0.1539\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1380 - val_loss: 0.1495\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1350 - val_loss: 0.1588\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1318 - val_loss: 0.1593\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1289 - val_loss: 0.1678\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1261 - val_loss: 0.1585\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1241 - val_loss: 0.1570\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1217 - val_loss: 0.1602\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1197 - val_loss: 0.1637\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1176 - val_loss: 0.1677\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1159 - val_loss: 0.1576\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1146 - val_loss: 0.1595\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1128 - val_loss: 0.1676\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1114 - val_loss: 0.1651\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1104 - val_loss: 0.1660\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1089 - val_loss: 0.1558\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1081 - val_loss: 0.1637\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1071 - val_loss: 0.1612\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1062 - val_loss: 0.1621\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1051 - val_loss: 0.1664\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1045 - val_loss: 0.1732\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1038 - val_loss: 0.1605\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1030 - val_loss: 0.1561\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1024 - val_loss: 0.1666\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1017 - val_loss: 0.1659\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1012 - val_loss: 0.1530\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1004 - val_loss: 0.1546\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0998 - val_loss: 0.1465\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0995 - val_loss: 0.1460\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0988 - val_loss: 0.1590\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0983 - val_loss: 0.1603\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0980 - val_loss: 0.1445\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0973 - val_loss: 0.1586\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0968 - val_loss: 0.1436\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0966 - val_loss: 0.1471\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0961 - val_loss: 0.1592\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0957 - val_loss: 0.1494\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0954 - val_loss: 0.1431\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0950 - val_loss: 0.1398\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0945 - val_loss: 0.1535\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0945 - val_loss: 0.1483\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0940 - val_loss: 0.1475\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0936 - val_loss: 0.1520\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0934 - val_loss: 0.1436\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0931 - val_loss: 0.1440\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0927 - val_loss: 0.1425\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0925 - val_loss: 0.1476\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0923 - val_loss: 0.1446\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0920 - val_loss: 0.1464\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0919 - val_loss: 0.1464\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0915 - val_loss: 0.1478\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0913 - val_loss: 0.1472\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0910 - val_loss: 0.1540\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0909 - val_loss: 0.1438\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0908 - val_loss: 0.1459\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0903 - val_loss: 0.1406\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0901 - val_loss: 0.1436\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0901 - val_loss: 0.1365\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0898 - val_loss: 0.1395\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0896 - val_loss: 0.1421\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0896 - val_loss: 0.1418\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0892 - val_loss: 0.1486\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0890 - val_loss: 0.1422\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0889 - val_loss: 0.1412\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0887 - val_loss: 0.1449\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0887 - val_loss: 0.1406\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0886 - val_loss: 0.1441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0c800cc10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(512, activation='relu')(input_img)\n",
    "encoded = Dense(256, activation='relu')(encoded)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(512, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0c8ba0910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = autoencoder.predict(x_test)\n",
    "decoded_imgs = autoencoder.predict(encoded_imgs)\n",
    "\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[1918 1467 1090]\n",
      " [1472 1092  782]\n",
      " [   0  734 1445]]\n",
      "[1918, 1092, 1445]\n",
      "0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "y_pred = kmeans.fit_predict((encoded_imgs))\n",
    "print(acc(y_test, y_pred))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.3023 - val_loss: 0.1871\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2051 - val_loss: 0.1553\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1735 - val_loss: 0.1430\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1561 - val_loss: 0.1311\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1461 - val_loss: 0.1274\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1390 - val_loss: 0.1232\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1333 - val_loss: 0.1228\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1295 - val_loss: 0.1213\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1266 - val_loss: 0.1209\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1235 - val_loss: 0.1178\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1220 - val_loss: 0.1164\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1204 - val_loss: 0.1155\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1196 - val_loss: 0.1139\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1185 - val_loss: 0.1126\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1179 - val_loss: 0.1130\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1170 - val_loss: 0.1127\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1152 - val_loss: 0.1107\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1139 - val_loss: 0.1077\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1136 - val_loss: 0.1090\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1129 - val_loss: 0.1085\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1121 - val_loss: 0.1075\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1121 - val_loss: 0.1054\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1115 - val_loss: 0.1051\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1106 - val_loss: 0.1044\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1105 - val_loss: 0.1043\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1096 - val_loss: 0.1034\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1092 - val_loss: 0.1039\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1083 - val_loss: 0.1023\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1085 - val_loss: 0.1015\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1080 - val_loss: 0.1003\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1076 - val_loss: 0.1009\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1071 - val_loss: 0.1009\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1064 - val_loss: 0.1005\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1060 - val_loss: 0.1028\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1054 - val_loss: 0.0992\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1055 - val_loss: 0.0996\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1051 - val_loss: 0.0988\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1049 - val_loss: 0.0987\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1046 - val_loss: 0.0987\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1044 - val_loss: 0.0981\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1039 - val_loss: 0.0993\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1039 - val_loss: 0.0980\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1037 - val_loss: 0.0977\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1031 - val_loss: 0.0960\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1028 - val_loss: 0.0980\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1028 - val_loss: 0.0977\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1027 - val_loss: 0.0961\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1027 - val_loss: 0.0965\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1025 - val_loss: 0.0969\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1020 - val_loss: 0.0979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0c8ba05d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAMAAAAAAJABPMQBAAAAAADIgP8H1wsZuwrlWqYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0b6102a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = autoencoder.predict(x_test)\n",
    "decoded_imgs = autoencoder.predict(encoded_imgs)\n",
    "\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = np.reshape(encoded_imgs, (len(x_test), 28*28*1))  # adapt this if using `channels_first` image data format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 5.49 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "y_pred = kmeans.fit_predict((encoded_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[1918 1467 1090]\n",
      " [1472 1092  782]\n",
      " [   0  734 1445]]\n",
      "[1918, 1092, 1445]\n",
      "0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(acc(y_test, y_pred))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28*28*1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28*28*1))  # adapt this if using `channels_first` image data format\n",
    "est = KMeans(n_clusters=3, n_jobs=20)\n",
    "y_pred = est.fit(x_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[ 457  557 1424]\n",
      " [ 278  915  677]\n",
      " [2655 1821 1216]]\n",
      "[1424, 915, 2655]\n",
      "0.4994\n"
     ]
    }
   ],
   "source": [
    "print(acc(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py:893: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=20\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 4.85 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pca = PCA(n_components=3).fit(x_train)\n",
    "est = KMeans(init=pca.components_, n_clusters=3, n_init=20)\n",
    "y_pred = est.fit(x_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[ 457  557 1424]\n",
      " [ 278  915  677]\n",
      " [2655 1821 1216]]\n",
      "[1424, 915, 2655]\n",
      "0.4994\n"
     ]
    }
   ],
   "source": [
    "print(acc(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing PCA would improve performance by 12.18\n"
     ]
    }
   ],
   "source": [
    "print \"Doing PCA would improve performance by\", np.round(((5.5-4.83)/5.5)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3210299770320450676\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7422230528\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 915310821641542645\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28,28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28,28, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_5 to have shape (None, None, None, 3) but got array with shape (60000, 28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-61c9452e65d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_5 to have shape (None, None, None, 3) but got array with shape (60000, 28, 28, 1)"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)\n",
    "batch_size=10\n",
    "u = datagen.flow(x_train, y_train, batch_size=32)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size = batch_size, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(x=x_train)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 for a new dataset\n",
    "\n",
    "# https://github.com/fchollet/deep-learning-models/releases/download/v0.1/\n",
    "# inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
    "# inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "\n",
    "iv3_base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(140, 140, 3))\n",
    "x = iv3_base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "y = Dense(3, activation='softmax')(x)\n",
    "\n",
    "iv3_model = Model(inputs=iv3_base_model.input, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze InceptionV3 convolutional layers \n",
    "for layer in iv3_model.layers[:173]:\n",
    "    layer.trainable = False\n",
    "for layer in iv3_model.layers[173:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "iv3_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as keras_image\n",
    "steps, epochs = 200, 5\n",
    "data_generator = \\\n",
    "keras_image.ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "checkpointer = \\\n",
    "keras.callbacks.ModelCheckpoint(filepath='weights.best.iv3_model.hdf5', verbose=2, save_best_only=True)\n",
    "lr_reduction = \\\n",
    "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, factor=0.5)\n",
    "with tf.device('/gpu:0'):\n",
    "    history = iv3_model.fit_generator(data_generator.flow(x_train, y_train, batch_size=16), \n",
    "                                      steps_per_epoch = steps, epochs = epochs, \n",
    "                                      callbacks=[checkpointer, lr_reduction])\n",
    "                                      #validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "iv3_model.load_weights('weights.best.iv3_model.hdf5')\n",
    "iv3_test_scores = iv3_model.evaluate(x_test7, c_y_test7)\n",
    "print(\"Accuracy: %.2f%%\" % (iv3_test_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "SVG(model_to_dot(inception_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the layers by name\n",
    "for i,layer in enumerate(inception_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import random\n",
    "import cv2\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "# limit the amount of the data\n",
    "# train data\n",
    "ind_train = random.sample(list(range(x_train.shape[0])), 2000)\n",
    "x_train = x_train[ind_train]\n",
    "y_train = y_train[ind_train]\n",
    "\n",
    "# test data\n",
    "ind_test = random.sample(list(range(x_test.shape[0])), 2000)\n",
    "x_test = x_test[ind_test]\n",
    "y_test = y_test[ind_test]\n",
    "\n",
    "def resize_data(data):\n",
    "    data_upscaled = np.zeros((data.shape[0], 140, 140, 3))\n",
    "    for i, img in enumerate(data):\n",
    "        large_img = cv2.resize(img, dsize=(140, 140), interpolation=cv2.INTER_CUBIC)\n",
    "        data_upscaled[i] = large_img\n",
    "\n",
    "    return data_upscaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# resize train and  test data\n",
    "x_train_resized = (x_train)\n",
    "#x_test_resized = resize_data(x_test)\n",
    "\n",
    "# make explained variable hot-encoded\n",
    "y_train_hot_encoded = to_categorical(y_train)\n",
    "#y_test_hot_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# get layers and add average pooling layer\n",
    "x = inc_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# add fully-connected layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# add output layer\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inc_model.input, outputs=predictions)\n",
    "\n",
    "# freeze pre-trained model area's layer\n",
    "for layer in inc_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# update the weight that are added\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x_train, y_train_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose the layers which are updated by training\n",
    "layer_num = len(model.layers)\n",
    "for layer in model.layers[:279]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[279:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# training\n",
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train_resized, y_train_hot_encoded, batch_size=128, epochs=5, shuffle=True,  validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hot_encoded = to_categorical(y_test)\n",
    "x_test = np.reshape(to_rgb5(x_test), (len(x_test), 28,28, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv3_test_scores = model.evaluate(x_test, y_train_hot_encoded)\n",
    "print(\"Accuracy: %.2f%%\" % (iv3_test_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectortoimg(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1].shape\n",
    "Orig = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), 28,28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb5(im):\n",
    "    im.resize((im.shape[0], im.shape[1], 1))\n",
    "    return np.repeat(im.astype(np.uint8), 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rgb5(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(to_rgb5(x_train), (len(x_train), 28,28, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectortoimg_3(v,show=True):\n",
    "    plt.imshow(v.reshape(28, 28,3),interpolation='None', cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectortoimg_3(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking multiple training vectors by plotting images.\\nBe patient:\")\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "nrows=15\n",
    "ncols=15\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        plt.subplot(nrows, ncols, row*ncols+col + 1)\n",
    "        vectortoimg_3(x_train[np.random.randint(len(y_train))],show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(data):\n",
    "    import cv2\n",
    "    data_upscaled = np.zeros((data.shape[0], 140, 140, 3))\n",
    "    for i, img in enumerate(data):\n",
    "        large_img = cv2.resize(img, dsize=(140, 140), interpolation=cv2.INTER_CUBIC)\n",
    "        data_upscaled[i] = large_img\n",
    "\n",
    "    return data_upscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp = resize_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectortoimg_3(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
