{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from proj_utils import *\n",
    "import os, struct\n",
    "import matplotlib as plt\n",
    "from array import array as pyarray\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.curdir, 'data')\n",
    "x_train, y_train = load_NMNIST('training', digits=[0,1,2], path=path)\n",
    "x_test, y_test = load_NMNIST('testing', digits=[0,1,2], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niR9C/Q2H4kPRU0/tVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb667feff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectortoimg(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking multiple training vectors by plotting images.\n",
      "Be patient:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAJCCAYAAAALCSnoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsfT+020Z2/vXv7DnwVrCr0NtglYba\naqE0gSvIKWI6DZVtTKdZuFoqzUJbbOgmeG5CJwXlrahtlkqzlJulVFFOQ7mi3AR0GsoNZDd4rqBV\nhafq/oqnOxqA+DMzAPn45PnOwTn2E/98vDNz55t778y8gYigoaGhoaGhoaEhj/930QQ0NDQ0NDQ0\nNC4rtJDS0NDQ0NDQ0FCEFlIaGhoaGhoaGorQQkpDQ0NDQ0NDQxFaSGloaGhoaGhoKEILKQ0NDQ0N\nDQ0NRWghpaGhoaGhoaGhCC2kNDQ0NDQ0NDQUoYWUhoaGhoaGhoYifnLg77vIY9TfqPi3Y+R1jJwA\njpPXMXIC0LyKoNtQHJeN1zFyAjhOXsfICeA15PXkyRMAALh69arqR1TxAgAdkdLQyGCz2Vw0BQ0N\nDQ2NFvD06VP46KOP4KOPPtrr92ghpXGhePr06UVTYNhsNnBycnLRNCrx8OHDi6agIYBnz55dNIWj\nx6NHj+DRo0dwdnZ20VQ0XlP893//N2w2G9hsNnD//v29fY8WUhoXgrOzM/jkk0/gF7/4xVFMOmdn\nZ/Dxxx/D999/f9FUCnF2dgY3b96EDz74QE88R46zszO4du0aPHnyhKUVNABOT0/h5OQErl27Bm+8\n8Qa899578N5778FPf/pTeOutt+Ctt96CGzduwN27d3Uf12gF9+7dY//96aef7u+LEPGQz0XiYLy2\n2y0uFoumvC4Se7XVer3GbreLcJ73xtls1pRXY4zHYwQA7HQ6sm/de7+Koght22b2CsPw4LzW6zWu\nViv2bLdblY+p4nWRaNVW8/kcAQAdx0HHcTBN06Pg1SKkOU0mEzQMg/VhAEDXddFxnMzf6Ol0OjiZ\nTGRs99rY6gC4bLZS9ln5fiUxL4vyAkTUESkNDQ0NDQ0NDWWIqK0Wn4vEQXjN53M0TRM9z2vK6yLR\nuq3SNMXRaISj0YitDkzTRADAXq/XlJcyoijCKIoyq2VJ7LVfLRYLZid65vP5QXitVivs9/uFEQN4\nGTUYDoey0anXvr/z0UMAwCAIjoJXi5Di5Hkes0W/38flcrnzmjiOMY5jnM1mGft1u90LicC2iMvE\n6bXi5fs+AgBaloWWZSEAoG3bbfMCRNRCClvilaZpxmFoIfUK+VQeAOBoNMIoihAA0DAMTJKkCS9l\nuK6LrutmuEVRJPMRe+lXedHZ7/dxOBwy2+2TV5Ik2Ov1MjaxLIvZynXdHXE3HA4vrA1bQGttuFgs\nMosEetbr9YXyahlCnMbjMUuZG4YhlVZZLpfMZxiGUSi+BDldClvVQXBsNeV0rLZS4tXpdNjYW6/X\n7P8V0nu12uZQAmovjRTHMa5WKwyCAAeDATqOg0mSlHW6vfAKwxDDMNwRCq7rin6EEqf5fI7L5bJJ\n/YUKJylbURSKX112u93MpEIiRrBOqtX2m81mmegYteFqtZL5mFb7VRzHO8JuMpkg4qvam36/vzde\ncRyz1ZtpmjgejzGO48LXhmGYWTx0u10REdrYVkEQyLaRKifpNiQBOh6PMQgCDIKA2UZhrCrxStMU\n5/M5ep6HruuyaKtgZKcJL4Y4jtEwDPbdKrUp/OJUQExdNnEgjDAM0TAMtCwLfd9nwmAPnI7VVtK8\nlsslG3eEyWSiGpWq1TaHFFHKjbRarXA6neJ0OkXf9wtXxALipXVeVEBJzsK2bTY5O44j+jFKnKbT\nKfvdjuPgaDRqU1g1ttV2u82Iy9FohGma7vCj3yGY3mtVsNAKhYQcpbEEU2d1nKR58aumTqeDnU4n\nIxi22+2Oc2iTV5qmrPjXtu1SAZXHdrtl6RjLsurEVGNb8ULP8zxcLBa4WCwuvKg7DEM26cdxzPo7\n2WY4HO6NF31XEASFvlFCgDfhxcAvoAaDQaMvIzFqGAZbuEpwEmpDEr2iT6/XQ9d1RX5bo37FL2yK\nUuvU/yXR+lzYElrjRQJ8PB6zv6VpqhqVqtU2byBicfHUflD5ZbRdeLPZwNdffw1Pnz6Fb7/9tvT1\npmmCbdvw7bffwg8//ADj8RhGo1HZy1s7NfXZs2fwL//yL5kzfXzfh/F4DKenp/C3f/u3YFkWfPfd\ndyIfp3Ty7enpKVy5cgVevHix82+u6wIAwPXr19kjiUa2+uyzz+Dk5ARevHgB3W4X7t69C47jFL72\n9PQUfvazn4FhGHB6egpvv/22Ci/pTvzRRx/BvXv3oNfrAQDAcrmEmzdvwp07d2AymcCtW7dEP6qV\nfnX79m343e9+BwAAjuPAX/7yFwAAeOeddzKve/PNN+HFixeQJImqrUp5EQfLsiAMw7rPz+DZs2fw\nD//wD7DZbMC2bViv14yvIC9hW5EN8jAMA/75n/8Z3n//fej3+zL8W2nDGzduwIMHD8D3fbh9+zb7\n+5MnT8C2bXjx4gUsl0vW59ri9fTpU/jVr34FAK8OlHUcB/r9PjiOA5ZlwS9+8Qt48eIFhGEItm2L\nfr8sL8bp5z//OTtKZL1el45/UdDYtCwLAM5tmutbym14dnYGP/3pT5V4Cfh55f5+dnYG7733Hjx+\n/Bhc14XPPvsMvvjiC1gulwAAmblRsu/X2ur27dvw/PnzOoq1+P7773fOCnzrrbcAAIrOdWplHJ6d\nncFbb70FL168gCiK4MqVK+zfyMfZtg1hGIp+ZO3J5oeMRmVU5Ww2Q8/zWPgZSgpa4eXqeDAY4GAw\nYGF9frVMKlNxJSyldlerFfs+0zRxuVxmQs5JkrDVgiCUOVEaYTAYoO/7O0Wu/OO6Lo7HY9GQsJKt\nttttZjszRaHqQO0vEAlqZbVCYV/DMFixOeKrIxB835f5uEb9KkmSTEF33XdTGwu0ozSvBjUEiHj+\nW2j1PJlMWEpSkJcwyFbb7RaDIEDbtgv7fq/Xw+l0KhJZa+wb4jjORKPyoLRCp9MRjvSJ8ErTNBP5\n7Xa7hWlPKrxtKSpVyYlsYZommqbZxvdlInsl40S5DalW0zRNoWjUbDZj7SmQeVDuV4PBgEV5i8pV\nttstjsfjwr7vui7L4hTMi7W2CsOwNLLZ9PF9v8zPtTJHUwlEUYZKMSpVq20OKaIyxthut2ygkYE7\nnQ66rouj0Qhns5nQpE9nRQikOxo3EoWYqZHKHCK9RhDKnCiNyHeYJElYqqNMXBmGUVewLG2rxWLB\nUpz5Oqg6UHpv32FyxPOBxE/2PMiekqkI5X7F19aZpik0sMm5TqfT1njR2VCC46gSVGxNqUkJXsIo\nG19RFOFkMilcmFHavaQWr7FvIKFS1XeIl4SYqeVFKTQSk2VFyVSzBABV6bGmvBARWV+ic7TaAqVO\nSwSrchtKpswR8dVvFKiFVeJEizrTNIV2xVb1feofnM8QshWJKZqjad6QSYGSbyFxQ+1WMnc2HoeI\nrwIMZT5SoVaqVtscQjyVGoMGNA1wlW3CJG4EdjIpN1Icx5lIC593LQJ1vH3vYkrTlNmuapVL4oo6\nmGEYdYNT2lbkWDqdjnStCr+ar3lv40FGE16Rg+cnAAko9av5fJ6prRM9QkAiaibMi6JHAFK7TUvB\nR0gKCoQbtyH1lSrEcYzT6ZTZ2DTNvdXX5EVK1evINzTcXMF40UKJJqwq8FGphpGpSk68yJDYdCME\n4l7g85XbUEIUMUhs/FDyDRXjpxZ0jAS/7Z/qy2o4lc7R/FE1KrWINE/X2KvROEQ8n+uobrls/lWI\nStVqm0MJqEpjLJdL5ogEVtoZ8I6kBkqNxJ/jY1mWUKSFOq/gNvpGHYciFHXiLkkS1nnqXlvBqZIX\nfb7KqdeC6b1GtuKLgYsmPFqZWpYlQ13YVlQQTMcYkHCRcUwU8REozhfmxe8uq1rMUDFzEASViwQ+\nclvweY0dpUgbpWnKFg4koirQyIGTuBURJxT1pLRyDWp58b6m7vN4wUdChNqTNvHwTz6qwbVlJSc+\nwiMS5dlut0wI8rsdh8Nh5ZEbuT6g3IaU6pc4z461o8DCQ4rTer1mbVSSGhcC+Ql6coJB2lZNxBQv\nXPY5DhFfZTfqxqJkVKpW2xxKQNUag9+KLrpris9tC6DRhDcYDITP8qDoVcOD5IRAA6auQ9AuBsFo\ni1Ibkr1UIouC6T1lW/E1FmX80jSVTctWccp8SBRFLNVBE6nsogHx1SQlUIMn3IaiQooXSFWLCt6J\nFzi0Rv2dxnyVkEqShAkBARFVxamWFz9JiKazafHToL6G8aI+PZ/PhfwmRaVUHs7mtbbiBVud3yQh\nqvKoRFnykBBFDDQRN4gMZ0BCmPqSwg5PBj7LU7JwVrKVqpgi3y4gWpTbkCBab8uXeAhEpWq1jb4i\nRkNDQ0NDQ0NDFSJqq8WnEqTyBU+yZUpXcCUhxIvOP+LrPBzHkdlpw1Sx4KGBjRR4mqZslVCWUuN3\nqQmm3ZTasEnBcr5OqmS1o2wrWvXWHYxItmxzZ9VyucykJURTxHUca1b6wm1Iu3ugIiJIbZt/itLE\n/GsL6k4a9fe6iFSSJJkzrfbZ3xHVDvmTSLPX8uK/37bt2igBHfjquu5OYfBischcTM2nC3PjotZW\nfGqwLjqQLyLnd8fxfGrGpHIbSs4jiIhCEdwaXgwULad+q1qHhJg9d4p2xUtwqrWVSlSKflfDyE8t\n+AyVCC/RbE4NL0A8otQegXahiITkJbbNowwvCpPzO94Mw8DhcChU90RptIa8hEHfVzSoJWuj6jjV\n8mqrTqrEdkqc+Lv06gQMCWgJoVNpKz4dRjvZVGzDg9KDbZ30TAWlxDEPvg/R1TX8gYv5Hax8sWyB\nMGvU36uEVF5ESVz1o9zfqb/IHhlBYrPpIZP54w8Gg0Hrtx3wi7WX4r3WVny6TqR2jC+MVtxRqNyG\nEqKIgVKkAnVMtZz4408EU9GF4A/UdV1XZUEqNA5lxBQt4gU3ITXiRX1ORhALirxabXMoASXlLEkY\nmKZZ6gxFqvMljFGKootbB4NBZWenWqFDXXtCTrlocpGsjarjVMtLtU5qPB4zsaOwkqoECTTLsmoj\nnVSg3MYZI/P5vPRUYtu2cTQaKUWmyMY1Tly6DctEAfX/fHRpuVzunKnG8yvpB436Owk+ngttp1YU\nUVWcKnlJrGgLQXaiCHjBRCPEK79N3XGcxmKdB01QnA+p5ZS/IqZOHNCYl1zw8VD2WSpCirgK+PhK\nTvyiI+8fgiCQase6c6cEOEmNQxExRb5DsHC+ES/yX6I7HeM4Zu1YM4Zrtc0hRZSUOKAJzbKswpBu\n1aFbJWjEa7vdZu4Vo+/OH8iJ+GpgCjqExrZCxMKC1/zBkxJQtpVsei9/iKfneSr3JZYijuOdexA7\nnQ76vl/o3KmNJQrBa2213W5xMpmwrdt8ISiJkMFggLPZTCilSOmcmoJU6Takz+UndUp7mKZZyC1/\nN6Dv+5mrdwps3Ki/57eqR1GUiWYI3vcnyqmSFwm3fF9JkiSTkprP5yxdRQcQu66bsRPZTpUXRbX4\n7e6+70uVJOTBp3xzE5QQJzp4UWQhR2ffSfp0Hso+q0wU8W2Yf6jPN0hXMSRJws44GwwGO/6B7tir\nEqOS5061MkcXiSleUOWvTBKAMq+qiDoPOm+r6HzFijsMa7XNIUWUVCPl7/3KT67U+SW2iLbCK45j\n9H1/ZysuHfiH+GpCElzhNOaE+CrUPBwOma3IUSvsDlO2lUjNFoGPQnU6nSY3vNdiu93iaDTaiRBR\nNGA8HmMURTLnktVxquS1XC7R9/0dkcdHq8oiViSQayanRuKg3+/jdrsVvnQ2v/Oq4qyiRv2dF1Ik\noni7ie6sFeRUyosOAqZ24BcDqg/9pia8kiTJRASpLQRPeGdnEOUnmpwvE+JEEyv5oaodbuSzqL8p\ntKOyz8ovkGWeBsfulCJNU1wsFuh53o7Y7nQ6OBwOcTgcsu/md8kKRmNamQsRd8UUH52iOUnipghl\nXlRqkP+uMAzZIqbI1/b7fZEFbK22OaSIkm6kJEnYj6dVHDUSNd4h6iDKuE0mE1b3wq8eKJrWsAhe\nCuTYO50Ou07nIlZ3iNU1W4jlUagGvKSwWq1wOBwWXoFAf5PIszfuV1EU4XQ6xcFgUMjJMAw2GUZR\nlLmCo21edOMAfS+JcxGs12u0LEu17kcIfMSTRFTdqd4CkLZVPt2fn+z48488z2POfD6fs6jGPmtG\nttst8wH5vpQ/L6osQkaRvqaHqvKbD+pSYZJ1ryKcam2VPzOrykb5p0H0Rxir1Qp93y8sEzBNU+Xc\nqVbnwvx1Mr1eL1OXKpGeVObFZ2Sq7GUYBg4GA5zP5zL+olbbHEI8NWqk/Kqz3++r7g5rlVces9ls\nR/EeUkgh4k7HUUjp1XES4kURk3z7jMdjlSiUCC9lUGF7PqR+qJRxEdbrNYtIFYWg+X62j+sW1us1\nc4x1uxzzoFP0K9DIVvmDBoui1QoQthVFWfJXYDS8cqUxrzLQCe+DwWBHKBU9JNrbPhiXPyewquRB\npWC4hlMr/r0BWuVE9+vl79iTPHeqdVvl6/Sor0meoK/Eixfq+T5umiarv1O9Q7SGFyBeAiGFuHsv\nH7+DSAIHGWiLxYKtcAQn49Y48TvEAJRSenWchHjl03v5CJRkFEqEVyugWgXXdWVON997v6J0S1Go\nn2pL2uZFB4juQSA0shU/Ifd6vaYCqo7TsU7Cyrzy9Vv800LkoBL5nXxFCwDRWhcJTsfahq0giiKc\nzWayOzX3Yqv8dTISqcZGvPKpWUp/Ch4/1JQXIF4SIYWIrBCMjxpI7ng66EALw/CgxeaIr069buGO\nq8a24ncL8hEohSiUCK/W0VLYdy+gvuU4DludHgMvQTTiREKqyVk7EpyO1VbHyqsW/JVbdKRMXsBR\nZP2Y/bsELhOnVniRmFLYzSrNixbtIgX5DVCrbfTJ5hoaGhoaGhoaingDEQ/5fY2/7OHDh3Djxg14\n++234fT0VOatb1T820GNkEMZLyVO7777Lvz5z38GAIArV660zQlAkNfDhw/hgw8+YP/veR5MJhMA\nAHj77bfb5nWM7QegeRWhURvevXsXvvjiC1gsFvDmm2/umxPAcdoK4Dh5CXE6PT2FmzdvwoMHD9jf\nut0uAJz7r/V6Dd9++y0EQQAnJydNOQnz2hN+lD5rs9nAkydPYDAYyLxNmtfp6Sk8f/4crl69KsVP\nElW8zl9w2YQUwLkz/eqrr2A2m8m87Ucx0J49e9ZEqBAa2+rs7Aw6nQ789Kc/hdlsBr1erymnKl7H\n2H4AmlcRGrXhkydP4Oc//3mbIgrg8tkK4Dh5SXHabDbwhz/8AebzObx48WLn3x3HgfV63ZSTNK+W\noX2WOC4jr/MXXEYhBXA+UUs608vWSMfICUCC17179+D9999vQ9gRXltb7QGXjdcxcgLQvIrQehs+\nfvwYAM6F8nfffcf+riNSe8FlsxXA8fI6f8FlFVIKuGyNdIycAI6T1zFyAtC8iqDbUByXjdcxcgI4\nTl7HyAlA8yrC0QkpDQ0NDQ0NDY3XBnrXnoaGhoaGhoaGIrSQ0tDQ0NDQ0NBQhBZSGhoaGhoaGhqK\n0EJKQ0NDQ0NDQ0MRWkhpaGhoaGhoaCjiJwf+vmPdwniMvI6RE8Bx8jpGTgCaVxF0G4rjsvE6Rk4A\nx8nrGDkBaF5FqD3+QEekNPaGk5MTePLkyUXT0NDQ0NDQ2Bu0kJLEw4cPL5rCpcDDhw/h008/Bdu2\n4eTkBM7Ozi6akoaGxpHi2bNn8OzZs4umUYvHjx/Dxx9/rP2ZRgb6ZPNzCPO6evUqXLlyBf785z+3\ndfXJaxn6vXbtGmw2G/b/3W4X7t69CwDnd2i1zOtS22pPUOJ1dnYG9+/fhy+//BI2m02mDQFetZ1t\n2/D3f//30O/3ZceBbkNxXDZeSpzu3bsHw+EQAAA+//xz8DyvTU4ALdjq0aNHcHJyAl999RUAACwW\nC7hx40YTXsfYfgA/Al6PHj2CMAzh//7v/2Cz2YBpmvDo0SNVXgCgI1IaGhoaGhoaGupAxEM+UkiS\nBFerFY7HY1ytVrharTAMQ9mPITTmlaYpwrkyxk6ng+v1WpWLCK+LRCNbLRYLBAA0TRMXiwXats3s\nBgA4HA4xSZI2eSkjjmOM4xiHw6HqR7TW30WwXC73xms2m2Gn08m0lcgzGAxwtVo15XWROGgbSuCy\n8ZJCmqbo+/5OfwqCoE1OyrZaLBboOA46jrPD0bbtprwuEpetXynxWq/XOJ1OcTgc4nA4LGxHwbas\n1TZHmdp78uQJfPrpp3Dv3r3Cf7csCzzPg9/+9rcyaYXGYcPNZgPXrl1j/28YBozHYwAAuHXrligP\nUV6M0+npKfzsZz9T/fxCOI4D6/VallOGVxkorTeZTJhdPvvsM3aT+4sXL6DT6cBsNoNerydDu/Uw\n+UcffQQA5+mFMAzBtm3ZjzhIOPrJkydw69Yt2G638N1334m8RYjX2dkZDAYDAAB48OABAJynYX/z\nm9/AtWvX4Pr165k3Ugg8DEP4n//5n0zNYK/Xgz/96U/wzjvvqPB67VMKPM7OzuDNN9+se9llS8EI\nczo9PYVf/epX8PjxYwAAls6j9P9wOITpdNoGJylexOEPf/hDJq1tmib4vg+/+c1v4O/+7u/ghx9+\nEE3vSdnq9PSU+aQyXLlyBQDO58Eq/Pa3vwUAKJojW7HV6ekp/PGPf4Qvv/yStSPAedrftm349a9/\nveM/aqDM6/Hjx7DZbOCbb76BzWaT4ZNHt9uFd999FyzLgk8//RQsy6rzqbWpvUNGo4RU5Ww2Q8Mw\nmFp0HAc9z0PXddF1XbQsKxMVamElLKx25/M5W4GPRqOMqu33+3uNspimiaZpSkcLyh7TNFU41dpq\nMpmwtknTNPNv2+0Wt9st9no9xqPX62Ecx2KWankVRe1Jj+/7Kh+z19VdmqY4Ho/ZmLAsq1VefFuY\nponz+VyKXxRFGAQB65sCn9GKrWaz2c4K0zAMdF0XJ5OJ7FhUbsPZbIa2bUt9X5qm2Ov1RPrbZYsc\nCGG5XLL+ko/s8/5/MBjs+BAFTsK8ZrPZzvwyHo9xPB5n2pd8nOu6TXgVIoqi1nx8FEUYRZEMJylb\n8fN02dOSfy/lxWeJ8o9lWTgYDFgbrlarnf5Er23ACxDxuIQUP7ENh8PSBlgulxknOpvN6j66zhhC\nCIIAAQBHoxEinod+eYHT7XZVUo9CnPgQc4P0JiIiS98kSVI2ASjZKk1T9tmTyaSSw2w2y0y+da+v\n4SWNOI530liGYcgM+jpOjSa79XqN6/Uau91uhmO3222N13g8Zg7HsizcbrfKfJMkQc/zMqK0RCg0\nslUYhjs2KXoOscharVZsMhEVUySiiGdNv99L32oBypzIh5IQKRpvy+WS2bXX64mKVGWfNZlMMr7A\nsiycTCalIo73cwK+WIoTCSnqv0XPbDbD2WyGQRAUPuRX9yWk+DYcDAa4WCwybRSGYYaHZVllPFrh\nRd/T6/UwCAJcrVbCCxt+LlTkBYhHJKT4iU1wUmVRIcMwROqVGjulwWCAAJBZcVNnpTogwzBwOp2K\nfmQVr53vLvp+FZAgo8laglOlrWilJlo/EMdxZvJ1HKduMm9tUqHJrNfrseiAYlSq1ckuL0hIPJHo\nEVwF1/JK05Q5oMVigYvFQoXuDvIr1QJ7KtsqDMPMomU2m2Um4jRNcT6fs/5tGIZoTZl0G/KTqaiY\n4kUU/Q7DMDAMw7IJWYhXHMesH3uexyZUmngFJzEZSLdhkiToui7rF3V1UHxbC4pU4TakBWQQBDsC\nSnBRzsREv99X5VUIElISkecdUFRtH0JquVyyfls3D/FtblmWyCJViRf9XpUAA28rRV6AeERCiiYy\ngY5Z+D7LsurCwI0nPBJLRQ2WpikOh0M2KD3PEw1LC3Gi8CQfEVMF8ZxOp2Wir9HEIjspL5fLzKQ0\nGo1wNBoV2a8VwTKbzdhkRsXmcRyjYRgqUanG/YrnxU+whmHgeDxGxPPoR5tCij5PomhWGBRRIIGR\nm5yUbJWmKXN6IikfWmRRG9dAug35RUMURYxb2aSfF1FhGGZ8XonfE+bF+4eqh0okXNdtIrqkbLVe\nr9n4Nk1TeMPEdrtldhWIagjZajwe75RJ2LYt7bPIXwhM4FK2alNIUSmFBKfacUhzIPmlOqRpyhY2\n+0plk1iTiEDLvrdW2xxSRFUagxrf8zzpFRQ17r7D5DRwqpw4vyLvdrsi6RIhThQ1oImkCWgSIMEi\nwanUVjQpqE7MaZru1J11u918xKyxYInjmDnR/OqTIn6SUanG/Wq73WZW671eb2c12baQakuUl4HS\nDwUpUyVbEV/btoXrZvr9/t4cOPkrmoCrxFSRiELEjHg3DKNIgEnzon5kmmZhXanMUyS6XnIU4jSZ\nTJivoYizrG+P45j5906n0zh6x/ORiFgWQjCKLdV+bQgpEi4kjiU4VY5D8kFF9a9ViKJIdJGqxIvG\nuWg0sei9NUK6VtscUkSVGiNJkp16Fdu2cTKZCA08Ehg1k3ijCW+73bJOJPJaquMQCIEKcSIH0kYU\ngezVxkqYj+ZQurAJ1us12radOTKBGyCNBAtiNqWXB9lXMiql3K/SNMUgCJjtOp1OaV8hUeJ5Xiu8\nKDWhuNVcGJSm5PqZkq1oPIlGDlarVSb131a0mlb5JFZ45MUUjY0iEUXgBXTBb5PuW1EUlS4UyC70\nTKdTDIIAR6MRE0xlx1/kas4qOSVJwiYoenzfl5p8efApIoomFQgEIVtR5Fw1es4jH8Uu8RnS7ddU\nSPFRljaFFPkMlU05tEjdR7CDfIxoSVDRe2tEWK22OZSAEnKWy+USPc/b2Z3mOA5LQ5VNcPSetguo\nCSQ+iibgIiRJwjoPALCzLFTTVWmash0KhmEIcSgDTQTdbreseFnYVlRUnJssG4GPvnU6Hb49ldsP\ncTelVwSJKEYdp0peq9UqUzRdd7ZW20JKJSLFF7quViuh4nReZL+0ubStkiQp7ffb7ZZx8n2/9KyY\nmsiDcBsul0tWJ1LU33kxxaes99cIAAAgAElEQVQ3i0QUImbOUSpIlyj1LT4SqFIfNZ1Od3ZO58ZL\nKacoili/JtHTtKYT8dz/8eLMMIy8CBK2FR8pk4jwFoKPYrexueKYhVSV6IiiCOfzOc7nc5YuJnHO\nz+c1c4QSryaLQsH31mqbQwkoqQkP8XwH32Aw2NliWTaR7KvzEGjgyarxyWSS+Q1UUyHAqxDkpJoW\nkPKcCiBkK35FBtB8NyGBj0jlVhnK7VeV0uOhEJUS7le0auWLyS3LEoritS2kSKjKTCTT6bQ0WuG6\nLg4GAwyCACeTSeYAXZoAX9bjSbdh1eTC7yDin263i57n4XQ6bXUjCp+yLGsLXkxViag8/wJnruyz\naIJ3HKfupQxpmmYWfyQOZBd/+RS9aD2NCDzPy4wfgWh16Thsw2/x/qIk8inVfrRoqDmephL7FlJF\nixI+ZVr22LZdV5aixIu+W8I37rxXMT3LHn1FjIaGhoaGhoaGKkTUVouPNNI0zdQRlOVB9x2Rop1u\nsnnYKIp20g2mafJpESlOVG/RpEgSETM1SAUpGiFb8aF2ldVAEShSAlBY1KjcflW1UXnQ7xIMFUv3\nq/xVLL7v127vVkjFVfJKkoStyCu2SWeQbxvZIuaXq1HpNqxapVMBLP170aF7AhBuQ0pfcL+nEHQk\nim3blRGPfUWk+LpTkX683W4zR7g0revMRygUD7vdAZU48Gnxl75Q2lZtlSTwfrBgfpBuP/osVexr\nLqyaAyndDS8jx/y1bvs+VJXGo0o7Ckb6a7XNvoWT9IRXBH7SL3NK+xZSJIZEBUwYhpkwOS84csJF\nafCrFNbx4Ae/SpErhbT5SVLhMMsM+C3u8CoVJMKrEpSSEtwKz36b4O4UpX6VJEmmPqaq0BxRqQ6g\nlhf1T0qX1IEXLXxKMI5jXK1WrD7C931WH8GP3ZdpJqU2LDsrhkQWPYoFzcJtSGevAUgdjloKPk1V\n0P6NfBa1V905e3SwMP2mtnYaUxqUBLvEkTCFoE08/EYeldQeoa30HvWHthZ/xyqkSHQULUb5E9lF\nNmSVQImXwo5m2ffWaptDiCepwZ8HX2haVmTNHy64r9Nc6fPrJuLVarVTFDkcDlV4FYJEQYNLdhEx\nW8egWuSaP5PFNE2lLagE/myekp2J0u1HW29ppVR2GjD/TCYT0SM1qjgJ9aswDDMRS9d1CyeyfQgp\nfiIpEdQZ8M5SVkSkaUqTuZKtqqIHfE2dylkyFZx2eFFEhI/mqYL/nJLPauxLyW7dbrdQxPB+QOKa\nKylO+ZPKVcTUfD7P2KrgFgklW5GQbRox43cC5haA0pyOVUjxUewi4cnXy+57QcOD3zwlC1o019QS\n1mqbQwkoqcHPg0KGtMItAoX2agypzCuO49LUAmGxWGRSkKZp4mg0anIYYCFo9SO6e7AM/HU8BdEI\nYVvlt3hXiYEq5A/0LJnUpduPbxOVRyAq1Up/z6f76IwvmthoQpQo3BXixadgqoqiEbP3Wu1h1VkJ\nXhDnI5X8zlHFwmbpNqRoXpMFDdmern9qg1cefBqMuNLOZpGSiRJIc1I4qZzx5w86phse2jhzi3jR\nglc2op6mKU6n050Ud07sS3NqKqRoIb+P7AyNM9d1d/wiv6BRjPAp8aL5WcUnCe6SrNU2hxBPShML\ngb/frqg+JIoiNgE1OAuiEmWnQNOFpXz6otPp8AfXiUCKU5NOw4MPRxc4cSVb8WKATuUWXZlQOyue\nBVaINE137qkSiUgNBoPMYYRtbZ2vQz7dR+28WCxEzztR4sVfP2SaZmUaiI9AKkLZVvyuQf4uTn4n\nnWK9i3QbhmHIhJ1KFIwXhgoLBynj81zH43EmeiJ5J2Edr0rwJ5V3u93aaB5fX2oYxt4OXJY98qTs\nfr6SK2akOTUVUryvKPEXyrbidz/nU7UtXGOmzEvVZoJHCtVqm30Lp0YTC2L25NG8s+EHpsA2X2Ve\n5MDpaor8LeHkkKouulTgVQr+zCxV8LUlBZE2ZVvl74orOJ18BxIHerbWr1pEq/0d8VW6j0/5lVy3\n0govSlfRWOOvpsmDJowGaa1GtsqfcUR1WLSYUdw2rtSGJP5LDogsBX88QoMiVykUXR9TcD6UKJQ5\n8feSWpZVGrng67Y6nc5e71Llo1JVPpXu55O8XkaaE302Lf5kDznep5BCzEYXHcdhfb+Fa8yUeZGo\nlenPURRlNtCQH5TkBYiXQEhRg9EhaqvVCsfjcSaV1PallnlQlMBxnMJLLpvUBalwognWcZxM5ET2\n4Z2q4IF7wm24XC4zO2toZ1pRO0nsnmlVsLSEVvt7HvmU3z6EFA8+IsY7SQLfdxRPsW9sK/5sqqJH\nQeAptyFNWoZhCEWip9NpJsV1iLQxgdqt4nwoUTTilCQJ82FFIpSv2+r1ek3rtoR4ldVEUumC7/sZ\nAeW6ruip6NKcyvq17LMvIYV4Pgb5YEKn08lsBDhUZJhA31tUUkJtuFwu2UGh+UO/yW8o1FcD4pEL\nKSoiq3pKTguXNUYleNEmuAqRgTQnPuLT1iN4p51UG+bvz6OUAp8q41McAnn1vQmWBmh1sisCn/KT\niHwo81oulxknSduZt9ttJnyveARHa7YqSt22HCUT4sX3cdM0M5cB0+N5XkYQi1y83JRXHnT6dAto\nxTeQX6Xdd7PZLBOFlTypupGt+JsU0jTFKIpwOBxmCqhJ2EmmQqU55Re8Zdf2XKSQQnx1xVXRMSiK\n15gp86JFwmKxYIKp1+tV2o7uowyCABeLRaOI1BuICAeE9Jc9efIElsslfPXVVwAA8Ne//hWuX78O\nnU4H+v0+vPPOO6If9YYqr3feeQd++OEHcF0XRqMR9Ho90e9swquU09OnT+H7779vkwN0u13elsq2\nKsLjx4/h1q1b8PjxY/a3Xq8Hf/rTn+DmzZvw4MEDGAwGMJ/P6z5K2lYHQKu2qsJmswHTNOHKlSsi\nL2/E6+zsDKbTKfzXf/0X/PDDD4Wvmc1m4HmeCBcRXpe6DR8/fgw3b96EzWZT+bputwsnJycwGAwO\nwmtPaK0NP/74Y7h7927mb6Zpwv379+H69ettcBLmde3aNdhsNuA4TsZXAQD0+304OTkB27ZlOFXx\nOsb2A1DklZ+T3nzzTXAc52C8PvroI7h3717hv5mmCQAAtm3D9evXwbZt+OUvfynqR+t4AQDok801\nNDQ0NDQ0NFRx9BGpFqGkdp89ewYff/wx+L4vu0ISxY9ixQIA8NlnnwEAwMnJCbx48QJM04Tnz5+D\nYRjw9OlTkejij8ZWLaA1Xvfv34cHDx7Aw4cPM9GpIAjg5OSkLV6vha0ogv78+fPM303ThPfee082\nonHZ+pYSp08++YT5Bsdx4C9/+YtMpqGOkzCvu3fvwscff8z+3/M8+Pd//3cAAJnohSivY2w/gEvK\n69atW/D555+DaZpNIk8qvM5foIUUABwnr2PkBNACr6dPn8K//uu/wsOHDwEAwPd9uH37dhNer62t\nGmAvvJ49ewYAAN988w289dZbOtWxX1w2Xsqc7t+/DwAAN27cUP2IxrY6OzuDq1evwgcffAC///3v\n25iAq3gdY/sBXFJeT58+BYBGgrcKWkhxuGyd5xg5AbTI6+7du/DJJ5/A//7v/4quQH+0tlLAZeN1\njJwANK8i6DYUh7aVOC4jr/MXaCEFAMfJ6xg5ARwnr2PkBKB5FUG3oTguG69j5ARwnLyOkROA5lWE\noxNSGhoaGhoaGhqvDfSuPQ0NDQ0NDQ0NRWghpaGhoaGhoaGhCC2kNDQ0NDQ0NDQUoYWUhoaGhoaG\nhoYitJDS0NDQ0NDQ0FDETw78fce6hfEYeR0jJ4Dj5HWMnAA0ryLoNhTHZeN1jJwAjpPXMXIC0LyK\noO/a09C4bLh79y6cnp7C6enpRVPR0NDQ0KiBFlIt4c6dO+yaAw0NVdy7dw8+/vhjePfdd+Hdd9+F\nzWZz0ZQ0NA6Gp0+fwqNHjy6ahoaGFPTJ5udQ5kWDvtfrwc9//nPYbDbw5ptvtsHrtbNVC3itbfXo\n0SPo9Xrw4sUL9jfTNOHevXvQ6/UujFfLeK3bsGVcNl7KnGjBcP36dbhy5QqEYdgWp0a8WoDu7+K4\njLzOX3DZhNTZ2Rk8fvwYAM5vCpcQLa030tOnT+HatWsAAOy298lkArdu3ZL5GOmB9uzZM3jw4AF4\nnifzPTsgEXj9+nVRTpW8DoDX1iltNhu4fv06PH/+HHzfh7/+9a8AcJ7mAwCYTqcwHA4PzmsPuPA2\nfPbsGbz99tv8ny6brQCOk5cSp/v378NgMAAAYIuIMAxlL8RuZKunT5/Cl19+CT/88EPm7+Qbf/nL\nX+b7TFNeSra6e/cuvP/++wAAoveTynACEOD17Nkz+OKLL8DzPNmgQR1a7+9nZ2fsvxtw1TVSGhoa\nGhoaGhp7AyIe8pFCkiQ4m81wMBhgp9PBTqeDcK5MM08YhiIf1xovRMQ0TdG2bcaB/tswDIzjWOaj\npDilaYqu6yIA4GKxUKGOiIiz2Yxxn0wmopyUbFWE7Xar8ra9cgrDEOfzOQZBwJ7pdFrHtbGtoihC\ny7IQAHAwGGT+bTwes3byfV/m5+y9DRVxcE5pmuJsNkPHcdBxHJzNZqKcjtVWpbzG4zH6vo+r1eoi\neEljMplkfDn5+H6/3xanSl7b7Rb7/X7hvJJ/+v1+Ud9R5SWN6XSa4eM4Dk4mE4yiqC1OlbySJMHh\ncMi+fzweq/yM1nlVod/vY6/Xw16vh2ma7oMXIOJxCqk0TTEIAjRNs7BDO47DBpxt220YQxo0+Lrd\nLna7XUyShP3N8zyZj5LixA96wzBERWQGvIiiz8mJhb1OLEEQYKfTUenYrXJarVY4HA5xOByWinRe\nKNu2jcvlUpSTEK8kSZgId1230Cbz+RwNw2DOXNBul00ctI4wDHE4HDI/QouxAvu1bqswDJkY9zwP\nXdfNPKPRSGRSlubFj23TNNHzPFwsFk0mERleUshPyuPxGOM4Zn29pQWpkK0Mw8Ber5dZRI1GI9Ze\nvC/odrtFfkCWlxTiOK70Ud1uF4Mg2FtQgV/s8TaTbKPWeVWB718A0ERM1WqbQwkoYWMsFotMh3Fd\nF2ezGW63WzbZp2nKXiMRlWmtkShKYJpmhlcURcwJSAgcIU7T6ZStSEzTxF6vhwCAlmVJdWZ+VUMr\n9IIV4F4mYXJQihGWKl5SmM1mO06BJtq8Mx0MBjuCfj6fi3Cq5ZWmKWtHEuNlWK/XjIdt2yJtvpc2\nzGO73eJqtcIgCESjjHvllCQJTqdT7Ha7O+07Go1wNBrJcJLiFccxjkajwr5V9BREghvz2m63hd9l\nGAaLqCRJUtnXBNDIVmmasgWhYRj58YSDwWCvEdj5fI7z+ZzZZjgc1o6nJElwMplk5qbhcCgyMbfS\n333fZ/Mhtd98Pi/0T5Zl4XA4xNVqVRaZFLZVmqaZ7Itt25konmTQoA6t+SyaZwzDyGSzFMVUrbY5\nlIASMgZ1FuowZWKEwsES0ag6YwhjsVgwjkWrEmrAFiJlDLRKI5E2n88xTVMmghzHEeocvIiZzWYY\nx3FmBchNhK1PwvkQPnVyybB0I07r9TozwXY6HTa5VomANE3Z6wpWYsq28jyP8RCxQxRFjL9lWXVi\nvZU2TJIEV6sVTqdTDIKAhcn5tDY90+lU5CNb7VeI5+26Xq/R8zzWl2nBwU/Y1N8lOAnxSpIEfd/P\nfHen02HRztlshqvVCheLBXtNPoVbAiVeNLEul0scj8eFbUU+tuXUUC3iOGZ8TNPE9Xq985owDFmb\nSUx6QraK4xhN02Q2EhCzGZAv4G1YI0ob93eyR9UCfblc4nA4LBTxpmnmo5/C/Yr3e/xiTzFoUIdW\nfBYfaVwsFhhFUSaipiCmarXNIUVUpTFEc6+K0ag6Ywhhu92yzlPGkecnmE+v5cQLTD5yFMdxaW1N\nHnkRxYMmdG613kqHJuRTibPZjH2nZC2EEqe887MsS6XWARFfpVaDIKjjVMmLXzHJOKIkSViqwTRN\nXC6XZWkG5Tb0fR9d180Ig7LHsiwm7gQjCK30q6rIU6/XY1EOGjs1/UzZVqvVKhOl6Pf7pfVJfDnA\nPtOz9D28f4zjGKfTKYuA5h/btvceVQzDkPmrbrdbKeCoj0sIHSFb8TWHvV5P9LN3sF6v2W+xbZtF\nbiR4CYMWzKIRujAMM+1ckIITslWappkFfN5P0dhyHEfm51Sh8byzXC5LF3YNxFSttjmkiCo1BkUr\nDMOoFUeK0ShU4cUjSRJh0UJRK8E6oEpOaZpmQrf5zhyGYa244yMp+TA64vlkkLNp4w5N4EUUpScR\nMRMJkyiMleaUJAlzRCSAmtSL0O/hQtrStuJtIlFrwZCmKROiFdEg5Tbk7WUYBrqui57nYRAETLjx\nkQRyXoITU6N+VRR5otD9aDTKTM78oqamjynZio+yOo5TKYjptaZpykSAlHiRSC+bePOpId6Wgn5V\nmtNqtWJ+zHGc2tQi70MFIWQrXmAU+UIZ8BOz53llaa5G/Z1SkJ1ORzgdm6ZpZoFRMC8I2YrPvhSJ\nJdWgRhspx7LPpb7MLXQzUBRTtdrmEOKp1BhUX0Q/vqoxSPErRqPqjFELWiHR6kP09QKriEpONJCo\n2LkIfIfP24UmXMMwSiftNE3Z+2s4CTsAxKxgKOrYtDpsMw3Kgx80lmUVphFkQTVmKkKKRAj1d8FU\nWCn41XVBX1NuQ+ozovyoLqfb7Yq8XJpTHMc4Ho930hZ85KkI1FYC/UvaVnyEt273Ei1UDhVFp++T\niRTwaUeBCVuaE7WdxGYJJgYEbSZkK74POY6DQRA02uG43W4zC12JzSi14Oc7GV9Bop02QqluruAX\nCiW1hWyMUdCgqG3TNMXFYoGe52VsVdCuyj4rDEP22XV1Wwpiqlbb7Fs4VRqj3++zMPRwOKz8JZPJ\npEk0qs4YlaAQpmgtC2I2z6+4fR4RX01qtKulDDSpUqooDEMhEUWgjvUyBKxsK0KdiEJUchRCnIoK\nJNvaXUI25VIOQraigU6DvcwxyYIKZwt29Sm3IfUlGY7U1m0V35LopMJjeizLYru76kDtL9C3hG1F\nvESEEdVkUR9XaHOlNkyShI17megrLf4EhIU0J4qKy6TyJf29kK3K6sVo1954PJau+eF9nWVZeZsr\n+1Gad2TmOz7S3zTtzy8Wynw44iub8nNUmqaFEU/+cV1XiVceeWEkAkkxVattDimiMsbgd5fUbaOk\nCbdBNKrOGKUgxW0YhvTKpWF9BiK+SrNUhEMZ+MJa6rwiIgrxlZB6KRSVJ2FEMRFFaDMNSqAwOzm2\nsr4VxzETInW1GHxhvmyxeX7rsGChsRT4XX0vIxHKbUhtIjPpUfRAoMamlhPtZuTTMK7rSqVB1+s1\nApyn0hquOBmSJMn4oSqBRptBaPwWTBoiUG5DmtxkorDkr+rGrAonlWMN+IWWgLgRthWN5fl8jsPh\nsLDOjjYqUDlCXb+mjEHB/KTUfnzJhoywo4VHGzWBvJCqWgTQWCPfWHQuFx3PsF6vM/NTLjAh3d+T\nJGHtJ7rpilAkpiTr3NhzSBGVMQaflqgLxRXt+LJtGweDAQvN7mM7OL8zQXZ3B2J2t4XC6gARs5Gi\nut+YPyS0bFdMEdqKSPGpKwGHjIhSxZS1nPhoQb4epcp51vVBfmKXSaHlz4kqOyuqKQrSu8ptSP1W\nMFWHiMUFziUQ4kROjaIklmVJbdknPg0L4DO88jVRVeA3iPBicDgc4nQ6FR2Xym1Im3dk/JaEgFbi\nRIsbmWMNJCJZyrZCPPf1tAmm6LymOv/AH08jeJRMJcjXyBwvQCldgd3QQrbifUrdQqBIPNm2Xbgr\ndDAYMMGX65/SbcjXiirsPsUoijJHI5REtGq1jb4iRkNDQ0NDQ0NDFSJqq8WHga99qNuKPp1OWbqm\nLMcNLyMQ/MnBuVWf0opFcmXLwK+mKeSosj2W1LIo6KyhbrcrHBKmYnPDMOo4VRJRiUYhZuvJaqJu\ntZz4/jEajSrD9/xTtXLnQ9ydTkdqKzG/YqLatTZBZygVRE6VV+gF/aEWFIERuDZCihMf0bNtWygq\nxaeRBFepQrbi+1bdjq/xeMxW3lV9z3Ec5tsmk0n+9ym3IaXXZdKzcRwzP1oDJU4q50NJpASVbVWE\n7XabmaPqangpY1BgP2lOFAkyTVOqtpP6Z4PUbIZXkiQsDWcYRuXYi+OYRXSm02nluOPrnXMRN+k2\nVPEPPPiDWWezWZkWqdU2hxBPhcbgj91X2TWxXq9xNpuVnnmjenZGHvyuQplJkELrAgXqlZzo91QI\nscagHHeT4w94EaVwYrnouVKVnMpOdeaFdr/fx8lkgmEYZibGslRL/gws2Z0m/PELVGzexu5BxHPR\nzBewt7VrD/FVqlf0TkRKa9RNOCqcZOsgSPhKiAghW/E7jmTHYpqm7AT4fr9feHBiQdpXuQ1pLFiW\nJcVTsN2V+xWlrGRSjoIpwUb9vQiixdYZEpDZ/VzFqxB8XZiMjUiYNKw13eHFbwhTKW8pAgmWNoQU\norqYklj412qbQ4inQmPwhaRNLt8l8M6WohESxqgEfzy/CPgCdYFJs5ITObZ9RDMIBefOCNsqv6Xf\nNM3MFStBEDCxW/XwEwtFWQpQySl/HEBeOPGgw+aqJkb+EE/F1Qr7Ln51W3aelwz4c81KREOjiYXG\np2iBt8SWeyVOorts+ImohfvQMrzq+osM+BO+LcsqqwFr1IYk/GRqR6if1mQJlDlRtEVG4NGkS5Es\nhYJgJagIKeojArwqv1N2px61dZtHRSAim3fI/ip1SHm0LaQQ5cUUf1xCg8Ufew4hngqNwRdjNt0K\nzt9ZRjtl2ryYVOb8Kv5QsDZONqfVQMVE3hg0QXGRQSFbkYASOQFb5qk4M6uS03g8ZqKpTnTyUbj8\nd8VxnImYKk4qO/2Kv24BQP329Pz1QG33d8RX47NsFUp37I3HY/R9n/EROEBRmRNF4KqiTTRRyxTK\nV3DK8OIPKm0yFmezGfsdlmWxKyxUeZWBLwCmlEtdqogiGzWTSyPBUrK7rRD8+VZ0tMehhBQ/R9UJ\nKTrklASHAK8MqA8oHFLMsh8SJ7VL24qigrI744rA37mam/sbtaGomOILzFu4qgkQL1BI8YfUyZza\nmkccx5nUyb6cEinoqtAp30AS6a1KTuTYqN6iTSwWi7IVYq2toijCXq/HQr/5KFT+mc/n7AiHqqcG\nrTlKPkdPq6IkSXA8HjNn1ul0mnAq5cWnCz3Pk3ZMNEFSFKNk7DTq72QfOrSw7H69omefd4/VHbxH\nvkAyDSFkK/7Cb5UU7XK5zIixXq+naishey2Xy8yCIL9QGY/HOyk8ij7UREQatSHft0Re10LUQAl8\nfWPdooev+8llLoQ48Sk0mYNU+WiRRMZC2lZ09EddRLgOaZqyCGxB1LhxG9aJKT6S/9qcbE5pHdXG\nmc1mrHEPcYkrNVBRBI0/ekDyzJhKTlTAWHB+USPkO3TTbagHQmuc+LoL3/dxNBplamA8zxMV90q2\nWi6X7PsELj5loFWywFUjjSfhKrFkWRbb1DEej3G1WjUt7BYGH/XlFyyKl91WcdrhlT/fyvO8QkGV\npilLe/u+v3NZdsNotZS9kiTB2WyG/X6/MHrc7XbR931cr9eZlLeCSBeCyPlQ/N2rgqK4dZ/FC6mq\n9sqfLyZ7jlTRWDNNEz3PYxfUl4HmHMm6VCVb8YsYx3GU5iK+/KJArLfW3/NiKkmSnUOaJYI3tdrm\nkCJqxxgUPaLG6Xa7tXUNaZribDbLOCaBlV2dMYRQlSum2gLZ825EOPEhepVC7iLwn1mwAmrdKbWE\n1jiVRVdc15Xd/KBsK5lLXBGzd1I2qL0TshelN+mstsVi0VaRfCttmC8UDYKATb4KY0TYVpRa4lO0\nog+dyt6SA1cCXddRdGUHiTw+vSR5J5owyH75KHuSJEyoity9KsBJ2VYiQmq73WZqFSVO686AyhGC\nICjc5UmHg/LiJX89iwRa8VmmaUpdXcNHdKl/tcUrj7yYsm2b9auqQ5pLUKttDimiSo2x3W4zncey\nLOz1esxBep6XOSWYf51E0W4rjZTfXcaHdE3TFN7lJMCLgXYOitxJKALKT9OOrwLOrTulltDaIONX\nfrT6U7xzq7FgoQFflS6quk9xH7z2iNY48VuX+UdhU4aSrbbbLQ6Hw0pR7rpuk/vcDtKGq9UKh8Ph\nzk5C8r8SvISRvyWAHn4sSLZj67biI4+j0YhNvmEYsutP+OhKSZRDiVMURTiZTHbmPP67KAKmUK/X\n2GfxtqELw4uENwU++BRzxXVnrbYhL6Z4f69QMF+rbQ4poiqNkaYpTiaTwm3B+afX6x288xD4c03y\nq9KWr67JgBdsote+5JGmKVu102c0uYvpAtAKp+12i77vK9lQgpMwrzRNM9f75BcHJedE7Z3XntAq\np/zxFBIFtyKcjtVWe+PFR0WobkeClxT4RSlfZiASnZXgpGyrovqy/GMYBvq+v7c0KOL5nDOdTgtP\nD5eppxLgJMVruVwK102SiGlr444oSEzZtq0izkV4AaI+2VxDQ0NDQ0NDQxlvIOIhv0/oyzabDWw2\nG/juu+8AAMA0Tbh27RoAADiOA2+++abKd7/RlBfh9u3b8Lvf/S7zt/F4DKPRqE1ehZw++ugjuHfv\nHhiGAScnJ+D7vpA9Hj16BMPhEL799lswDAPu378PvV5PllMprwNBylYHQmu2unXrFnz++ecAcN6f\nAAA+/PBDuHbtGjx//hx834fbt28fnFfLaL0NT05OAADg008/hcViATdu3GiLE8Bx2grgALxOT08B\nAOCdd97J/1MrbbjZbJhfJ7iuC4vFAt5++22Zj6riJM2LcP/+ffjqq68AACAMQ9hsNvD8+XPodrtw\n5coV+Md//EcYDAZF9hHhpcTp7OwM7t+/DwAADx48gH/7t38D27ZlP6ZVWz1+/Bi++OILePz4Mft/\nHq7rwmAwgA8//LCuXUGHVoAAACAASURBVPfS3589ewYAAN988w1cv35d5SOqeJ2/4BiF1J7QWiOd\nnZ2Bbdvw7bffQr/fBwBgnbtFXqWcbt68CXfu3AEAAMuywPM86PV64DhO5nWPHz+GzWYDf/zjH2Gz\n2QAAQLfbhXv37tUNvss2sRwjJwAFXnfu3IGbN2+y/zcMA168eAH9fl+2j/3o2vDOnTswHA5V3nrZ\nbAVwnLykOX3wwQfw8OFD8DwPAACm0+mFL5Rbxmvvs1rEZeR1/gItpABAgdf9+/fh008/hfV6DQCg\nOvireFVyevjwIXzyySdMINWh0+nA73//e7h586YI18vWoY+RE4Air4cPH8JgMAAAgOfPn4PjOLBa\nrWT7mG5DcVw2WwEcJy8lP/r111+zCGwDvPa2ahGXzVYAx8vr/AVaSAGAIq9nz56phKDzaDTQHj58\nCA8ePGDRJx6WZcG7774L77//PgwGA5mJ+LJ16GPkBNCAF7Wl7/tHk+poCT+aNmwBl43XMXICOE5e\nx8gJQPMqwtEJKQ0NDQ0NDQ2N1wZ6156GhoaGhoaGhiK0kNLQ0NDQ0NDQUIQWUhoaGhoaGhoaitBC\nSkNDQ0NDQ0NDEVpIaWhoaGhoaGgo4icH/r5j3cJ4jLyOkRPAcfI6Rk4AmlcRdBuK47LxOkZOAMfJ\n6xg5AWheRag9/kBHpDQ0NDQ0NI4UdMWJxvHi0BEpDQ0NDY0LxKNHjwDg/DaG/LVSGseHL774Ar7+\n+usm1+fsDdSXAID1pWPjeAhoISWIO3fugG3b2vFoHBSbzQZWqxU8f/4cOp0OXL9+Ha5evXrRtDQu\nCeiS2wcPHsB6vYbvv/9+5zWGYcB7770HH374oewNCK8dnj59CleuXLloGhl88803cPfuXXj69Knq\nLQet4ezsDO7duwd3795lFzrn0el04MaNG/DrX//6xzNfIuIhn4tEI16DwQABAAeDAcZxfAheF4nL\n1oZ7R5qmmKapDCclXmma4ng8xk6ng51OB+G8NiDz9Pt9DMOw7qP23oZJkuBsNpMdD3vjtF6vcbVa\n4Wq1wu122wanvfSt7XaLk8kEoyjaK6/JZIKmae70H9u20bZtdF135987nQ5OJpOyvl7H6yLRShta\nliXSLm3wEobjOJm2i6KoKUclW61WK7QsK9NfHMdB13XRdV00DAMNw8j8u+u6uF6v98qLR8ttJ8IL\nEFHXSGloaGhoaGhoKENEbbX4XCQa8XJdl6lswzAwCIK6VVtTXq0gSRKVCNpla8O9YLFYoOd5aJom\nhmFYFgVqzVbr9XpnxdfpdNDzPAyCAHu9Hvv7dDqt+7i9tOF8Psf5fI6DwYCtPsfjscxHtMaJ55Ff\nCc9mszY4tda3lssl+r6P3W6XcQyCYC+8oijKRDG63S5OJpPSKF0URTibzTLvcRwH4zgu8x2XyTdI\n8QKAtqNSjTlR37Ztm/GzLEskKi3LqZTXbDbL9Kf5fF46/223WxyNRpmI+mg02guvPCzLQtu2Zcd/\nE16AiJdXSCVJgtPpVLSBsCkvElLUmWmSo4mlAVq1VRiGOB6PcTweM8do27as6DtIGypg75x48cRP\nzJQykuAkxWs+n2cc5mKxwMVikXmN53lskhNAa224Wq0KbcKPA4n+1ZjTdDotTHvyf2sp3ajUt+I4\nxtlshv1+H/v9/o7Io//vdrut8wrDkLVTp9PZ6UN1WK1WzI40YReIisvkG6R4URu1KKYacQrDkPWV\nJEkyiynTNMt8kiqnHV7k9+g7fd8X/pI0TXE0GrH39no91ZSxsL3y/mA8HmOSJJgkiTBvSV6AeAmF\nFK1C+c4k6MQb8SIhRR2LF1TwMhesuEJoZCty2oPBoLSmBl7W1bTASZjXarViuXPZZzQalQnk1vsV\nCZYioWDbNosgzGazslVOY1vxqz3f9wv783q9Zq8R7GeNeG23W/R9fydC1u12WYQjjmM2DiaTiejP\nVeYUx3EmMtztdnE6nbKakcViISM0RTgJt+F6vcbRaLTjF/i+NBqNcL1eY5qmTEzV1HIJ89put7jd\nblkf7vf7ypNHkiQZO1uWlf+sxuMwiiIcDoc7/cswDOz1ejidTi9k8cdzITG1p3okIczn84z/TtMU\nPc9jiyrDMFSiL0K2StOUiWnBCGoh1us165ckpvZRb7rdbpkmyGeQDMPA4XCo2pa12uaQIkppwluv\n1zgcDtE0zZ2Jjv5/uVw2NUYtaELlHR+tjHkBMxwO9158u1qtSp22ZVk4HA5xOBzifD7PdGKJFEzj\nNhwOh6Wiru6ZTCZlE3Mr/apMONGExxcCk8NS4CTEa7lcsu+uStdRW0usCKV5RVGE4/E4k36iPuX7\nfuGkT+LFtu2mvCqRj7QURYGprRQcvrStkiTB+XyOnuftLGAMw8B+v4/T6RSn02mhP6DFYA1X4QmP\nBK7CoqkQ+c8cDocivITARymqHok+VcVJyj/w310TlWvKSwi+75f6bt6OLaXXM7ym06lqW+xgu90y\nQeb7fpkfa8WXuq6LiOc+g3xCPqggGcmr1TaHFFHCnYccen61QqtQWg1TR8oNchVj1IK45AcUhQ1H\noxFbZZqmiePxuGmkDBHPbTGdTivTBP1+v7IGgiY7ABAN9TduQ7LXbDZjUby6h35bxSpQmdNqtcLh\ncLgz6fERlqLvpD4WBEHZpKdsqziOGZ+qFPVkMmECooUU2g4vEt75RYrneUIOh35Dw75VCj7SQrU7\nVTwUIsNCtppMJixqWraAEVzUsUhDTXpPiFcQBJn+3FLtJqsLpM/m7Ko8DklAGoaBvu/vtBU/JiSi\nnFWchH0WIjJuSZJkFqoNxFQjTpTKKxtbvNgRnAerOGV48TVzsiniIqzX68z8VTBWGrUh2SJvB6r1\nG41GmcWzRC1VrbY5pIiqNAZtpc47qU6nw1bDeaFA6Q7LspoaoxZlQorHdrvN5LAtyxJxrKWcylZu\ntm2j7/vCThsRcTweswlSYGt4I1uR8xVsF0RElodXnFhKP7NMPAVBILRFnkQMhdMlONXaiiYVWkEV\nIY5jNvglnZkwL6rzo0mkqpC0CGSjhrVbhaD0AkB5jcV6vZb1BaKcMrzyvqnX61UuYKqQT++VfEYt\nL/5zAEBmq7kwaFXPRRGU+jv1E9M0S3nSok9y0VDFSVi0IGKmD5GY4qNTCmKqESca+1VZjuVyyfpA\nv98XsZtQvyJ/YBhGa+Kc5qGSKFejNqyK3hHSNMXpdJoJ0gjUUtVqm0MJqFpj8JOdYRg4GAyEhAK9\nT8CBNGok6tAidQfL5TKTHun1eirOEtM03QkxNznDqt/vs8+q+R2NbEUrZIkVEhONNZsHpDipiice\nVL9EEUEJTpW2IrFpGEalc6ZJrNfrSfGW4UU1CzSWZAQ6vV8iGiRlK3KOVZEWihbK9jkBThle/Opf\noT12wKf3VKOdxMlxHJXaMCFQyoSb+KT7O99HitKy1AcVau7qOAmLljiOd8Q4Ta4NxJQypyiK2ERf\nBz71bdt2Y/9O/omEZFvIp4xzi8NGbUjzm+jmr8ViIVpLVattDimiKo1BTsXzPKkiSUpJ7GsrMXvz\nS2PLoOhAPN/3pQo3oyjKfIbMrok8eEdVMxE0shWFhGUmZBpYNakkKU5kN8WdLYj4KlJGKR0JTpW2\nokFf1p78bpk6sVUCaV4UMVCpsSEhLPBeIU4VaaUd8FEDWRFYwynDiyZaPv3TBHx6ryQSW8uL+lHF\nZgghpGmKq9WK7YR2XbfwUNiXv1m6v5MYK4s4Uw1ig/RkI5+F+Eq4FEU1G4gpZU4UnRMV7VEUsUhL\nTaF8ra0E/J4y+LbOfXajNqT2kU3tC9RS1WqbQwmoWmOQU5FttILVUhkaNZKKkEI8H4BF9SfciquW\nE9UPUfi2yXELvDDbR9EfTTYy4WByYKZp1r1UihM5FZXUC4FPU5akjaRtlSRJph4sDxK85BgkC0mV\nefFpIllnFMdx5W8S4JUBX7dVklJFxFd9h57lcilUj5eL7Arbiq+RanpWTT4tV9BPa3nxfVykn9OZ\nUbPZDIMgQNd1C2tR84/ruuh5HtlN2jdQxLDI31C0SrLWLo9G/h2xWkghKospZU5kM4kjfjIcaYNW\nwViutRUJKdFIZxzHrESgrh9S9JH6PrcgadSGBZ8nBRoXfL9/6Xtqtc2+hZOwMfjJRcYQaZoyYaDo\nwGsbic8Xq4JPP9DzcoIU4sSreMMwGtVC8OeClAgzZVvxqTBRUHpiMBjUvVSKE39khSp4kVci9KRt\nVbdooHauaB8RKLUhpdIE2qL0vTVRUyFO/Lbrqr7Op9tkntzkImwr2oknEymoAn+US0FUvZaX7AIv\n37f4x3EcHAwGGAQBLpdLpXKEMpDvK8oc5DkFQaAyGTaahBHrhRSikphS5iSbqiKkaZqp1TUMQzqF\nRkcJkMDNg4TTcDjc2eUrml4vyFwotyEt4EXSoEXvJVvT0+12eb9Tq230FTEaGhoaGhoaGqoQUVst\nPpVQDZnTqq6mQFGZl8hKpQz5nXzwcuUnoHYLQemOpoXn/CqwIMKlbCtS9gJXmDCQfQTaXYqT6opu\n50u5tpPgVMqLIjdl/XW73bKQOn1vr9c7yGndlKIzDEO6f9Eqlt6rer0IX4tUl+6l9FTdw6evCup0\nhG1Fv4t+Z1t1Uqq8KvplIfijUCzLwsVi0eZxEaUgf1MU6QzDkG3m4NtdMjKl7LMIon5eMiqlzKlJ\naQIVn/P1tZwfFLIV/971el0agaKxwP+3SLvRvM35feU2pF27shsuxuMx+53k9wpKKWq1zSHEk7Ax\nKGQuW+wqWF+lzEtFSNHx+NTBKERaIBakOKVpyiZYx3EabUvlT8jNCTMlW/F5b9FizJJceRmkOPGH\naTYBX3ArcV1GKS8SjiKF0bPZLDPQD3GoasF2d2HQZKiwE42BP8W9rZ1o/JlABf1B2lZt10mVnHRe\ny4v6pugJ3CR2VReGNbxKQbWGdcca0I0IvKCaTCatbOmvg4yf5w/SVSzsrkSSJGzMyyCKop00VcG5\nhkK24tPORcKp1+vheDxmi/Ber8d8m4jPJT/ThpCikhLRkoT1ep0RhP1+X2Xxx55DiihhwSJaqLxc\nLtHzvIwaVjwLQoiXqOOZz+eZyZd26knyKkUcx2y1orjdGxFfFf0VCDMlW0kU/jPQCllwk4EUp6q6\njDpEUYSTyWTn9HiVws08ZFeacRxndpXk8vdlUO7v/NEMslEpfsIsmTRrOfG79doQUnR2jQKnUlu1\nXSdVctJ5LS/+wEbRIm1euCkuxJT6FfkZkeLp1WqVich2Op291cASyM+X9bk4jgtP/nddF5fLZdnC\nSIkT+VLR/h9FUcZHUESvJKonZCs+ekm/kxdOeVAfFJ0D2hRSor4+SZKMnRqe9cieQ4oooQ5NE1eZ\nUyi6WZo63HQ63YuQohVy3WS/3W4zqynHcVo/V4fnRM5QNuIShiFOJpPMCoKel8JMyVYSR1EwUKcW\njLRIceIP0xRBmXiiU749z2slIiVyuGsRVqsVe6/AYqPROKRVrUo0j0+jSUR/GGg1LrOoKgN/WGHF\nBePStmojvUf3ZPJp3Fx6r5YXicSKc852wO8IPeT9oHykUSSSxy/y9r0rG/FVtI7389Rn8lEey7Jw\nNBrtrdicfFfdQjmO48yucDoxvo1zAulIDNnNOjQ3172P5so2is0L0oQ7yEf3R6NRG/f0AuIRCilS\nljT5xXGM0+k0M/j5zhwEQdNtqLW8+DM18uCviOFXTxJ1Ocq24k+jruq0tMOi6F4wXvQFQUCrDSVb\nieyyKnuPYHRGilPdDkLqW9S/isSTwCpf2laquwn5oysEatAajUNaEYueMM3ficnbUaLuJwN+vKvW\nuC0WCyaiFA96rbQVtaMMvzAMcTwel15u3O12pbaD88dOiAojvh6p5R2hteBrM7njFHYQhiGz7yFu\nY0B85edt2y68EQEAhA+KFuBVCRIGReOcUlC+72fa3vd90QhyY1tVgebwMr9Lc2abxx+Q4M7PPWEY\n7tSbuq4rW3dWq20OJaCEG4nC+qZp7qwC6O/D4VBl+78yrzIhRSk8fsCNRiPZFWqjDk2Fy6ZpMkFJ\nq4iyi407nQ56nsdWW6qhXx58Okb2kajVkOJU1G4knoruS6MT9fd1FQuBVpAy50Pxh+0JRh4aO8u6\nU6YpOpw/h4hOkS9Z5Ahx4tMEfN8WQZIkbFyIrOorOFXaqu7YDkqdi15urBrt5CMSInWTo9Eoc4ek\nAhr1q+l0mhEAtm1nNgbwqbNOpyMaNWvc3/PHwlBf7na7LNuhACVONPb4eS5JEgyCIJOaJUHa0mnr\nwraqQt2GlfF4nImktsGLxhZ9Xz64oRDgEOUFiEcopBBxxzH3+3026TcI8yvzIodOjU4ql+eooHLr\neAmD0nOWZe1cakyP5L1g0rbKn5El80gUNUtx4ut1isQTCafBYNCkb0nbiqI9lmUJfSd/c7rA9Q/K\nvPLI1ztURfA6nQ4Oh8PWU9m0mOp0OrUCN45jnEwmzKkahiGamlSyVdHhs9vtlqXNyxYNw+EQF4tF\nawXUdNEv/e7BYFD52fzdiirnhYlwqgPV/ZX5K9M0RVJUIpyEeZGQosV6k4N8BXiVgs4tBDivYSMB\nlY/29vv9tuccqTasAo3b/GIxiqKMEFQ9z41HvjB/sVjs1ChL9qU8arXNIUWUcCP5vo+2beN0Om20\nxT8HZV58iohf6ZJzb3gzduMOnSTJzvbubreLvu+LOmxRTq0NNEVIccqfes2Lp4aiXIRTpa1IiNRN\nZPzqXUJEKfPiwd+Pll840ITjed5eUx1JkmRESbfbxdFoxCJedJ1JXiTbti0TtVa2FX1vv98v3BZO\ntlO83FiYF12pQ5Ot67qlvpO/5FnxHrVWfUP+1HnFw4Yb9/coihoflSLBqxTUNt1uN7MwoAUxtfUe\nOLXm3/nSAAK/ICzxfUq8+NsneF+RO2aoCWq1zSFF1LFOwrW88sfGSxaqqfKSwna7LUsPtMnpWNuw\n/A37EU8inGqdJQkkx3FwPp+ziSQIgp2Used5stxbacP8qdNUX7OHCF4tj7L6vjw/hYWNsq3yJ6vz\nafOKjS974xWGYSYiNxqNdgQc8eJX8S3xuki8Nj6r6LR+13Ub3dAgyKlVW9HCYj6fZ+6dpfrHNkpK\nEHd3F5qmKXWOoQBqtY0+2VxDQ0NDQ0NDQxFvIOIhv++gX5bDGxX/Vsnrzp07cPPmTQAA6PV6cPv2\nbbh69eq+eV1KW+0Z0ra6d+8e3LhxA9588809UVK31aNHj+DGjRvw/Pnz0td0u134/PPPodfrHYwX\nj7OzMxgMBtDr9eDDDz+Et99+W5aHKC8hTg8fPoTHjx9n/mbbNgAAuK6ryk/ZVqenp/BP//RP0Ov1\noN/vg+M4Kt/fKq/T01O4efMmPHjwgP2t2+3CtWvXoNvtsr999tln8OLFC4iiCK5cudIGr2P0DQDH\nyauU082bN+HOnTsAcN63x+OxyvhX4VTJSxa3b9+G3/3ud5m/eZ4Hk8kEAKBorCrx+uyzz+CTTz5h\nn/8f//Ef8M4776hQLkMVr/MXaCEFADW8Tk5O4O7du3Dnzp22OzSAdkoyeO1sdXZ2Bvfu3YP79+/D\nX//6VwAAuHbtGrz//vsAAE36m25DcVw2WwEI8NpsNvCHP/wB5vM5vHjxovR1y+VStp/pNhSHtK3e\nffddePbsGXz22Wdw48aNQ3ICaNFWp6encOXKFXjx4gU4jgNBENT1MyVeN2/ehNVqBXfu3IHr16+r\n0q2CFlIclDvPZrOBq1ev7iuqoZ2SOLStxHHZeB0jJ4DXhNdms4EnT57AkydP2N86nQ5cvXoVut2u\n7Apet6E4lKLog8FgT3QA4IC2evjwIXS7XdGIpxKvR48egeM4F5V1OH+BFlIAcJy8jpETwHHyOkZO\nAJpXEXQbiuOy8TpGTgDHyesYOQFoXkU4OiGloaGhoaGhofHaQO/a09DQ0NDQ0NBQhBZSGhoaGhoa\nGhqK0EJKQ0NDQ0NDQ0MRWkhpaGhoaGhoaChCCykNDQ0NDQ0NDUX85MDfd6xbGI+R1zFyAjhOXsfI\nCUDzKoJuQ3EcjNfZ2ZnMOTy6DcWhbSWOy8gLAHRESkNDQ+NHjc1ms48bGzQ0fjTQQkpDQ0MZm80G\nTk5OLpqGhiI2mw1cv34dvvrqK3j69OlF09HQuJQ4dGpP40eMR48eAQDA48ePWSrBcZx93Y+ksWfc\nuXMHfN+Ht99+W4upS4iHDx/CjRs32D18X331lezlxRoaDHSZuGVZbV8afPTQESkNDQ0NDQ0NDUVc\n6rv2bt++DQAAt27dEnn5ZStkO0ZOAAqXpf7nf/4nLBaL0hvoO50O/OY3v4HRaFRX8Ppa26pl7IXX\n2dkZAJyPuTt37rC/h2EItm034dWqrU5PT+Grr76Cr7/+Gv7mb/4GRqORCqfWeUliL7zu3bsHAACe\n58GLFy+g2+3Ct99+C/1+H+7fv9+E12tnqxbw2tnq2bNn8ODBA/jyyy8hDEP49ttvC19nWRa8++67\n4Lou9Pt9kShVa20ouXmiDrXF5oCIh3xawXa7RcdxEADQ8zzRt+2dlyIuEydhXkmSoOd5COcDAAEA\nbdtG27bR930MggCHwyHats3+vdvtYhRFKrykkCQJjsdj2bfJcmq1DYMgwOl0eqG8oihibQgAaJom\nG4dBEDTlpYwwDDEMQ5xMJjgYDNCyrJ1+p8hp7+NwsVjgarXC1Wp1EF6z2Sxjm+FwiFEUsfYUxGvp\ns0Qxn8/RcZymvC4SSraKomjHpxf5d9M0C/99MBhgGIat8ypCv9/HXq9XN5+IolbbHEpANe48aZpi\nmqY4Go3QMAzWOPsUUnEco23bOBwOcT6ft9Uoorz2gtVqhdPpFEejkQonIV7b7Ra73S4CABqGgb7v\nYxzHpa8Pw5C93rIsTJIEkySR4SWF+XwuO/lXYa8OPEkSdF0XAaCuzfbKa7lcZhxkt9vF7XaLy+US\nAeBgE0uaprhcLjEIAnRdN+ML+McwDOz1euy/FTntZRxut1v0fR87nQ4CAC4WC1wsFnvnFQRBxkZ8\n/yfxWTPR1fG6SOy1DeM4xiAIWJudxyAa8WoFURThcrmUfZuwrWjeDYIgM9Z6vR5OJpPK/hKGIU6n\nUxwMBpl+5/s++r6PaZoq86pCGIYZPxAEAQZBUPR9oqjVNocQT407z3q9xm63yyZbctzUoC0YoxTk\njOmxLAs9z8PpdIrb7Vb1J4nwUsZ2u8XtdouLxQKDIMBer7ezSh8MBiqcanlFUcQmXMdxhG2UJAlr\nUxpoErykwK+qKr5LFHtx4Ov1GtfrdcZx17TZ3njxE/BgMMDBYMCEbpqmzMGWiF9RXoWIogjn8/lO\n9DI/JvlxyTt3sl9NP9zrJIx4PhFPp9PC36CwcJDm5fs++77ZbIaz2Szz7zQmBCO1jThFUYSWZaHr\nujgYDFi0dTqd4mq1UvWre5l3qL8X9buGvKSRpikuFgvms/j5MN+eipwyvNI0xV6vl5kDPc9TCijE\ncZzpgzQ/5Pp9K23Y7/fZYo//vk6nU7ZgqUOttjmUgFLqPEmS4HA4zBij2+3ier3G1WqFAICu67Zh\njFJQKNwwjMKQpWmaaJom9vt9HI/HuF6vZX+msq3SNMXVaoWTyQSHwyGLXFQ9NOnN53MVTpW8kiRh\ngq3X60mvAGglYRgGGoZRFMVqpV8VtaNEZFOUk7IDn0wmzAYkFCSjPq3wSpIk40Qnk0nh6+g1gs5c\nilNRxMlxHPR9HxeLRWWkk+dW40Bbb0PE88jnfD7fmYhN02RjtSbt2AovEkmGYZSOe/Jzgv60EScR\nP0X93nVd9DwPgyDA2WzG0qASvkHKVmma4mw225mEaRFB886hhFQYhjgej3cW9EXzkITAEbIVCRL6\nfIW5rfD30MKH+j83TzRuQ34OieMY1+s1Oo7DFuk0N0mK9VptcwjxpNShl8tlZjU+Go1wNBoxo5PB\nBOofRIxRiiRJmDOP4zgTrsxHefjHdV0MggBXq1WdoBDitFwuWUqjKMKUF5vdbhd7vR4GQYCLxQK3\n2y2rhTAMQ5VTpa3IYecGhxR4h1EwMTfuV+QILcvC5XKZESye56nwbm0STpKkMAwexzFzZofiRQ6P\nVnJVTnQymciIUSlOFMHxPK+sjqgStAreZyqbRxiGOBwOM76Ld+Cz2QzTNGU2Gw6He+NFEQUa81Up\nIOpjAr6hilctptMpG3+r1QpnsxkGQcAiP67rFtqubFHITYiNbBVFEfq+n1lkdTodlhYi4UY+tNPp\niHystK3iOMbZbIaDwaDQDrZts7mQxgP5DMdxRP1Xra2of3Y6Hex0Oq2WtURRxKKSOb/ReByS+CvK\nMkynU9a+hmFk9EQNarXNIcSTlDHiOM4oYcdxCvOw1KEtyxL52DpjVIL4FIW9qVPUpR9IFdNKWoBX\nBrS6zTsS13XR930WFq/CeDwWnfCkbcWvBJqkPIljyWBoPNmNRqPMBEYpNBpgCpG0ViZhvk7MNM2d\nuhnJ9FkjXtPplH2f4zi1UR/JYmUpThSRliy0Z6Ax0+/3VTgJ+6zJZLITxaAFzXg83rEh+ZSaKJ4y\nL762zjRNodon8l0CglWJUxRFrF+JiOIwDHG1WuF4PGY1cbx9c/5YyVbL5XIn2uM4Tmm7tDjvZNDv\n9zPzHi/mPM/D+XxeOg75TIBg3WelrdI0ZSKOFvD7AF/L9HLOaDQOqV6zJJuBiLtZrk6nU5edqeMF\niEcmpGazWUYxVuXraQXVwsqglhc5Y9HUSpIkuFwucTQa7Qx+wzDyg1SKEwkB0zSlBQuFNwUGhrSt\naFXUsN4os6uoQPA1Fiw02eUdeRiGqmKq0eBHPP/NNMHYtl3YrjTJCRYDK/FK0zTjZGTakhx525Mw\nrYxV+xU56263q8Kp0lbz+Xxn8jNNE4fDYW07UV9T3KVa2beSJGH9xbIs4T4jGL2r4lUJ8oWqaXS+\nbxb4YmFb0SKFFVgC8gAAIABJREFUj+obhoGe59Xaal9CiupayQ8MBgMp/86nHBuMQUB85YMl6o+V\nQVmMlwKwkS+lPi/iK9brdSbo4bouK6wvQK22OYR4qjUGf5wBNaBIKJFeLwjlRuILamU6d5qmTGBQ\nCqlAxEhzIudNO9xEwEcNGoTud3hR5+PTn3lst1tcrVYsqla1Ct+nkKqzQRiGbCVm23bT4mmhfsVP\nDsPhsLRtqM0liiWFeVFUlT/WQGCVlgFNwgIrYilb/X/2rh7IjeNKP1W5auhozEsMRiAvweoSQZEg\nJ1g7ERSBjAxecBpGAqODGNyBFxhkBFkBSDsw6AjrCw5UYiwTY50cyAjrSwBdAjKZJS+YpZNZMZpl\n9C5Yvd6ewcz0zwywwKq/qqkid7HAh9fdr1+/v6ZTpkIuZAj83NRUlAzT6RSbzSY2m82lPLtGoyEt\nM4XwkNbcok3VsiylkMxoNGJzX5NXIiikVygUVLyqDPx3SvB6K+ssvuhEtoJ3VYYUgQ+piTzBUVBR\niMS+kCorWssqLWIot2wwGGjNuR+MNm1dSu9TKBSUIgpk3GfwWAPiBRtS3W4Xu90uU3S2bStVH6zL\nkEI897jILjg+P8G27TQXqTKnIAjYpkeWtAgKYb00Tku8KPmTvmer1cJqtZqaUJrGga/siJG19vgh\nniuptOo3PnYvaUxpzSvecInxUi6B5JKU8K3Li9oakGFAbQ1UQXMg701YMz8sBJIzhXEVOIHrutjt\ndmOTj8vlMvb7feUNjw4LAuWdykv0h5IerxB832ffTXMjjgUf0tMJEwVBwNYk7RkKnFJlxacSCPLV\nEPHMmJbwcMrwSgQdmlQPD0EQMIdEllA2GZgqezHpVnpKpRJLY0nbnyKGqfZ8pzWuoB9xNpuxeZmh\nvxUgorkixsDAwMDAwMBAGzLWVo5PCNQ7hK9OqNfr0jF9+jvJU5e2tYt4HmKQOYkoJnpqcXJdl31/\nmZOUQn5UGqclXr1eb+k0En1KpVIoHyEtB4OPW8fE+jN5pGhMRCct3iuVodt6Iq/RaBQKDcl4OSWr\nvJR4RRs08v2hVCEK70rwSgTNc1XPD4G8yaRvFDhBo9GIbcFQLBax1+tpVTNROFfi9Kyts8iroZqk\nT+tPs11ELPLKjVpVq4jhcMjGuFarpa6BHNvuJMLzPDbnVTwsiGHvX1y/MAEnQDz3SMnOHd/3Q3t4\nUsuSuPZAlMP4Q86b1hjqhvVojUjoVKFts04jKlEYQRBgt9sNbS6O4wgVJ2126zCkgiBg/NJKwTUS\nPbU5TadTNmnTFtxisWBGXV7lsQQypKjXS6/XY+E+/rP4XKCkBcqHCBO4asuKb2ORpCin0ykOBgNs\nt9uhUE6xWGR5RDGQlhXlZgyHw6VKoUKhgK1WKzGsRoZ8Hg1oo/2hKO+EqoN0jamMlWiJIK661UMU\nvtFo9AqI54nlZFRFN4pyuaxkVPGhRgG0dRYZ3hLhQwYqZMnLkCLDleZ31tyoVV4vwlfulsvlxLFc\nhyHFf45lWcr9m2jsdfPJZFIgeFDaAV0Rg3gmz263G5viwfdeJJ3xg5GtNYbRhqEyB65ovykBhLbN\nOo0o4eTxfR9brRZTVHS9SNICVFBIImFIgYyBJI8Kn/dCm28GXlLgWyIkbTTkebjIewn5yZ6kpOk1\nq+hsTnIqlUqsH1e9Xg/dHZf0VKvV3HMzEM9OnnH5N5R70+/32dwnYziH3AxAPDPMms1mYj8yugJC\nxeNCm6Zm1/xEaOSHhcAnrCdsgNJjyBvDcd6qcrmM3W430SCmPKSM/ZqEc4s/PMmAb9opkRsj5ESe\nEb4KbTKZKM0nPjcqQxGDtM5yXZetxUKhEGu4rcuQQjzX26VSSThXqEo8TpfF9JdKlRXlJQIAu8cy\nCZ7nhfKM4l5L1zpFu7Hzzw97l9YY+r4fOgTYti30pqX1m4qB0LZZlwGlNKE9zwtVVNi2HXtfDlm7\nko36MvOiRRRXbRNNUs6hy6w06MSd1BKBJq/CiT6zrKLgF3ic4cufXD3PSzolaHNKu2yT3wTr9Tp2\nOh0cj8dr8XQSqCIsrus6VYTRRicJaV5Uet3r9WJPkJQ4Klpn0apIxQq5RFBytuIVOQyUIEwnYQVO\nUmM4Go3QcZylsaM+Up1Oh61LxSrETLxk7s8LgoBtKqKmnRK8GETdy6lrebPZZE2D6QJnOjzwIb1V\nGp08+LQMy7KW+rmt05Dik8f5QzC/Xmu1WmwojQ5CCffhCWVFhxfqf5gkf3qdiueT9PtgMEDHcXgv\ne6YxXCwWoXlXqVRi9xpFb5SIFyBuqCFFWCwWS31aCoUCszZJaJIl4bnwolgwbSpkhStWesnwUgLl\ngUQv/aWTqUK/rTROyrwI/CYTnbzUYRxA++oa6c8vFAqs6/xgMEir5JJFrrLiQ39J10Ks2sDzfR+H\nwyE6jrPUXZmMkUajgYPBYGmu8326EgwvZU6qtxhMp1Ps9XqJ3aFXdb0IYrJRRYaV4gXUmXjR4SHJ\nk8eHeBWvAEnlxF9d1ev1mPe3Wq3GyiXtkQjpiTgpj2G0NQIvQzLqc/DuS4G/u7Reryd6kKnjeV45\nsLw3MGlfi3qjckAuYzgcDkPrnlqWEH9Fb5SIFyBuuCFFmE6nSyX1xWKReTky3vGlxIsscGq6x5eO\nixIVFXkpgT+9kKzo1m7imwMnrTHky6oBwm/Bd9FeVSNAUuy6uT+anLTnO4+48N+6PLCE6XSaGDag\nk1+v18PFYhFq6pgwnsqcgiCI9cZR41vyVler1djTuW3b7LqkTqcTNw9WMobkzYgzSNdx+EtqrEgH\nLRrPpDCWBi9p0H2p1LWcDg5RT5Zsu5kUTtpjyLdGIB26bkMKMdxbj/Y+uqRbM6dRSlYUZeGviuLn\nLR3eVbxRefCSQTTcR3qA9kPbtlWKV4S2zTqMp1w3lvF4vKTQJasLcuEVDRPwrlTN++Vyk5XneaFT\nRLPZTOzkrclJixd5xWhRIp4tUt7jkjFWfZFY6XznQeG/dXpgoyC3fL/fx3q9vmS48B3aEzxIWpxo\nHne73dSrmMjz4zgO9vv9tXsz0jCZTNg9fKtu9op4Hmrl87EoD4jkqZDLKcMrV0jcUyrDKRMvaiLM\n9zpctyGFeGbU9fv9TNdvSXBa4kXrnW+YTeFqmlu61bRZeMmCwn1RA10x31Jo26zLgMp9oQ0Gg1zv\nF1IBb6w4jqN72a2IlxaoyRi/wSmG9dI4afGivBAypHgDSrEJ61oUuCLWMt81sDZeaYnrMZu0Fqfo\nZc68N4y8X6PRSFepX9ox5O/PWywWS+GanOV1kVjpGPJVfRdhSOUMLVn1er0lz2rWK8Hy4CULCvmp\ntkkQ8ALELTakEJHdor5OjxTieZmwbl8USV7aoJAC75nKiZMWL0okj3ouWq2WqiK/NEppDbgQXnzS\nerVajTOStThRe5R6vY7dblfVwyrCpR1DCrU2Gg1mCFACcYZQ96WUlQh8VV8O+TUXiUyyoopVxfDY\nynnJwPd9nZxYoW1jOpsbGBgYGBgYGGjiA0Rc5+et9cMi+CDld0q8Xr58CU+fPoUHDx5kY3SGJF6Z\nZfXo0SO4d+8eTCYT2N3dzYMTgAavR48ewbNnz+Djjz8GAIBPPvkEPvvsM7h69arqW61MVhmQq6xy\nxLbx2kROAFvOa39/H27dusX+X6vVYDQaAQDAlStX8ua11bKSwcnJCdy6dQt2d3dl9f+PVlYa2EZe\nZy8whhQAbCavXDg9ePBAx+D7UcpKE9smK4DN5LWJnAC2nNfJyQlcu3YN3r9/D47jQL/fz2JAiXht\ntaxkcXp6Ci9fvoRyuSzz8h+1rBSxjbzOXmAMKQDYTF6byAlgM3ltIicAwysOZgzlkQuv3d1d+PDD\nD6Hf7+dACQDMGKrAyEoe28jr7AXGkAKAzeS1iZwANpPXJnICMLziYMZQHrnwevnyJezs7ORAh8GM\noTyMrOSxjbzOXrBmQ8rAwMDAwMDA4NLAVO0ZGBgYGBgYGGjCGFIGBgYGBgYGBpowhpSBgYGBgYGB\ngSaMIWVgYGBgYGBgoAljSBkYGBgYGBgYaOIna/68TS1h3ERem8gJYDN5bSInAMMrDmYM5bFtvDaR\nE8Bm8tpETgCGVxyE7Q+MR8rAwMDAwMAgN5ycnFw0hbVi3R4pAwODLcXR0RG8efOG/b9SqeRx3Ugm\nHB4ewunpKQAAFItFuHHjxoXyMTD4seP58+fw+PFj2N/fv2gqa8NWdzbf29uDvb092N3dhZ2dHWg0\nGmkv3za34SZyAthMXpvICUCT13w+h++//x4Azi6WrVQqF8JrPp/Dn/70Jzg8PITDw8PE19VqNajX\n61Cv1+HatWu6vKQ4HR0dwe9//3t4/vw5zOfz2NeUy2X4l3/5F7h586asYbVt8x1gM3ltIicARV6H\nh4fwxz/+EabTKTPSX79+nTevSyErHuSF+qd/+id4+/YtjEYjuHnz5oXzisPp6SkcHh7C7u6uzMuF\noT1AxHU+uaJcLiOcCRgBAEulEk6nU5xOp3EvXxsvRWwTp03ldZHILKsgCLDf72O1Wg3NZ/6pVCrY\n6/XQ9/2V85pMJktrCwCwUChgtVplTxxPx3HQ8zwdXqnwPA+bzebS55VKJcbHtu2l39frdVwsFiuT\nFSEIAmy1WirjIwOzDuWRyxg6jhM7r1fA6yKxknnVaDSw0WgwmZXL5Y3gFYXv+1gul7HZbObBCxDR\n5EgZGBgYGBgYGGhDxtrK8ckNo9EIAQCLxSK2220sFosIAGhZFlqWhePxWMWqvEhsE6dN5XWRyCSr\n4XCIhUJhyftEXpZKpRL6nW3bOBgMVsLL9/3QidK2bWw2m3FricHzPBwOh6G/sywLB4NBEk9lWU0m\nE+ZtsiwLHcdJ5BQEAY7HY3QcBy3LYt9jNBqlfUTm+d5qtZjXLk1eiri069DzPOz1elir1bBWq62S\nkxSvIAiYF9ayLDaecAEeqWazmTrHc0Du82owGIT0Buk0wbpbOa8oFosFsxUcx8mDFyDi9hpSNOl7\nvR77Ge+WtW07GmK4tEppBbiUslqRYtKWVafTCbnBh8MhBkGw9LogCHA4HIbCaRJuaWleruui67pY\nKpXYRtLtdpXDVK7rYr1eD21AnU5HllcshsMhe69arSYKGy7xqdVq7O81jDupuTWbzZjRRo/jOOj7\nftZQ36Vch91ud0lenucpja0CJyleNG+LxSILBfMHHNd18+aVCN4oKRQK2Gq1ZMLTeXDSmleu64bC\n6oPBAHu9nk54b6XznT+QwQ9h/xx4AeKWGlKz2YxNsujGQyccAMButysrjIvE2jm5ritamGuT1Wg0\nwslkIvvyTJwqlQpWKpWkHDpdaMmq2+2yBR2Zp6kYDAZsE2q1Wpl5eZ6HxWKRndJKpVJmpT0cDpln\nWGEdhkC5jvQe7XZbmw9vsAJA7t5q8hq22+2QkVAoFLJ6qC6dzuIPu7VajR2IR6ORqvdClpOQF0U3\nbNsOzX1aE+s2pFzXjc3TKpVK2Ov1LtzojIIOeJQjhXh2+CNDdDgcXggvAh2gaF3SgbFarebBCxC3\n1JCi00PMaRfH4zGOx+M4a3gtSmmxWOBkMsHJZIKdTgc7nQ62Wi0Wqok5Ea9FUbqui71ejykugWJY\nuawGgwFTVJKhqjReUuC9JY1GI6syEnFK5EXzM2FTF2I8HjOlkNXLwnu5yuVybsnS/X4f+/0+e2/O\nWBZyCoIAS6USU3gCg1EK7XY7dMqPfE/t+U7eA/5Qt1gslkKy5KFSxFp0lga0ONF84EOtNC7dblfp\nQKHASciL5lm/3w/9nB/DDIcvLU68bnQcJ7aQolar4WAwiPVia3JSnld0IKQ1xc9x8krFOTxWzSvK\nj55Wq4XT6VTVWya0bdZpROWy+BeLBQs/xG2EQRBgEATsNZLCSITrusww6vV6zDgiw4hX+GkPndBj\nOK9UVp1OZ6kCSyInIfcx9H0fe70e9nq9pZygdRlS0Yocy7LYeGooIxGnRF40HnEHAVnwG3iCohLy\noo2N3iMPwzKpuo7zKAllRQqYlF2GsQmBrzaMGGda893zvNRcEDIOeA+VouG8Ul2aAcqceA8FLysK\n3fLejJw5pfKiQ03cGuIPGQpec1leqSBdRcYdhfejoXPSY4o5VbnMKz6kHScfPu+MT8FZNS/67Ki+\nJ1mSx69YLMq+ndC2WacRlcvipyRA0SmVhMf/SIYXH5/WefgS8WazyZRHq9VK4pybrGazGXY6nVjj\njs9JkHCh5zaGnudhp9NZOlGVy2V26spBAUiBQjzNZjOUHE3jpuCCluG0xIv3lpZKpcwGAn9ijlFU\nqbyirnfN787geR62Wq3YXKGI91MoK97QzhjuCWE2m7G0AMuyePlrzXcyGEW5FlEPlYJ3Snsdxm24\nKrpLcLhR5kR6NRpOoYMx6awM0JIVbbZxhxpehhnWh9b4kbzi5haF9vr9/tIhWTKnKrN+J6+xaC+m\nsKmkVyqXfcf3fWYE27bN9C6BDKlCoSD7lkLbZl0GlLZxQKDJQ4o66fRM7sUsHik+zwrgvIqq2Wwy\nD8Z4PMbJZJLWtyqktFPi2plkNZvNsNVqheL59Ni2jY7j4Gg0Ss0rU+Akzct1XWw2m6GNleRKpxea\n7OvKkSIPDFVrTKdTljfFj7WiG19aVo7jMMUteUJLBSkp4q3Ci/424+aFvu8vGcr1ep0ZLTFIlRW5\n3clLlieIE60VzkhTnu/E07Is6fwZDe+U9jqk9a57GBToCGVOZJTEGWi8jlinZxjx3Dsct+Z5j4aC\n11yWVyo8z2M6XATXdbHT6SztAWScUl6VBCdpXUoODRmPsYJXKpd9hwy8YrGYpIPiHC26vABxiwyp\ndrvN4ulpJ0AybGI2CWleZIzRgChk94cgGcJRltVkMok1ngqFAjOcoid5Oj1LJu1mUuBRl2qj0Yid\n0Os2pCiMEBfajLYhaDQashuktKxs22YGRx5VOEEQhJK6VfJ+aD7o5qUEQYC9Xi9kQFWr1UTFJcEL\nEc/DemR06mI2m+FgMGD5iXEGA/f+yvOd1raq/JK8Uwkeqky6lIwXmRwz0pmSXkBtgyVufvBeFYn5\no8opkRelgCQZcHwLhCQjwPM8nEwmKzkop8ksCdPpdEn/xhg7mebVZDJhhwgZPcZ7pTSb9Urxms1m\nTIeXy+XUz1I03IW2zTqMp0RhULiLrEe+a3K73WbeH8qrISGlTSz6m4iiFAkjFnxZp2rVkIJLU4lT\ntL9JoVDAZrOZaoz4vs8227yNA4LneSEvnmVZ2Gw2Uz9PMvFdhpcUSAHEeG8Q8UyxdjodJivLsrDd\nbovCMFKyohOm7CmTkshFSrRcLiedqlN50WaumkQbl+dWrVZzM4bJwKNkdREWiwULTbbbbaxWq0vh\nxegJvdFoYLfb5WUrPd/p+9P76XpQJKv7Mm8svEc8DWR0SeYpKXOiQ1/cWufD7PV6nenwZrMZ6qcm\nSA7W0u+098SBr/asVqvY6XSwXq9jtVpdOsTm2TeNQLpe1XvNt3IoFosqeblCXr7vs7WvwovkpelU\nEPIajUZsPdVqNWHonL5DDvshIF6gIRUEQeiELvuIvEN8KXdEMWkNEl9hpRInz8GdmQi+AqnZbAqV\nOSl+hcZ3WrIajUZsPGVCF2nKVZGXFGSTDD3PCyn3QqGQtrFLyYovaU77fL6aUWa+8wnUEWMmlZeG\n7GNPuho5TKmyos9I2ZhCiLY24GVMm/JkMsnNGOY9gDIKOw1x1X0ReWpvLAQZrxRf/i9ZcKDMKS2E\nxhcXpD2CA4iyrET6IFrxlfaswpCicVGJiJDxRa0cEjxG2vOK9KJC64DQ/FpFtTg/f2SvfVHUf0Lb\nxlwRY2BgYGBgYGCgCxlrK8eHge9YXKlUcDabsTYD0VYD5FKt1+upXg7+PWNOGdpWOFm8lmVJhUJy\nqlRIBTU9hB9OB2knY77pnSS0ZeV5XuiU3W63E2Wwbo8UFSGcTftkRC/tLZfLbF4qcAp9CP/ZkSII\nRFz2RPFPWnhvnR4p13VDHhnNBNxUWfFl3zKhPQrXwg8nXgnvkwqnpYmSQ0uDEKLd7XV5JUEU3uOr\nN2XkLeCViLRk8+l0ytof8Dp/OByydSexbpVlJfJIjcdjtu9Ei4xo3ZCHZhUeKb5oSiaEzO9TglC7\n1ryi6IxMqJjgeR6LUkjoC2VeMnlscSD9LpmSILRt1mE8xQqD7zWTpZ8OgY/dJiiFTEqJ+Mr028mp\nd4YQ0+mUTdKkjtR8FZQCMivw6AYRx42458BLGgmJ2Yh4tunweV7FYlEmnCstKz6UTYp4NBqFDKhi\nsYiDwYApcBC49vncQck2A4CI7Huq9sehDYMMF43eU6myonBKSruQEAQJ91k5Jc6tDC0NGMgwTTGY\nM69DxPTwHm1ESXmDCVDmRJu8buHOKgwpel+ZnMUk0BzQOGhJQXbD5yt489RZPHijW/bwQIc8ycIR\nKV5BEIT0I6+rZXMWFQudhLbNqg2nRGHwlTR53IFGJ1lKvlUUhhBBEDDOlUolccAUvVFpvKTgui5b\nbHQi55HWJ0WDk5ICn06nocuko6c2Ge+QJC9pEB8y7KjaJnpPY6/Xy70bL7/4m81myOtFBhSB73mU\n5JWiHjwJhnIqLzoY6B5iyBDTuHA2VVbkfVDpK8Qn3Gs2TtSe71m8UyTDZrOZlNuRyzpM8krxP1es\nllPmxOeXqRrf5DnKO0cK8byruW61IB2MNCovpUD5sGnrlG+MKVlFqj2vVO7Qo9cWi8Xc+6ZRgUm0\nHyBdtC4azzQPqSIvQLxAQ4o/hWctBaf+QKQQFPvXSE9qupcMIL66hW9SlsP9QtLwfT9knVNYhK/W\nU1ReuShw4had7PV6PanXly4vadAJcjQaYbvdDnkz6NZ3Rc+CtKx4b06SARUFb3hFwbu1Y36fyiva\n0Vm1+kzRZS/DCxHPi1Doe8mE0vnqX03DMNN81/FO8QeuVbU/4BE3j8j41LjHUIsTjZGqV4o2ZEFF\noZassvR1I2+/oHgl0/jRwSIpudt1XeYlUmgXoj2vkjrUR7FYLFK7nufJi5qTRptQl0ol7Pf7sa0p\n+KKWjLwA8QINKf40nsWQ4jcmzW68SkopzfrP+cZrZURbI5By13Cn56bACXRpJH96l1BCsrykkdTx\nWaFvlCynJV5ksBQKBaEBRUjyJvCKSjcsxPdC0uknxYf48sxz4+exTHUQ9U2Tfb0CJ6W5FeedivNQ\n8ZvROnQWYrhBJ98Ytlgs5nlXWyr4hsqy841P2RDkeGrJivJqFXU2Ip57dTUvD5cCH7qOjhPf67BW\nq6mMY6Z5Jdrn+KthFI30zPN9sViEbhSJ6nhycCj20RPaNus0okLC4Dc03a6xvBElcRLNTSnx8WhS\n4rKWugIvLfT7/ZDBosEnjZM2L0Rk5bjRkFYOvKTBh/Cq1WpqZ/qMnBJ5qX5eNMfF930mw5TGlUJe\nfGM92UKKKDTKoYWyojvsZPvV0IlTIvyjykl5vstcWEyGooTMcl2HcYcIzbQKbU58QZBIZwdBwMKf\nEoaOlqx4va0y/3mjUBBGyjx+0XweOpDRzzUuG888r0j/xBUoqHQ9z5sXj7jQHx1yFO87Fdo26zKg\nloTB3wyveor0fT+0IeYgDGVQUixtQmThapxscjdYKPHRtm3dazZylVUUQRCwxaZ4RUlmTnQXYY53\nuK1UVojLuSykBCj3IEtYiPf+2LatnCvCX9wrWfUlJSvKgVA5bFHYXSPfJfcxJO9U1ENFHm3JvKRc\neUWT2zNcEpyJE38ALpVKOBwOQ+EXuqCXvC3UE0mTk5AXn8sju/HLFIIIeEmDCnfoIEUVjsRZw5Oe\neV7F5QLzFZayXc/z5hUH/m7CaPhPsu+U0LZZpxEVEgbl79DCli295bsq27adRy6S9iDFnfDyblCY\nBYvFIu8S9dyMA8SzPB1FZZ6Zk2b4TodTrrKKzjUJBSrNiw/1WJaF3W5X6SRJSlXy3jklWfHVn6ID\nU4Z8l5WMIXlgeQ8V6a5VX9WUhFarxapHNSouRbykMZlMEtt9ROe5pGGsLSs+t1WmySodmiVD2rnI\nig7p/HrQOfgIOCnx4qvTKQSr0/U8b14i8OG/HKoJAfECDSnE5VNnvV4PJaYFQRC6V45feNVqVdXi\nzX2QKBZME0onzp43p5ywlgmNiHm5fi8Sa5EVn+NSqVSyKPBEXrx3qlgsxl12msiNFGiGcFUi+M7F\npVIJB4NB7LwhL7eGp2XlYxjNn8q7IlQWnudJd43X4KWMwWCAtVptKR2hXC5jv99fW97PYrFgRQ7l\ncjk2zOe6Lgs1SvRqEvGSBuVJkWxIVhmq3XOZV7xXij/oaeYp5sZLBZI2hNC2MZ3NDQwMDAwMDAw0\n8QEirvPzYj9sb28Pms0mvH//XvgGxWIRvv76a2g0Gqqf/YEqLxkcHR0BAMAvfvELGI/HUC6X8+K1\n1oGJYCWyygE/all99dVX8Mknn8jOfS1eBwcH0Gq14NWrV+xnpVIJPv74YyiVSqHX/u1vf4P5fA5v\n375lP2s0GvCHP/wBrl69qsorVVaHh4fgOE6IV6VSgUqlArZtAwDAmzdvYG9vD4rFIrx+/Trt7WQ5\nCXmp4OXLl3Dnzh3493//d7h586bMn5h1KI/Msjo6OoLPP/+czbFisQjXr18HAIB3797BfD4HAADb\ntmF/fx92d3ez8FKS1c2bN+HZs2cAANDv9wEAoNlsqryFDCdlXp9//jkcHBwAALB1uFgs4Nq1axfK\nK2ek8TqDjNsqxycRnudhq9UKVXTBD2GMdrud1h9KFit1G15kvsEKsHYXqyS2idNW8xqNRthoNEIh\nhaSnUChgq9XKegGoFAaDQaiZb9KjmAt3KcdwhdgmTsphtE6nE1s+b1kWOo6TV08+JVB4W6Pnlwon\nZV58ygFVsG8Cr5whtG02wiO1JmybtbuJnAA2k9cmcgK4JLwODw+XPE8AAOVyGXZ2dmBnZycPXkqc\nTk5O4LtuDF5+AAAgAElEQVTvvoPnz5+Hfr6zswPXr1+HSqWSBydlXjlj23htIicATV7z+Ry+//57\nAAC4cuWK6pwS8VLiNJ/P4be//S0Mh0MdDlHkKqubN2/C1atXYTAYZKAEANs3389fYAwpANhMXpvI\nCWAzeW0iJwDDKw5mDOWxbbw2kRPAZvJS5nR6egpXrlzJSAcAcpbVy5cv4fr163lw27YxPH/Bmg0p\nAwMDAwMDA4NLA1O1Z2BgYGBgYGCgCWNIGRgYGBgYGBhowhhSBgYGBgYGBgaaMIaUgYGBgYGBgYEm\njCFlYGBgYGBgYKCJn6z58za1hHETeW0iJwAFXjmW6xIuraxWgG3jtYmcAAyvOJgxlIeRlTy2kRcA\nGI+UwQpx//59doWOgYGBgYHBZcRWGlJHR0dwenp60TQMBHj+/Dl8/vnncHJyctFUDAwMDAwMVoKt\nM6SOjo7gl7/8Jbso0WBzcXR0BK9evYJbt25tvOH79OnTjedocIajoyN49OgRfP7553D9+nX44IMP\nQs/Ozg7cvn0bnjx5AsfHxxdNdytwcHAAh4eHF03DwGArsTVXxJBX41e/+hXM53NwHEf1bp9ti79q\ncdrb24PXr18DwNlt3J9++qnOHVGZZXVycgL/8A//wP6vMV4qvLTH78mTJwAAcPfuXajVajAajVTz\nurZtXgFsJi8hp/39ffjtb3+rvOE7jgO/+c1v4MaNG6qcpHiJQLrr6tWrqn+6ljF88uQJ3L17F+r1\nOuzv78v8icn7kcePUla0Vv/85z/DtWvXZP9s28aQYes8UgYGBgYGBgYGGwNEXOejhSAIsFqtYrVa\nRTizTLFYLKq+Te68ckKunHgZ8bLq9Xp5cJLmNZ1O2WdbloUAgJ1OBzudjgoPWV5aGI/HS7Kq1WoY\nBEEenJR4+b6P7XYbfd9X+bOV80qC53k4mUyw2+1is9nEarWKzWYzC6/Uz6rVamyMbNvGZrOJw+EQ\nPc9bev1sNsPhcIiNRoPNPcuysNfrJa2DlcpqMBiwuTUYDFT+dOU6q9VqMblalhUrTwVeF4kfhX7P\nCSuR1WQywclkgpVKhc2pRqNx4bxygNC2WacRpS0MXokWi0UsFAoIADibzVTeZtsGSQv1ep3JqlKp\nMFmRIpfcqDPLajgcIgBgvV5fMliGw6HOV0vjpYzZbMY2WQDAZrPJZFUul1UMmsyy8n0fy+UyAgAW\nCgUcj8eqX2clvOKMJcuyQnKLGuwZeMViPB6jbdvMgOr3+0qGrud52Gw2QzxjDL6V6gbeWAEALJVK\nOJ1OZf50JbyCIMAgCJiusCwLS6USO+xk4HWR+FHo95yQq6xms1loj6a1SnpCYZ/etjFkzzqNKC1h\nOI7DBsa2bVwsFuxnit6NlQ/SYrHAyWSCw+FQxfuSKyeSDQCw0+9wOGSbUaVSkdmIMsuq2+0iAGC7\n3UZExF6vFzr5TiYT1a+WxksJrutisVhkmyptrPzPFYypTLLijSj+cRwHHceR9RBk4rVYLHCxWCwZ\nTHHGEj+G5IXqdrvs9a7r6vIKYTwe43g8Zu9br9czeetGo1HICIyszZXqBhrfdrvNDBZ+faYgd140\n34iTbdvMk0CGfAb9II0gCLDX6zEejUZD1Xshy2klm7DrurI6TJlTt9vFTqeDw+FQ1Vkgi1xk5bpu\n6OBOe3Sn00Hf99kBolarrZQXecBKpRJWq1Ws1+ts/x2Px2x+S+gmHV6AuOGGVLvdZkp7NpuxSTUa\njRAAsFqt5iWMVMxmM5xMJjgYDLDT6WC73WahxrQTOgDIKP9cF3+cIYUYNhIcx9HlJM2LePT7ffaz\nVqvFFhcZxYrILCvecIlb4FFjSsKQ0ZYVz6VYLKLruiGjhOSkGA5S5kUhL5GxNJlM0PO8WJmQQs1g\nHDCQtzDB6NFG1DjjNsKVbMK+76Pv+0yWZKDwHirBZpwrr8ViweY2zTl+DdJcHI1GorfKxGkymYQ8\n5XTAq1Qq6l9KzEnIazAYMH3OP7yskp4MvBLR6XSWPqdSqaDjOGwdTiaTLAeLTPMq6uW1LAtbrdaS\nbvA8j42zxJzS5kWHdtmH9Fq1WsVWq4WdTgf7/T6Ta4x+E9o26zCetBY/CceyrKVQh+/7TBmuOgRD\nJzWZp1wuY7VaRcdx2CKUOFHkqsB5QyqaD7JYLGTdrZnHkE4JcWEq2nSLxaKqMsgsK3JBp3mcXNdd\nMnA0OKXy8jyPfUaUy2KxWMoJrNVqqt4paV50SOE9FKqeMDLEJLwKqZyCIAh56DJ6KWJBuqVcLos4\naa9DRAx5eqJGAm2WhUKBGVwxyI3XZDIJeaUrlcrSZ9IYShxQtTnxhmylUmGfWSgUsFAoqH4tGU5C\nXnGHCNknLw8sD0qLsG17yeCMPoVCAavVqqoHS0tW5GXiD3oirznJlltrufNyXTdkK/T7/SWHR7Va\nZfM/zcBKyBMU2jbrNKKkFz8lZwIk59PQJqNwWs80SKTUO51O6FSQBNqwV326i4I/7cad5On39Xpd\nh5M0L1IAcYomCAJmaEmGGkW8pECnKAnjKNZblPA3yrJSCSEOBoNQjtAq53vamInAG2KavBDxXPGW\nSiUslUqqif9SCIKAfdcfcpVyM1h4UHgBALDVai39nnQEeWpjkAuvwWDANr9Go8FypKIIgiDrQSsV\nnuexucx/X35jzjDe2rLiiwFIr4tCQbR+V2FI0Voi48P3fZar6DgOM4STDAIJT7+0rGiudDqdkCFS\nr9el9AS/1vjoRFZeUaQd3KMgeU4mE+z1ejLFUELbZp1GlNTi5xOT0yrNSOFKhKkI2rx0Fjlt2hLV\ncrkqcN4tHDcpPM9jlveqTsJBEDALPwme5zFlJDDqZHgJwXs4ZfMOoknghUIh7m+lZUXGmGoeVrRq\nrVqt5qosCXHhWBWQos2yCZNsRqORbDhACN/3cTQaMYOF93j9kMOnLKteryfMkanX68z7GncgpA0z\nxRuTaR0inqdHJBlzUZDeErxWi1NSzgyfN6YR7hdxEvIiQ0rF+0kb9yoiDnw4WASqUO10OiphNClZ\n+b7P5iave1TztigVRyL/TnsMSb8r2AOIiNjv90PcEvgJbZt1GE/SwphMJtJ5EYvFQvYETNDmRcpd\nZZHTwEoor0yKMgqRIYV47s1L2awyKXDaIEqlUurrFotF7Ak1BVqcaCHLnlh4RENNMcpASlZkQNFc\nigutiDAcDplSsywLu91u2suVx5A2FAXDNgQyGgSHh0ROvGGRBZ7nMcMpLpGf3xR+8PBJy4rPe6Kn\nUqlgu93G0WgUCgtQ8m2a54I3IrIY6VEEQRAK88saxzQGlNOluLGk8kmSBZ+wnGQEzGYzUbg5075D\n80EWpEMlEs61ONE6Vwmxk6Ga4fC+xIs/DOjqBcTz/Lu8dRaBIke2bUs7PHhvmcDTL7Rt1mlEpQpj\nNpuxhSZrVdKmlLF6QjhICouGgS//1+QVCypJT3p45ZlknNCC63a7SRNbW1aI54aLTLUGbzxncP0m\nYjqdqrz/EvicjoQFJ5QV74UiA1M3hOH7fmiMyc0fY+QrjyGvjHQgaYglctLwMqPneTgcDnE4HGKz\n2QwZJlGjiVz3MetYWlbT6ZT1SEt6SqVSyDhIMwyTikNUefHwfZ/pLNu2lQ8PtOlp9N1KBMks7nDF\npyNQxVWtVluqHs2w2aVCx5AiD7GEbDPtOSpjR+tHop+btKzIi67RyiAEkjHlICUYiJn2HZXwHuK5\n00Eif0to26zDeJJS4GQZqli9tAAzthkQDlKj0ZBZyCEkJZkq8IoFnz8mepI2JJpAmjFhIWhBS3qZ\nmNEpsQiU55WixysE3mPWbrdZKwdJToxXNDRHRmaGMAaOx+OQy92yLOx0OryBpjWGCkUSS4ieClW9\nGQrhcIa4CifLsrBWq2G32829XxNV/tHGGwQBjsdj7HQ6iW0j0nQab0jFrEUtXUrGZLFYVB5HPhSY\npyGVZqzIJnuvypCi6IbIg86Dxi2PKtU40FpQOfzx81KTUyIvydzaVNDhYlU5gSrhPUpxyWgMs8dc\nEWNgYGBgYGBgoAsZayvHJwTKN6BTsOr1HGSBr7K0EvH8lKbSz4ZO5xL5Hkqc+NLYuN4n/JMUj161\nR0rHsxBNBk84SSvNKzqZ65yi+GR4QRKqtKwGgwEOBgOVPKdU+L6PtVot5O3iTrBaY6jZ7JaBD7cn\nhMITOSmc8hn49iSFQkHWAyXLaUlWfGgv6eQ/nU6x1+ux/JK0Tv58P56Y9aI8hqQfFK57QcSzfBHy\nvEuMgfK8ilai8ZhOp2weRxsp8qHsVXmkSFerXDu2ao+UqlcfMZc9J5EXX3Gpe/MCn3+XUPSTad9R\nyZOi8ZPcG4S2zTqNqJAwoh12acNScUXz5boZylCFg0SZ/aoVAfS9NCsVYiGbyJ0GPilxFXePKeQP\nhECTm5KyY8ZUihNvXEh2ag6BTzCn8E0KlGUVl+ekE+YbDoe5d+vWbHbLwBtiika6lgFOOkCjr5wM\npyVZUX4HKeys4HuFxSRaK49hTGsHITRyqpTnFa+rVXuUkXEu+D7aOovGU6XIgQ7XEgchLU4qeaY8\nSMarqI5T7AkVi2g+XB68eMjkSdEeCvIpDELbZl0G1JIwfN9nJ/RoT4xSqYS9Xk9qweXQUVk4SAqx\n5xAkq/2UOSkYj7EgI4HyPRQ4SU1onSpHRAx5J2nBRjZGKU5x14uoGFNkhJVKpSyd6aXmFe+diuQ4\npSKaK5dHfg1iuPRaJyGeeJFXNAaJnHTK0BEx1LxUs2WCsqwUC11iwbcioXYkWXkhnm/yMnLUzKnS\nmu+kqyUvt0bEcOm8JicpnUXrSBa8V1+TVyoob6tYLCp1MSfdvoqGy7yRrnNf6mg0WioGifDMNIaI\n55GNtLnP92+ThNC2WZcBJRSG67rYbreX2vKLXOPkLcpQHac0qUUIgoCFNWjS5JlAjXie/K6TQM03\nTtTsmyGEpCcOEc/c+t1uN3Qy55/IiUyJE18JKhs25q+wkTRUM8kq6p2SudCWN6JWEZ4lZaxjJNDp\nnoyDGJkncuI7FKfMzSWIGl9KQFlW9HmqHgMefAJvgv7SGkN+DNIOo9PplK0PyeuQRLxSwV8ULhO+\n5YtF8izpj/1jRUNqHe1torqQ72Ke1Mk8rW+ZBCchL4WeUAyTySTkLKGIQ15VqjyiOiSJv2LoW2jb\nrMuAUhLGeDxGx3GW7hyzbRubzWZo8ogEJymMVPANJqkbaqfTQcdx2Gk4qewaQFh5ocyJjzWreqUk\nrXFtWaUZnVRK2+/3sdFoxLbsL5fLrKpjPB7L9mxKRNSYSjvZkevasiyVXJvM8x3xbFPj5xBVCUbn\nNG9EraonC23wmkZJKFyv0GoAEc9d8wqXfoeuYtEMOyjLipoVSqzvWPBtOTRyAoVjSBtq3ByhdhG6\nHltdTojhe9GazWbieuQ7+ksaq5nWIX+JeVreaVRnZbi3VIher8c6mSftLfTQnZj02lXpBsRwe4w0\nzGaz0AG5UCjo9phT0qUkgzhjUpa7Ai9A3FBDikDhv7iJVCqVsNvthu4sW+UFoKJ7eqKTmr8QUZNX\nKsgrJdsdG/HcULBte2XN7SgMSkYL9fdJugC0WCyi4zg4HA6zXBCcitlsxja9JHnxTTsVw0O5zfcg\nCELl5zTPyaPJb0KrDGWTLERGCfU063a7bAOKlv/HtIxI5cT3m5E1aMl7lSFPSktW/JxRSZDnjftV\nHWhoHUZDYtELXnP24Emh3++H5knUWOHbeigYeZnWocwFxXHPKg2pKPgu5tRuI+kuvrwKZOJAazRp\nH1ksFqE+arZtY7fblRnHXHRpUniPv9cxp4MDe9ZlQGlPHgJ5M+LCf6tWSohnVi4t9Ha7zVyrdBpe\nwdUGqeCr0srlsvDzeQWa1xUCceCNtejiJq9ivV7Hfr+vIzPt8Uu6moWqsEixa1TR5T7fyTvFe6j4\nw4Tkpq3Ni78U3HVd5oVttVpSF4Dyt6vHyFPIiU9ItW1bugAlQ56Utqz4PlaNRiPVQ+z7PnY6HSbb\nDJuw1Nyi+TMajZY6nWucymV4SWM2m4WqBOMOyop5OJlkRVEGvmIw7tHIG8x9L4wi6S4+DU7SvPie\nUIhn+tVxnNAcs20bO52OysEmF10alyKQMb9LaNusy4DKffLEhf8EOUxr4aWBTMYBv9k6joOj0Si0\n8Hu9Xug1q96E+U2QTpsKjRF1eUkhakxNJpPQNR4qSbASnDLNK1r83W5XpbtzbrySctbiDKZms8ku\n8s7DqxgEQajy0rZtqfBZhjypTLLiLwWmOU/eaHqiDVkTmrvmyosONXwyvmVZedxjmNt85y+QzXgo\nvXT6fYXILCvKd7MsC5vNZmj+W5aFrVZLuTozD14EPrxHqSIyXnYNXoC4xYYUIRr+I89VDC7lQvN9\nP9SLJu2Et468n06nk5TflAcyjx8fCuafDEnDK59Xi8UCK5WKUvgoKy8ySuKMJUmDSZVXCGRI8nO7\nWCxiv99P9ProXPUh4CQ9hp7nLR3s4p5arba2/Dves0ghDd0rPiR5XSQupX5fEXKRVfTQTB6pFegG\nZXlRBIbSDWgdrKDPHCCazuYGBgYGBgYGBtr4ABHX+Xkr/bCjoyN4//49AADs7OxEf/1Byp+uVQgR\nJPFS4nR0dATffvstvHjxAoIgYD//8MMPoVqtQqPRyIOTMq+ckYusTk5O4Fe/+hXM53Mol8sAAPDf\n//3fcPXq1Tw5KfPKGZl4HR8fAwDAtWvX8uJDUB7Dg4MDuHPnDrx9+5b9rFwug23bode9f/8eDg8P\nwbIs+P777+HKlStZOaXySsLz58/h+fPnoZ+Vy2X45JNPVOWZmdfdu3fhyZMnUC6X4c9//jPcuHFD\n5fNVeW3tfF8hLq2sjo+P4caNG1Cr1eDRo0d5zK3cxvDo6Aj+8R//MfSzer0O+/v7efM6e8FlMqQE\nMAtNHpdeVicnJ3Dnzh149OgRAEAWJbBtsgLYTF5CTnt7e/DXv/4VxuMxvHv3LvF1pVIJ9vf34w5T\nqpykeK0QmXnN53O4f/8+/Nd//ZfuQSEORmfJ41LL6vj4OM/DVq5j+Omnn7KDFcDZWlDQCbK8zl5g\nDCkA2Exem8gJYDN5bSInAMMrDrmM4cuXL0MeKsLu7q4Gpa2TFcBm8tpETgCbyWsTOQFcIl5ff/01\n3L9/H1qtFgAAOzTnzOvsBcaQAoDN5LWJnAA2k9cmcgIwvOJgxlAe28ZrEzkBbCavTeQEcIl4HR0d\nwccffwyLxQIAMqUpbJwhZWBgYGBgYGBwaWCq9gwMDAwMDAwMNGEMKQMDAwMDAwMDTRhDysDAwMDA\nwMBAE8aQMjAwMDAwMDDQhDGkDAwMDAwMDAw08ZM1f96lKa3MEaY8Vh5GVvLYNl6byAnA8IrDSsfw\n5OQEvvvuO/jZz37Gbh7IwAngEstKE9smK4DN5QUA6zeklPD06VN49uwZHB8fw+vXrwEAwHEcePDg\nwYXyMjAwMDDIB8fHx/CnP/0JAAC+/fZbmM/nod/X63Vot9tQqVQugp7BJcLLly91u5unYiMNqSdP\nnsDDhw9jOxcbGBgYpOHu3bvwm9/8ZhX3BBrkjEePHsHDhw+XrvypVCpwenoK8/kcnj17Bs+ePYPB\nYACO41wMUYOtx4MHD+Ddu3dZOpwnYqMMqdPTU7h79y7s7e0BwNmdWV9++SX87//+L/uZwRmOj4/h\n2bNn8ObNG5jP5/DJJ5/AlStXoFarqbjDDQwuFR48eABPnjyBzz77DG7evHnRdAwSQHddPnv2DAAA\nqtUqAAA0m024efMmu3D6+PgYvvnmG3j8+DHcuXMHCoUCAADUarWLIW6wdTg8PAQAgIcPH0KpVFqJ\nIWWSzQ0MDAwMDAwMNLExHqnT01O4desWHBwcgG3b8PjxY+bGNTlR5zg9PYWHDx/C119/Hfr5wcEB\nAADcv38fSqUSAAB8+eWXcPfuXXa6+7Hj6OgIbty4cdE0DFaEvb09ePjwIQCc5UIYLOP09JSd0KMX\nPxcKBdjZ2YGPPvoIrl69ujIOJycn8Ktf/Qrm8znYtg17e3uJ3sNr167Bo0ePwLZtePjwITSbTcb9\nsuq1g4MDpt/5hPsrV64s5YlpXtL9o8HJyQk0Gg32/1evXq1kH9gYQ+rrr79mRtTz589NeCoBZGwC\nADQaDSiVSlAul2E+n8ObN2/g4OAAXr16BQAA9+7dg2+++Qb6/f6FhDmePn0KL1++hEajsZIEPxVQ\nLsZisTC5M5cMtB5okwUA+O677y6Kzsbh6OgI9vf34T//8z+XErmTUKvV4MsvvwQAyFV3nJ6eMiOq\nWCzCZDKR2tQePHgAz549Y/wPDg5y12lULRiH58+fL/1sPp/D999/D9euXYPhcJgbj/l8Di9evGD/\np/CnCBQeBQD49NNPwbIsADgbvx/rfnrnzh148+ZN6GcvXry4nIbUfD6Hhw8fgmVZcHBw8KMd9DRQ\nXPfg4AAKhQKMx+OQnHilwnun5vM53Lp1CxqNBvzhD39Y6UkzCqq6pNg0nQzWbVjN53O4d+8eAADc\nvn07VilGQZvPV199tWJ2m4n79++DZVnwxRdfbLQXbz6fs3n1/v17qNVqcHBwcGEeqf39fXAcB27f\nvg1ffPEFAMCFVZsdHh7C7373O3j69Gno57ThRtsL/P3vf4fFYgEvXryAg4MDpkcqlQr8+c9/zuUA\ncufOHWUjivDrX/+aGVLPnj3LzZC6fv360margrz3q7///e8AANBqteDnP/85nJ6eAgDAmzdv4Ojo\niL3u3bt3IcOYN774f+/s7KxlT43qVfr/+/fvYTqdsn+TRxQAoNfrrUzHPnr0CJ49e8by6m7evAlP\nnjyBFy9e5F+0gIjrfGJRr9cRALDVasX+vtPpIJz1kcBOp5P0NiIo8+LR7XaxVqthr9fD2Wymy0GF\nVwilUglLpRICAI5GI+k3HwwGaNs2AgCWSiV0XTcLJylZEYrFIgIAFgoFNn70lEol7HQ6uFgsVN5S\ni1O5XA59drfbTX2967qMu8RYp8pqsVjgZDKRlXueyDSG7XabyatSqWC/30ff91fJSxn8OAEANhoN\n9H0fAQAty8qDkzKv6FwDACwWi9jpdHTmgBYv3/fRcRz2+ZZlYaPRwNFohEEQCD80CALs9XpYKBTY\n2i0Wizx/LVkNBgMEALRtW3XdIyLibDZj36lcLkd/rT2GNIeKxSJWq9XYp9PpLD2tVgsBAKvVatrb\nK3Oi/VBFzyMiTqdTnEwmOJlMQvNwOp3KcmK8ZrMZe6/JZIK9Xo9971qtxuRiWdbSfJd9CoVCdD7m\ntg6n0ynjxn8PGmdFCG2bdRpRS8JYLBa4WCzYYvc8L/Zb9Hq9CzekaBDosW0b6/U69vt97Pf7WopB\nwIuBNgeSk4wy5OG6LlYqlTiFqMpJekK7rsvkhHgmv2azic1mc8mwIqMqA69EjMdjtmjp35ZlxSkX\nRDyTNb85xyhsWU5AciBDNrq5kjJyHIcpqcFgEFr4GQywTGM4HA5jlV+9Xsd6vY7D4VB5Hgp4KcH3\nfbZZVCoVrFQqjI+CESzipMRrNBqxuUbzPDrXFY1SZV7T6ZR9pmVZ2Ol0EvWqCL7vo+/7THfU63UR\nr0R4nsfWwWAw0ObD68EItMeQNlzV+UzjXavV0l6mzInknaSjZMDrsBgdIpRVdL+TeYrFolCv8QZe\njOMkl3XI6/B2ux36Hc1BRb0qtG3WaUQtCaPb7WK320UAQMdxEr8FnWQu0pAKgoAtOH6SRi1sx3Fw\nMBioDJSQExklmtY0488bUwJFnnlCk5LhlG8IZFjxG81wOBS9rTKnZrMZmjd0iiyVSkuKk9+cy+Uy\n4zaZTHQ4MV6z2SzWmNI5wZGSajQaodNxv9/HyWTCK99MY0in/2KxiMPhkJ2So4cJx3FE8pGVlzSC\nIMBarRaay/x8Jq4S80nESYkXzZ1erxf6+Xg8Rsdxlk7v5CXKg9d4PMbxeMw+o1qt5uYF9TyPrYUf\njFNlWdG6ExgdQvDyi/5Khxf/nqqgfSlt79LhRLLWNYARMTTXFDiFXkxjRk+73Q7pmslkouRAIJ1i\nWVaS4ySXdUjrnz9cRX+naMwLbZtVGUxSwqjVakwhpim9TTCkEBGr1SoCnLlcPc/DwWCAjuOg4zix\n4atisYjNZhPH47EOLwbekCoUCkpfmgdvKCQZOAJO0rKiRSgKoyGeK45VeKQoHEreiSAI2M94BRgE\nAZMNbc4UUm42mzqcQrwmkwlTILShkDLqdrvMIKrX66GQQty8Snosy+LnWqYx5A8OpIw8z2Me2GgI\nq1AoYKvVklGsmRUlha1s246dMxSWjJ5GNThJ8+K9UUmejSAIcDAYMJ2XcCpX5hWdW4L31EJkPSvJ\nyvM8xi1LWsQqPFL0nuQ5VwFFSvI2pHQNO4LneSEPkQKn0IeSriR9qbCeYsGn8CTM0czrkMYkSTdI\njpkKL0C8YEOKd32nbaKbYkiRgk5SVIvFAvv9Ptbr9ZAHIqshhYghRZnlpMKHmgaDQZJlnllWZHSK\nvBVkJEoaiNpKid/cFosFk+VwOMThcBjycNBcpLCzwAsoLauo10DGyOThui4zvobDIZMxPZGxzDyG\nZCwlbX6LxQLb7faSh7ZUKmGv18Ner6dy6pQCGbdp4VkKSwoOCzKcpHmRtzfqjYoDfzIXrGUhLz5k\nlsdml4RI5EBJVrxng0+JUE2HmE6noTkWgdYYkv7R8fTTXBTsSUqcsvAh0PyikLcCpyVes9kMZ7NZ\nKN9IB2Tc0ZxPmPeZ1iHPM8nTy3vaFSC0bS60ao/vYSJbvfH8+XPpvlJ5Vxx98sknjEMcdnZ2YGdn\nByqVCqvU6fV6uXThpfd78eIFPHv2LFTqzYPnNpvNQlcvvHv3DmazGfz0pz+Fd+/ewf379wHgrIou\nz54sfK8aUcUSlRuTbPPEyckJAADYth36fjs7O9DtduHevXtw+/Zt9nPbtuHg4IDNmZ2dHbBtG968\neUUQUBIAACAASURBVMPeK0vVI82Dvb09uH37Nty/fx8KhYJ0BcmNGzcYt/l8Hqp+6XQ6uVei7Ozs\nwHw+h5cvX8ZW/ZAcu90uHB4eQqPRgDdv3sCrV6/gf/7nfwDg7LqWvEBXRwGcVYQmzS2qCF1X5R6N\nRaFQkPq+pL/u3r2buRLum2++gXfv3rFKvG63K/yb/f19AAD46KOPpPUj6RGqgFJBtVqF09NTGI/H\n8ObNG3blC71frVZj/KvVaiInfjzzrvrV0X/v37/PlQMAsOrB69eva78H3Uv785//PDMfWvftdhse\nPnwIjuPAbDZT1oPffPMNAJxVTa+i/czp6Sk0Gg14//49tFqtxIrOcrnMdHqu/aRkrK0cn7CZlxzv\nDoH3SKk8Ees586mTdy0n5Rh5nsdO6IKQkIhXCKPRiIUP0hKg43JZRE/MKTbzyUDEk6ASAlTlJHLb\n8x4d27ZjPS/0mul0muQB0ZIVP6dVq3N4ryKFlvPixYO8ECIvRxAEoSqxPE/oBCoUAAmvTxAEsZ5I\nDU5SsqI1J+MtV/BGCXnx4VfyHIjAV2PCDydzx3GEHiIK7/zgXdeWleu6LCUiLmydlkPFz7GYOaA1\nhpRULai8S+XT7/fTXqbEibypiqGnEPr9/kpCaOR1VeWmENrVXoc0FnG5r1HQepXxHkvwAkQ0V8QY\nGBgYGBgYGGhDxtrK8QmbeZIeqfF4nNjfI/rwuUQRD0LmEzried5InBchCALmwahWq1lPw0vvHQQB\nO8UlVR3wvVYAIFTdNR6PQ6X11H5CgZOUrCihTybplU45krF3ZU7kuYnzIFI1UqFQSPx8Ptcr4TXa\nsuLzfQR5dAx8aa9m2bX0fBdVXhIfkpFlWSupvORzH2QTqWmdSpaQa8tK0cMk7Jmnwou8dDKeX8Rz\nTwXpyDiPUKFQwEajwTxUi8WCeUm4RPrMepSwWCxCPJK8s0EQhHLBdEr645CHR0pQAabESdYLnAby\nOlJVvAKnVFm5rhvKLZUFRR0kcha1ePHefZm8O9qfcsqhBES8WEOKTzbPkkCd9J6RxZZ5Y0E8nxRx\nipBK7SXaC8jwigUpw0KhkCgz6vWjOFlkOEnJqtFoyCiY2MowTV6JoA01yVCKls4n/X3eoT0CKT3L\nsoShGb6ysFwui2SWeb6Lku0Xi0Wo6WpGoyUWruuy9awyl2kOCsIuIk5CWakYRjI981R4SZbfI+J5\nZR+tS1qbVCDjOE5iWxd6uPWcWY/yvGj+pBWckDGXRwJ13Ps2Gg1l7nwVdwqUONH+IjlvY0Fznwpp\nFDgJx5D2H9u2peaw7/sqFZtavOgwTrpKZORFexxKQGjbrMuAihUGeZEkJqMUVtWwjUdSnhJfdrnq\nbt0irxflS5AsNEqOM8mKFLKonQFV4cieqHU4kddHtyScTsGrqjRBDJfzp40VX1mYNb9GFqQEo8bm\nZDJhsimXy7n2TUM8N3ApN4fme7fblTLY6GSfh+cnCfwaa7fbQhmkHcJ0eMkaAXxOncjTwecwUfl8\njBGbeV4RyBBN8Z4g4vmBhtpvxEBrDFWM0SgkK5OVOJERlGU/XKUXHfFcD8n0BFP0/mjzGgwGS42U\n02So2LRXaNusy4CKFQaFnLJsdDx4F1+MqzaXjYW/giIIAlbSTp8rG6KR4JUIvkFe2mTO4JXSlhWV\nuZKCT+uArRICFPBKBG12aR68JJChl1f7gzTQWBUKhdgNWdQ7aVW8oiEy2shojOv1uurVMVKcyAPY\nbDZDxhR/UKKrO+I2DMmu0yJOwk04Gh6rVqs4GAyWZEIJtwreKCEvvnVIkmeVN0Y1vdPouq7sVR5K\nkCyJXwotJugTrTHU0EEMtBkLDHslTuRdydJviz/IJuiKTLqB33/SPGd8KoqkYZhZZ9HVRvx6jNMP\npE8lE86Fts06jahEZRmJv2uDb3SXV1VHHPjTkW3b7LSn2hdIwCsVfLdsumMs7jUkjxxb4qfCdd2l\nZo101xe5mmmcZUOAErxSQXxUlSV5EPJoyClCtFs3rwRleietihcfIot2OtY8/Ghx8jwPR6MRtlqt\n2PvseMXZbrdDIfAMnKRkFde5nOY8dS/X8EZJ8SLvAzU3jupQmlMSoeA8eCmBwtppBh6/aQs2Pa0x\nlOwFFQtJz7sSJ3rPLKkuNAc1OEmPIRm3lmUlRmDISM0h4qCsS7vd7tJtEtVqlRmo5HTJwVMGiBds\nSBGSrlZQwbrazyMut86Xca+nQJsTb0yVSqXYTVYxuVXESVpWrutir9eL3fRok1HoaC7ilQreoJT1\nGPIJzqsq2Y0imgNVLpdD90wquvtz4UUhMpIFrS/d+9Ly4IR45mkhA4XyZpKMq3WFQYMgSLxOh/e8\nKEDIa7FYhDaMSqXC5ivlbCZ5OTMgF1nJXMHEX/ehySmVFxlzZEjxxTj8MxgMli4tpjWRpyFF80QX\n5OUTHCByme/k1Ym7ikXDG5UbLwLdTsEXoJHxRFEkyTwpoW2zTiMqURj8l9JZ8PwGtMr28wQKG9Ak\niptICshssPCbSKvVCrnJVauKBJy0JjRvVEUNK8WrGbQ5kcK0bVvo1XFdl50MJQzQ3Bd/nPGpkXya\nCy9+rtu2nZZ3kZVXZlCovd1uh9aEhPGc6xgihq/TIS4aHjwpXrPZbOmCZArnKXoxs/KSBnk00rwV\n5C2SNAS1xpDvTaX7aBZ9hEAe6Oh7l8vlxPs1u91uyNjzfZ9FeASGZy7zna8gjnr0ZMZ3VbyioP2w\n1Wot3XcpcVAW8QLEDTGkEM9DCOVyWTXngi2GYrGYeww9Dnwb/7TYviQycwqCINRojyxwKmPWCG2t\nZEITeMNKMW8jEyeaYwBn4bqocqa70GhDkgyH5C4rMuSSbjCXRC68qNKsVCrl5dVY2byKggyrVVUL\nqcB1XWW9psKLcqRarVbIQ5VHEY8CL2mIrtQhI0rksZLglMorakjxhgv/OI6z5JHq9Xoy3KQ4UauJ\narWaywXnq75LlUBVlwDhymbac3K8HDgXuK67NOYSKTlC22adRlSqMHjrtlwuS1W++b7PNkeJk1eu\ng1QqlTIlBErwUsZisQgZCxlCCyuf0JrIzIlX0GQMF4vFpdBQtVqV3fhWIis6oWboJ5Mbr1qtpmME\nqPK6SGzbfBfq0tFopNTrJydeUuC95HEhIQpJKm7E2zaGUlgsFonhRcdxmKEXLcjI4zJsWVCqC11w\nHNN3TBZrG8PFYsHCxhI9xIS2jelsbmBgYGBgYGCgCxlrK8cnFa7rhmL77XY71jPl+z72+30WgqHc\njQxWpTLyaiCaJycChRt7vR7W6/WQq3iVfXXWgFw4kXs3zoVeLpdVT/LbJqtN5XWR2DZZbSovKZDH\niddFVNFLUQnJLvkynLZaVjrwfV83AV6LVxAEsS1KNIrH1j6Gs9ksttpVgRcgInyAiGu120QvODk5\ngXv37sHe3h77WbFYZLdhv379mt2QDXB2m/NwOJS5DfyDLLxWiCRem8gJYDN5aXOaz+cAAPD999/D\n7u6uzltsm6wANpPXJnICMLzioD2Gx8fHcOPGDXj//j00Gg0AAHj+/Dm8ffuWvaZSqcBgMJDR6TKc\npHitED+K+T6fz6FSqQAAwPv376FQKMDR0RFcuXLlQnnlhDReZy/YNEOKcHh4CH/84x/h4OAgtMgI\n9XodHMeBmzdvyr7ltg3SJnIC2Exem8gJwPCKgxlDeWwbLyGnR48ewb1795Z+XiqVAACg3W6D4zh5\ncpLitUL8aOb7gwcPAADg4cOH0Ov14KuvvlJ9i20bw/MXbKohxeP4+BhevXoFAAA/+9nPoFwu67zN\ntg3SJnIC2Exem8gJwPCKgxlDeWwbLymP1KtXr+Dw8BBOT0+hUCjA7u6uqvdJhZMUrxXiRzffd3d3\nYTgcwrVr11T/dNvG8PwFazakDAwMDAwMDAwuDUzVnoGBgYGBgYGBJowhZWBgYGBgYGCgCWNIGRgY\nGBgYGBhowhhSBgYGBgYGBgaaMIaUgYGBgYGBgYEmfrLmz9vUEsZN5LWJnAA2k9cmcgIwvOJgxlAe\n28ZrEzkBbCavTeQEYHjFQdj+YN2G1KXA0dER/O1vf4OXL18u/a5SqUCtVrsAVgYGBgYGBgbrhjGk\nFHBwcAAPHz6Ew8PD1NfZtg137tyBf/u3f9NpSvajwunpKezv78P169fZFQMGBgYGBgZZQM1fZ7MZ\nAADcuHEDrl+/rtvQOxVb0dk8J2i7DU9OTuCf//mf4eDgAAAALMuCWq0G5XKZDcrLly/h3bt3cHBw\nwO5vs20bHj9+LLry4Efp+qUrgEajEbx79w7q9Trs7+/r8rrUstLEtvHaRE4AErxOT0/h8PAQXr9+\nDScnJ/Dxxx8DALCDgeJ9Y7nxksXJyQn89Kc/VeF56cZwhTCykkcuvEQOD3J0/OY3v4GrV69m5fUD\nO4mbjXN8LhJavGazGbuR3LZt7PV6opuicTabYb1eZ7dgO46jw+sikfsYuq6LnU4HO50Okyf/WJaF\nnufp8rpIXKr5vgZsE6dUXqPRKLTOk55SqYTdbhdd110LLxX4vo/lchlHo1EevC4SZr7LY9tkJcUr\nCAJ0HCe0p1SrVXQcBx3HwVqthqVSif2+UCjgZDLJygsQ0VTtGRgYGBgYGBhoQ8bayvG5SCjzms1m\naNs2AgBWq1UZj0kIo9GI/X2z2cRms6nC6yKRyxj6vo+DwQCr1erSKb1YLLJTOv2+1+vp8rpIXJr5\nziMIApxMJuyZzWar5pUbptMp452R0xIvz/PQ87ylOV2pVLBer2Oz2cRqtRo75wEA6/U6LhaL3Hnp\nwHVd5h1utVoqf5orp8FgoPunMpy0eLmui5PJhHnRO50O9no9nEwmqp7FteiGyWSCxWIRq9Uqtttt\n0Ry7dDqLPKvwgyeq2+2i7/uxr53NZlir1dhrx+NxFl6AiNthSPm+zxTjeDxmE3symaxEKfm+j77v\nMyVTq9WE4bwkTCYTtCyLKdIYpZH7hB6NRjgajXAwGKjIR4aTFK/xeIyO44S+t23bzJicTqeh1w8G\nAwQALJfLuryU4XkeDgYDxqlarYZCjuVyGRuNBvb7fZEBfemUEuLZGMYZAqpPzPzLdQz7/T42Gg1s\nNBohtz3/WJaF9Xodh8MhDodDbVmR4UHzpFAoYK/XS1TYhNFohI1GI7Qe2u22jE5ZydyazWahlAXJ\ntSfDSxmdTocdVHUOqxKcpHnNZjN0HAcLhYJwXheLRWw2mzJG1Vp0AxkR/EMhrZj5uTad5ft+0pqL\ngzYvMoyKxaL0nkdzz7KspT1JgRcgbrgh5bpu7ARJ8m4IFJo0r1arha1Wi+U36BpRBDIUSPlG3i/3\nCU0bCy8fxVOf8hguFgtst9tLSqhWq+FwOEyVYRAEbJMReD5ykdVsNlMyBizLwna7nbT5rU0pLRYL\nlcNDJl6kZOiUm+RhSXps20bbtlV4SWM0GjHFGfeUSiXGmTzC0d9HZCglq0qlEjI8VDd9z/OYXqH3\nWLfnYDqdsrEhA4bWnsgglOClhMlkEjtvND1U2rKaTqdL89u2baxWq2wv6HQ67MDFzynSDUEQJOm4\nleuGXq/H5vV4PMZWqxXiWC6XZfec3HgtFgtsNptoWRZaliX7Z1q8+v0+GzPVPMToPq84huxZpxGl\nNUg0IcrlMpbLZaxWq9jpdLDVai1Natu2mTdGURgM0U02r5BGrVZjyj8Swsp9QvMbHy+fer0uaxQK\nZeX7Pvb7fTYu0Y2q1+spbTTNZhMBhCGG3OcVKcpo+GoymeBgMFjasGO8k1rznQ8/9Xo95mWt1+ts\n/KrVauzpWFC8kIkXgeaPYhIyjkYjNtfq9boKLyEmk0nImKHP6ff72O/3E9eq53nY6/VC3qSI0hXK\nipR1qVTCUqmkYnQsYTqdMu+ZbdvMQxSDXHXpeDwOecVoLmuMdWZOnuexud1sNpn3hOemaKhqb8Ik\nE9u2sdVqSen86XQaOqySflc4aOUCXo78+Lmuy+YqAGCn05HhlIkXeXyjaxQg1jMdB2VeQRCw76/g\n+Qr9Pe1fvV4vKb1EaNusy4DSHiSqihkMBoknlfF4vHSiiMmPkOJFGzop6bwwnU5xOp0yr5QEL23w\nG4brujgcDpnhIPmdhLLilQh9p2azqW148rJZx+lOZl7x3AqFAluwEWNPal61220lj07asw5DiuaL\nqsFAJ7xut4vdbleFVyJ83w9tsoVCATudjrJHiOYVGcfcWhDKivRLSnhQCb7vMx7kIYo5TeemSweD\nATMYyGghkPdRIU8qMyf67pVKJbTWeV2l6J1SlhV9b4CzUKuOcTydTkMh0lqtJssrF9B6i9Pr/J5T\nLBZlOCnz8jwPO53O0oHPtm1st9vMSJE00pV5kTcuy15Nhz9yCijyAsQtMKRIUNHFHwd+YSiE0EK8\n+AmRY4ItA00sztDLfaGROxUAmAxc12UKKkNSN+NFOTRkYOQBOj1peBWVoTKvEMNKKdKqQXq+R0Ou\nFIJqNpvMIzUYDBKTvClE3G63Zb6i9jokr2ypVJL5nBDI4EhJ9lbiRCdrUs6dTidzqN3zPLY+fjBe\nhLKiNZVi5CuDN+pWGYIhb1rS3KEQm0KeVCZOtPaSQjGe54XkkpDnI8tpiRcZ+bSWVb2uUbiuGzpo\nrTriQJjNZmxepnl8iJeOzkrCZDJZOkyTITIYDNg8JseExJ6jxYt0g0TCeCJ4r1aC90xo26zDeMo0\neUipk5dFBD7UFDnNCHnRRkku0VWAPBOcQst1ofm+HzoV8BgOh3GnkzhIjSE/+QTJelIg5baKsFAU\nqvOKQAqe80pIz/eoR6RYLCqdgkk+ETd9ErTXIW9kqoBy3QQGhxQnCnfxoel+v6/EJw0UfvjB2EuV\nleu6sutGGb7vh0IwEUMnsy7lD5cJHsJQjqLkfNTmNJ1O2WeJDJjBYMDGv1AoiDZLKVnxxoeCp0QI\nOvytI+JAID0i8iTy0QkBp1ReQRBgv99fKupwHCdR/5MuSahYj0Kal+u6bF0m5GJKYbFYYLfbDemZ\nGKNPaNusy4DKNHn4LylKJqM4Z8zgCXnR35HnYBUgY0YirJAI3/cTwxqLxSLk8YiCFpWgNFxqDHkl\nLblQUuF5HjslJjTovLB5RaAQE2ekK893vlQ3xguRCJL3qg2pmO8oBTqICDwbQk5BELDctWjzVtu2\nsdFo4HA4zJSnRIbUDx6/VFmt0pBCROa9y+LtjAMfDhWNpWKelBYnvhJaNozoum4o54Y8yLpFH6tK\n3SDQ9+M8ybnqLMRloy1pHVD1Oc0rDtLzarFYsMRxXl8WCgXsdrvC8DpFLiqVisxXk+bFy0B1HMfj\nMTabzdjG0AkHDqFtsxV37e3u7sKzZ88AAODFixdw48aNxNdKtnyPxf/93/8BAMBHH30k9fqTkxP4\n9ttvAQCgXC5L3RVXKBQAAOD777/XZAnwt7/9DT7//HP2fqVSCX7605/CJ598Am/evFn6LB63bt2C\nx48fw+HhIezu7mpzAAD44osv4OHDhwAAMBwO4dGjR1muw4Br165BrVZjV/E8ffoUvvrqq0wc06Ay\nrwgnJycAAPCzn/1M+3OvXr0Kf/nLX+DTTz+F+XwOt27dgtFoJJTd+/fvAQDg+vXr2p8tg8lkAgCg\nfCfVdDoFAMg8r65cuQKPHj0CAIBHjx7BfD6H/f19ePbsGcznc3j69Ck8ffoUAABqtRrU63UAAPjs\ns8+kxvD4+BgODw/BsizY2dkRvp7e8+3bt3B6eso4ij7jxYsXAABQrVZT79wkeTmOA3t7e/D73/8e\nut2ukFcSTk9P4c6dO/D06VOwLAv29/eFF6nv7u7Cixcv4MWLF3Dz5k3tz07DnTt34M2bN1CpVKS/\n340bN+Avf/kLXLt2jc1/AP1rd4bDIfv3119/LXz93t4eAADU63WpveXTTz+FN2/ewHw+V1o/xIV0\nH+HatWtQKpViOQEAdDqdRF60NwEA/PKXv5TmAnCme//0pz8t8alWq9BsNqHRaEi9D+2li8VC6fNF\nODo6Yv/+8MMPE193fHwMf/3rXwEA4K9//SuMRqPQPLJtG27dugWfffYZfPbZZwCgaUPIWFs5PlII\nggBHoxG2Wq1YN2Ia+OoPyVg18H8LIE5A5ks7iZesR4ZOntVqVcQrEXw7hbQnTlaSXg3pMeQrBPNo\nrEceO4j3bGQ+3bmui71eb6k4QTSvqBEj/HC6406B2vN9sViwU55MGE3RU6TFi06wOu5ySuAXJGNn\nGsOk8SPOMp5FkiPndRbKirwi4/E4NsQUBAErPY9WscqemGMSg5XHMJrELtuUVDFPSnkMRXlRaaBc\nnAwl6oh47q2XzevkPXqUnyfyghJXCY81A+Xs8fuJzJM2VpT3E1fRJ5JVv9+PrRau1+s61xwxHZfh\nCrCluUU5paK9jOYd/5RKJWy1WirpKELbxlwRY2BgYGBgYGCgCxlrK8cnEZPJBNvtdmwPCsuyWJ+O\ntNO467ohy1616V6aRyoIAhwMBrH8QCF/Ig+PFB8b5q/y6Ha72Ol0WI5XXHJu3h4pklXkO2kjCIJQ\nw8DIqUHLmzGbzbDT6cR2vqbEdlFJO19xF/EwaM13Ap98K6rGo/kpWX6vxUs37wDxvPhAcGrVlhUP\nPmmf5ouoypa8SaRTOP0glFVcIcR0OsVutxvrHSM9pJrUTPNetpqQQPkw5A0rFApKVceKCedKY6iS\nYB4F6RZRZVoKJ8aLz+ETed74xqn8I/JM6eRI8V6TWq0WqtodDoeha2o6nQ6LAqR5VKKNX1VlRftd\nXJ/Aer2uVCVHfy/hAZKe77IeKf6GhlKppOVRE/ACRLw4Q2oymTDjKM6lWalUsN1uS7um+cZyEB8q\nEfLiB4YGJynRrt1uL10ZIeMqJMWQJdmcKs50wi+SSaXSE5pc7ZFy8kzgO8tLFgyEQBtmXEKhTrIy\nn1QfkwSvZbBE+dL7p1WmkeGQw43lieD7QKlAISE7k6wQz4wGmsdkMMgYUbxhoxr251sm0DwS6Sy+\n8CWmHUss+PYRMrwQl5sv6pb0KyScK40hXybfarWkiysWiwWTuUQ4W2oMecM7iUf06hBqnsvvL1GD\nKkvVHv++efQoIx70xOxLyrphOp2GQp1kmPT7felwp0TVrTQvvoVFWuFCWhW7AoS2zboMqCVhRO/y\norjlaDRSrsbhFSsZNjr3C1G/FbLCo96nSqWytKDp6hAZjwLiedUIt1FpbSxkIKgoTDLAIjk+cVBe\naLTIJCvKhDx5Y5FTeKmc6PqL6AZHzUJ1NhfeiEqQt5bBEgXf6yeJZ2STFUGLl+JnMMQcEFR5SYH3\nuhSLRSnDne9erXvIQlxuqloqldi8SltPxFemUk3VkIoeIKOHhlqtht1uV+qQp9CYU3kMu90uG4NS\nqSTkEwQBMwzzbEDLe1fi1hnfXyrO65JkUPGGrGofKf6wl/Ugynev12iKK1yHcQ046R5VakkQhYzB\no8pLpWqPzxW7dB6paDKcbo8YuvWaV6wJwhLyiruDjSZJkluZL12WCe/FeK+0JjR/0pXp8Mwr3Dwn\nNIFkkFdzTsRzpSfRswkRw+EJgLNkYt2mqtGmgCkFCNpKKQr+JBzX0DJSsi+CMq9oHygVkCcrQ7NX\nITzPUzKiomPIe5olOS3pLJoHKgqZ1yuisaMx/kE/SI8h6T26iDsujE0pErTBRo0ZhYRzrTGczWYh\nXmkXN9PBTOGuUylZ8WG06IGbL+IRHbpmsxkrroga16p3qeZlSPEHBoHxmYvOSroOplarheRHBk9M\n13dtXtHin7Q5QuF4VceDJC9AvEBDCjGcX0MLS9YbFb24tFKp6OZmLE3qSqXCvE+yGwoZhEmKkreg\nJdv1pyIIAjaJYy5hDcHzvFAoJM8uwTxIIWTpMrtYLFhoj2TKLUAhp2hugM7Fsp1OJ3T/Vl5hUBnQ\nBkLhB35cYxrr5cpLsg9ULMjAkTDytGRFYXTilzaHXdfFVqsVGkPNSkKtMYwDzcs02fq+HzVkM/Hy\nPA+Hw2GiYUWbEN1fynszND3WQgRBEPLsxV3crJAXJcMpxIv0JulOmkt8tbBK9TEZVHHrVcCLgfeS\nqXqCEZcPDKs4KKdhNpuh4zhLlYfFYhF7vV7SNTW58KJxTBszPmqkGTER2jbrNKIShdHr9dgAWJaF\njUYDO51OKOmOTlE0aXmPUbfblTF4pHipbrwECtklDRSftJfXFQJ8mINOIcPhkMlsNBqF2jTIJOQK\nOKXyIo+KajNTz/NiO+aSouO8lUJOJBOSi23bqRco08lmOBxio9FY8mjlWbIrC14p8p6XVRtSdFpX\nuHMNETHU9C/DOkyEyIiiROvBYLB0bYXjOBcyhlHw11Akee1I2avcAagCz/NYW5m4Ng38o3l4kAZ/\ncbNlWUy/K+ZFyXBa4kXeO5pThUKBfaZuZCRLJ3/eS6bSoJcOffzdhKssRBGB1mG32w152XidKvhu\nyrzIAE6TW5bmnRK8AHFDDCnEs4XFbyCip1QqYbfbVcmnWqmyJLd4XDdxihHTolV1/abB9/3EChP+\nUewBoiUrSjiWyMFCxLNFEHWPFwoFpuh1TneI5ws6+t62bbOKl6S8EvKAKZwMc59XdCM53/3c932m\nMFfFi+RFdwDyT61WW6oeoocOEZJVm0qcZrNZKB+j1Wqxz63X64melnq9rhLWXaluIJAyjxYsRD3z\nEhVfufHyfZ8ZV3yYRmBM58Ip6p2KGsCKUJaV67pLod8VQMgpCIJQ0VKpVIo1ZMkQpgpi3kCp1+sq\nToC1zPfhcLi0pwvWpDIvkl3a+FF3dkmvmCovQNwgQ4pADfdarRZT4vV6nSnPfr+v4u6VFUYuoI2O\n58e7yxNOerlwIrk1Go3QBthut3XkpS0rMlDiTt1UAROtgiQvpERIUJnTeDzGRqMRm4ROn025I71e\nTydHYSXzioxBUhK84bcqXklXJsg+kp4sJU5xeShJT6VSSfU+anDKdWNBPP8+tPFFD0GR5OC11zmf\n/wAAIABJREFU8SJQY1GBNyhXTuPxOJQvq5AXJcNJyGs0GsleqqsDKU602UcPBuVyOc9Dn4jTSuYV\n38B6FSF2cmIApHsxaQ/QuFpKaNt8gIiwRqz1wyL4IOV3ufC6c+cO7O3tQafTgQcPHsD+/j40Gg14\n//49tFotAAB29YUEr62U1d7eHty5cwfK5TLMZjM4OjqCb7/9Fvb29uDVq1eh11YqFfjyyy+lr19I\n4SUlq+Pj4xCHSqWS6UobASdpXmk4OjqCX/ziF/D27VsAALAsi11Tkjev58+fJ/7B27dv4eXLl6kf\nePPmTZlrMbTG8Pnz5/D69Wt4/fo1+9nOzg4UCoXMV9KkcBLyUsXx8THcuHEjdE0FodfrRa9EWhsv\nReSus+jqpf/4j/+Af/3Xf5W6ukeSUyZeOUBJVqenp7C3twePHz9e0pcAZ1e00JVIN2/elLoSSYFT\nIq88cHJyAn//+9/Txlab16NHj+DevXsAANBut6HT6Szp9k8//RQODw9hMpmo6ow0XgAAprO5gYGB\ngYGBgYE2ZNxWOT4XiZXzot5Y5XI5dMu44B6+SyUr6kwOkXAUwHmPr06nk3c/j4vEyufVbDZj4WGF\nGP+2rcOLxFplxScX1+t1Fta5aF4K2CZOm8pLCq7r4mQy0Ql16nDaVFlJ8eJ78VHvwPF4zIqvKKyu\nEcYV2jYmtHeGXHidnp5CoVCAd+/eAcBZGObx48fQbDZ1eG2trO7evQtPnjwBgPPbtb/44otVhmC2\nVlayODg4gJs3b0KhUAiFt1JwKUIda8LaZbW3twefffYZXLt2Le1lZgzlYWQlj22TFYAkr8PDQ/jq\nq6/g8PAw8TWO48BgMMiL19kLjCEFADnyun37Njx9+hTq9Tp8/fXXMrH+S7fQDg8P4eHDh/DrX/8a\nGo1GHnlIIl5bKysV7O3twe9+9zuYzWYyL982ZbmJnAAMrziYMZSHkZU8cuM1n8/h4OAAvvvuOzg+\nPgYAgA8//BA++ugjqFQqMrmcsrzOXmAMKQDIkdfh4SFcuXJFZaDMQpPHj15Wz58/l/XsmTGUx7bJ\nCmAzeW0iJ4DN5LWJnAAMrzhsnCFlYGBgYGBgYHBpYKr2DAwMDAwMDAw0YQwpAwMDAwMDAwNNGEPK\nwMDAwMDAwEATxpAyMDAwMDAwMNCEMaQMDAwMDAwMDDTxkzV/3spKBI+OjkT3Dm1baeUmcgLYTF6b\nyAnA8IqDGUN5bBuvTeQEsJm8NpETgOEVhx/HXXt37tyB77777qJpGBgYGBhsKejC4Nu3b8Pu7i5c\nu3YNPvjgA/jggw9gZ2cHdnd34auv/p+974dy40jOr3vP740uGukSD6MRnYC+REMnB/2SoZQc5ARk\n4puLBEYCEwu8wAdeYFCXgFYAUhEoB5695EA7wTIxqMRDRVhFgBwYYjIrJlheMitGs4rqF+xVs2cw\nf7p7BiCW6u+9eY/cxQIfqrurv66urr5deLn3Txk3b96Emzdvvm4arwUXviDnzZs34eDgABaLRVkR\nzIumdveRE8B+8tpHTgCaVxZ0G4rjovHaR04AJbxOT0/hiy++gAcPHrDrvcpg2zZ0u13o9XplNze8\nUbbKw+HhIdy4cQMAAKbTKVy/fn0veNWEvSvIWeuHkYgCAIiiCN55552il++0kQ4ODkSvR/lJDLSa\n8JO31c2bN6Hdbos4Kt2G4rhotgLYT177yAmggNejR4/g9u3b8OLFCwAAaDab8E//9E9w9epVsG2b\npYscHR3BDz/8AF9//TVMp1N49uwZAABYlgXT6RSazaYsrwtnqzycnZ3B+++/D8vlEgAAHMcRvcZq\nq7xqRKmQKr3VuOanNnQ6HXbTs2EYIn+yE15xHGO320UAyLvRXZTX68RFux38dWJntppOp6zPt1ot\nXK/Xe8FLEheJ0xvPK4oijOO4Ll6FCMNQ5bOqcsrlRX6axtNisRD+sCAIsNlssr/3fR9935fh9TpR\na78ajUbMDvRMp9PXzqtGlGqbNyJHSkNDQ0NDQ0PjtUBEbdX41AI+GgUA2Gg0RP5s67zCMETHcRiv\n2WxWhZc0Wq0WjsdjlT8V5bSvK4PXiZ3Ziu9bAIC2bWMYhhiG4WvlJYnaOEVR9FPt77XwiqIIHcfB\n0WhUF69CUATIcRzsdDo4Ho+lokCKnDZ49ft97Pf7bBwpfH+GXq+XGJMZUak3ul/FcYyWZbGoXqvV\nYm38OnnVjFJts0sRVYsxSEQZhsE6cavVEvnTrfKaTqdommZiUAk6+Vo48eHVZrOJzWYT5/O57NuU\ncdrXDq2EMAxxNBrhaDRC13WZQ7BtG9vtdl6oXpRTrbaibT3LsjAMQ+awbNtG27aztvne6DYkEQAA\n2G63sd1uYxRFdXNSmlim0yn2+310XTfxeJ6Hw+EwT/hulRcPEt+2bcssREV4FYL6bNZDNur3+ziZ\nTGRsVMYpwYvfHjcMQ3SxWwjf94u2tS7SGJTmNRwOmXCK4zghrBRE8lb7e6/XU32LUm1zYZLN6Vjl\nwcEBGIYBh4eH8P3338OtW7eg2+3CeDwue4utJLLdvn0bAAAePHgAAACu68LVq1fhwYMH0O/3YTgc\nqvIS5nR2dgaXL1+GFy9egGVZLHESAMDzPBiNRnDp0iXRtyvilOD15MkTlmCYl2zZaDRkP1uFl1T7\nfffdd/DZZ5/Bo0ePSl/bbDbhz3/+c1GNsp0kSF69ehWWyyWMRiO4ffs2nJ2dwY0bN+DJkycAcN7v\nUseya+F1dnYGjx49gq+++grm8zk8f/6c/c51XXj//ffh448/hitXroi+ZeU2PD09hQ8//JD1PUKj\n0YDDw0MZLmWchHmdnJzA559/Dr7vC538arfbcO/evTKutfet5XIJH330EQBAwk9InrRSasMrV67A\ns2fPwPd9OD09hf/93/+F+XzOErfTsCwLHMeBX/3qV3Dt2jV47733ig4Uldrq9PQUfvnLX7LvPR6P\nodvtFlEWxr179wAA4M6dO2BZFvzf//0fcS201enpaaJ0z/fffw/ff/994oU//vgjzOfzzDd59uxZ\noh0BAAzDgB9++KHooFMt/erk5AQuX74MP/74Y+LU/N27d+Gzzz6DdrsNh4eHom9XGy8eR0dH0Gq1\nAADg5cuX0Ol0wPd92bd5M5LN04nltIqg8OxwOKyqKqWxXq8TyYYAgIPBABERJ5MJAgB6nleFlzDI\nDq7rYhzHOBwOcTgcomEYCABomiYOh0OZRE8hWy0Wi9wVpshjGMbGqp1/ut1uOqpX2Va8XaiNPM/D\nyWTCDgeEYYi+77MVu23bbLWlait6X4WVdiIaxXOIoggty2IrQMGVsLC9fN9n7132tNvtsuT3Ml5C\n4CNRtm3jfD5nEVjqU5PJRPTtyjgJ8fJ9PxGNdhwHB4MBTqdTnE6nGAQBBkGA4/EYO50O638CXGv1\nWfP5PMGz1WolIgoSUOJE3zvdT6IoYjYaDAbYarVy+12v18uLLJTait/BENzFkIbruoxnCS9EPI9g\n8v6ormcX/Yrs2W63Ez9fr9fsO0lGpWrt7+PxONO2NE/XxAsQL8DWHr+Vlw7Fep4n0mlEjCGFIAiY\nQzJNE03TTPCaz+dsi60CLyEUddr1eo3tdpt1INu2q+ZtbfAiIVIkjKo4hNT2mrKt4jhObC10u93S\niT+KImw0GggALK9C1VYU8rYsCx3HET7VEscx45CVy0Fbkxn9rVJ/5xcvzWZzI58liiKczWbY7XZZ\n/6OcrRIoc0qLKPosErn8KSzJML6yrXg7eZ4nJJSjKEpwJRFRJ680ZrMZa6dOp4OdTgcRkzkuEiet\npDmt12vmI0QRhiHbJqVFjeo45AXLYrGoMzcrAVpcGoZBW82ltuL9kuu6OBgMNh5ekPNPOn1jPB6z\nNi5A5X61Wq3YnJxlyzyRVYLa+jufu9btdrHb7bIAB4Bw2o0IL0DccyHFi6jZbLYhAmgVmuOEZIwh\njMFgkJhg1uv1xoRMTsOyrCq8hEAipij6FQQBm4xpRVZSmkHYVvT9FVcgjB89vFPJWCUr2YoXUZZl\nifYXxo245KzahWyVdUS40WiU5mHR36WjUfx34ycJbiJX7u80yYtGd9brNRPMtm2X5SopceJFlOM4\nuZ/Br0Jd160aJSvkRc7aMAyl494USSaBndG+tfgsPocnS2BSH5OISklzIoGhkI+FiK/mgvF4nDcJ\nFtqKJlHBxW0lkK/5K89SW/HtUzVSRnOPaZq1RNHzQAv0vAWL4pxQmVcURYk5JN1XSGgCCB8GK+MF\niHsspNIiKgu0ShHcLqnEK91AvV6vcKuMOpHAdpoyJ371IzJhjEajRGifohkSnHJ50aTiuq4I9Uws\nFotEKDZjAFYSBoIRkwTiOE6InwwI2YrfBvY8j/Vd4jUejxN9JZ24WSa4yLFxwkepv/N9SuawQhzH\n7DuWRIOkOfGnYYtEFGE+nzO7WZYl8j2kbTWbzZTsxCOO48RJTIkTX8K+lLbuAPJTIBSiUtKcaHta\nVSiQUM9aUJdwAsRX80mVE3p5mM/nzJfyY/uv37XUVlEUsTbiIlnK4AMM24h0is47NCd0u11R6pV4\nrVYrFjAwTTN3wUzBEImxW6ptdiWgpAa/iIhCxKLJLfPlqrx4x2yappCzoQYVKMqpbCvBiSuBMAyZ\nWKEcDglOubzW6zUTaYrF2JizpFCsBK9ckANXnfDqEFLkKCkUToKJz8OiSX80GmEcx4ktO5FIATkH\nbttDqb9ThFPlhAs5WIquSK6GM8GfLhMRUYQoilh/MgyjLJQvbSsSQIL5mbngI5UZ2zGVfCl/xF80\n8im4FSPNid5fYlJNgPrAarXK86mFtiJ/XOEkMyK+2tIeDAboum5hfpOokELERBqEQo5fAiSeFfyo\nUL8inmW5RhSVEl3oV+HFp9s4jlO6YCaNYZpmlTmaPdsWTtKNJCqiwjBkq3lBKPEajUZssDiOI1qt\nnEWvBMKHSrbiE5BlVjDkXEtC3Mq2ktwiYKDvY5pm5nZpCa9clE14JFjybEj5blW29ih6QUnRaUyn\n00RkgvLuZIQpbQ9wk7F0G2Ylvtq2zRL/B4MBTiYTnEwmueF6mvAK8lCEOKWP6MuIKB58rgTlBlXZ\nQpvP56xPUJt6npeZ11JmK8TkcfyMaK6yL+X9qEj/SecQlUCaE7WDqvAkG6luV9FCWHBCR8TzCAdV\nLO92u4n0iPQYoVzR0WiUFmtCtuIXTiX5TUK8aW7ISS9R7ld5B1/yULYFWAcvfjHSbreFD1XxJWRK\nhFepttmFeFIa/GUChHJXJLaShHlFUYRRFCUStbvdrtT1BrSdJJDUJm2rsgTkPEjsWyu1Ib9FILOq\n4v+u5PtIcSIBkyW24zhOtG/eRE0TQJXTQrzIKXKS0+l0IzlfVJTWIaTW6zX2ej10XTd30uAdVhaI\nv8K2QgL8STx6P1VMJpOEQMxoa2FbkUgqsk36KWpzPj8mw6bSbcjnAxZtb2RBIkFY2jdkbD0LgxbN\nJTmnhbYqiCjnIq+dm80m9no9nEwmIsJMyFa0cKB2q3qVDh/pzojCKc/RtOATnXck00+keUVRlDhd\nLQM+HYF8Qs5irVTb6CtiNDQ0NDQ0NDRUIaK2anxyIRONQsxcgZdBiNdiscBGo8FW5IZhiFa4ToD2\nqXOO6orwygWFMhuNhtTKhWxcYcVZumKhdhEN/SK+2m4UiL5IcaIVdnovf71eb1y3An/NaeA58/lk\nCltVjBd/JFmkvwZBwHImRPPNMr6rchvyWK/XGAQB+r6fsFneFg2Nm6pbe7S9y9eHUr0OJl15OiO3\nQ9hWlKPDnzSdTqcb23p8RK+IN18+oQovWk2TvUzTlD5BW0PEOhfUd1RylGj3oUo6guTBpMTnkj9T\njIpK2YrsVCUCi5jMj8uYg5R8A40jqmIuContPSVeYRiyNAiZbVE+r6rRaKhetwWIe7K1JyuiEDOT\na8tQysv3/cQWgG3bwjlRaUgU5ZSy1Xq9Vqn7svUQKw+Z8C8vVgSchxQn/qQPYbFYMPs1Gg02GfL3\nRZGToL8vEZ5C/Yr6gmCRVmkQV65PVGrDLPBCKqutqI+Zpln0NlKc4jjeqAkj48R5EUUiR4KTsq34\nrZU8UcILF4DMgylCvGhhkFVjSxaC23vStqIJSyZHicCPHwVOgKgm5ChvjNpI8TSdlK1oXqtwnQki\nJkVgRskJ6f6uWG8MEZNzT8nYVR6H8/mctZNIwU0+r6rVaqmWbGHPLkXUhjHoKCtf6bfX6wl1WL6u\niCAKeaUvn6THNE1st9s4HA6lVgkSRTmlOg7fARzHEXYMu0j6I8gkJBKvbZwW4hOfEZO5Mq7rJvrZ\nYrFgzr7VarH+ZVlWmfMvtZVAwroyKBJBDp/7TrWKg3QietYYzajqnAUlTnzbUf22MvAiqiTJuVZb\n0SlNOjCQhTiOmb0KBHYpLz4hn584XdfFdrudSHwvyFtjSEelqkQV0/aQKcbJQ1BcFNqKclZlq1rz\np+kUTyNL2YoSxWXzfbLA33qQEunS/b3KQSLEV35eMQdWaBxSTixA/ilVxcK9pdpmlyIq1xhhGCZC\n3CJXmkicihMxBiKeT6bj8Zg5trSDoocu15zNZrkJahJFOaU7zmw2S2wdeJ5XOLEonPCrPLGITKp8\naYJtXC9CfWQymSTq6eQdwebFFPGqECVjvNIlFKrWieFB1wGRAJThJQO+oGJ6hZsueKooPIU40Jgs\nqw/FiyiBrflabUX9mibhNHh7UQFThSRXdiCm7GBA0WNZFuPZ6/US25IKBywyQX1HVRwI1oAqtBW1\niWxBTv5wgWKUSLpfpReAqqBUgoyFhHB/py12fgus1+uxwIJohJFEjmJpFOFxyI/7tDbgt74lU3ZK\ntc0uRVSpMRaLRaLopWVZuREniTpNIsbIxXq9xslkgr1eLzOvhl8Fdrtd9H2fhdUFi3Iq2Yru1OOv\nqskTn7InLQo4CXfosq3EOI6Zw5A4Ei3FiVZRfCSlzAZBEGReR6TAKcGLP41XV1HAbd+1x4M/ms1f\nLzIejxM11rZxdJ4Hf5o2L2+KF82CzrJWW/H5aukICH/LgIC9lHgtFgsMgoDVOxoMBtjtdplg4ktr\nFD0FNYCkOJGIsW0bgyCQPpFGc0JJRKjQVnEcs+8sc0UM7ZqQMFaAdPtRfpNs9Cydu8eLQNXro6j8\nQ1G9LLoajOpWDQYDDIJgY3uZn4fqKgSdBb7gJrW1zCJMkhcg7pmQIgRBkDj6bNv2RvFIicrhIsYQ\nRhzHrJNSQbasTmZZFvu54nUsQliv16yIIgk6vvovJTpLJqbXYqui7USa7OhC4Iq8MkFCgyYt0ejl\nbDar/VoDvl6QbO2vPPCLjoyK0bWKA1rZ2raNtm1vLCqazabooqYWTvzRdHLgcRwnfl7TilMa5Lto\nHK5WKxyPxwmfJmivWnllgfwZHSigybfVajHhJVFxPRN51yN1Op2suksb4A8wFKDUVvztC6KiiKKF\nNDErlCWQbj/KbzJNc+Pe0irRx6rXR9HBk+FwyIS5yIXLrutip9NJRK23eSUS4qutXNM0E3O0SLHO\nDJRqm12KKGljTKfTjY7TbDZZIndJUquMMSqDrgnwPG/j5vKSCbwWTvP5fGNya7fbqgmCtdiKv9iS\nnzT4PIyaeOViPp9jo9HY2iWlBZw2ePH5FpKXeW6Ar8xLRUxVeYkgLxrbarW23oZ5mE6nicgKvxUv\nedK2NltlFTVNL7B2GRneEqQ4hWGIk8kE+/1+6SXmVKPJ933mMwSTvUttxR/UAZC7uLbCaTql9kvP\nISIPv02bfvr9Pu+Da+1XvMAaDoesFl1Z1DPD/rXy4hea5HMV63OVaptdiijlwe/7fmILgx7JxLed\nOqUwDFlF3F0IKcJ4PN4I3Stc1FmbrbJOAUkmmIvwep0QthUdr1U5qkvgC4lSDtc27tNKg5xl3q3z\nEqi1DekevpI761Q5SfPiT0vRY5omep6Hk8lE1pHv1GdJoDInykftdruFKRMkogQWzUK24nNoZE6J\n55VSEYCSrSaTCY5Go8SYo6cgx6gqp9r7VRRFGAQBjkajhMAyTVPmUnolXlRws+C0rihKtc2uBFTl\nRqLOw1+8KzkRv7FOKQ0KR9PgV4jG1GYrPvrEX4OQjlJV5PU6IW2r2WzG+nCz2RRe5U6nUxZ1Ediq\n/Mn0d/INnucp1Xwr4LSvttpXXsqgNqTIRrvdTkQYK9SY2+BFSfTkh0SqrfMHCCRxkdpvp7yiKEoL\nwr3glYFSbaMrm2toaGhoaGhoKOJniLjLz6vlw05PT+Hzzz8HAIDhcCj6Zz8r+N1OjZBCHq9aOB0f\nH8Ply5dl/6xWW929exc+++yzxM96vR7cv3+/Ll772H4ABbyWyyV4ngfPnj0DAADHceA3v/kNNJvN\nxOtevHgB33zzDUynU3j+/DkAADSbTfB9H65cuVI7rx3gjWnDHeCi8aqd08nJCXzzzTfwww8/QKfT\nUeEEkMPr9u3b8ODBAwAA6HQ6MBqN4J133sl8g9PTU/jFL34BhmHA2dmZGPliXvvYfgCaVxaKeJ2/\n4CIKKUVctEbaR04ACrzOzs7g8uXL8OLFCwAAsCwLjo+P4a233qqL14W01enpKXzxxRfw4MEDePny\nZekH2bYNd+/eLZtQKvPaMt6oNtwyLhqvfeQEUMDr/v37cOfOHfjxxx/BMAy4efMmtNttaLVaG6+9\nevUqLJdLWCwW4DhOVV4XzlY7wEXkdf4CLaQAYD957SMnAEVe9+/fh9/97ncAADCZTMDzvDp5XWhb\nnZ2dweHhIXz77bcwn883ft9qteDatWsb0apt89oS3sg23BIuGq995ARQwuu7776Dfr8Pjx8/Tvy8\n0WiAZVns/8vlEl6+fAmj0Qhu375dldeFtNWWcRF5nb9ACykA2E9e+8gJQJHX2dkZcz7j8VjlLQB+\nIraqCReN1z5yAtC8svBGtuHx8TH86U9/gsePH8Nyucx93WAwgLt371bldaFttSVcRF7nL9ixkNLQ\n0NDQ0NDQeGOgT+1paGhoaGhoaChCCykNDQ0NDQ0NDUVoIaWhoaGhoaGhoQgtpDQ0NDQ0NDQ0FKGF\nlIaGhoaGhoaGInYtpKRvtK7xqZ3X6enpNnnV9t3Pzs7w5s2b+OTJk9dmq5qei8RJ89piG56cnODB\nwQGenJz8FG21r7z2kdO+8tpHTpqXPC8AAPgbkRdpZOOPf/wjtNttAAC4du3a6yWTg7OzM7hx4wY8\nefIEbNvOrNhbN05PT+Hx48fw9ddfw/Hx8cbvr127Bq1WS7bApBLOzs5UKqhrCGDXtj09PYX//M//\nhC+//JLV+ZlOp3D9+vWdcdDQ0NDYgMjNxjU+rxO18orjGC3LwkajgY1GI32LdR28KiOKInQchynr\ndrtdlVMhrziOcTAYoGmaQkrftm30fV/mK0lz8n0fXdfFIAhkPqcOTm9UfyesVitcrVbYbrfRcRyV\nfq/Eyff9zH7V7/elv4MEpzeyDWvAReK0r7xeJy6arfaZFyCiFlKoyGs6nSYc+mAwqJuXMsIwxDAM\n0bZtBAC0LAsBAE3TrMopl1cYhgnR5roujsdjDIKAPdPpFKfTKfZ6PcYJALDRaOB8Pq/CKxd8O21J\nUEnbKggC0e+7U15FWK/X2Ol0NoTMcDisi1cm4jhGz/MSbTiZTFi7SiwOVDgpj0Mag2EY7hWvmrBz\nTvP5HEejkQqnC2erin2mCqd9tdU+8wJE1MnmGhoaGhoaGhrKEFFbNT5SWK/X0tKxALWq3Xa7nViZ\nG4aBq9WqTl5KWCwWaJom2wZxHCcRnRJc7Qjbar1e43q9Zu8vEV3CyWSCjUaD2XA8HqvyykUQBBtR\nlJojU9L9iqIplmVhp9PB6XRaF5dKvLIQRRH2+300DIP1c8MwWJTIMAzZcSrEKY5jjOMYW60Wi6by\n28BhGDIb1oBafQONN8uy0LIsXCwWr51XGIbo+z72ej10XTfxdDodHI/HMu24s6jBdDpF13VFtnEv\nWjQjF6PRCB3H2ZZfKOK0r7baZ16AuMdbe3Eco+M42Gw2VQVKGrU10nq9ZhOL53lsUnFdt05e0pjN\nZowX8YmiCBFfCb/JZFKF0wavZrOJzWaTiTb6PBn0+33RLVJpW5GQchxnI3/LdV2cz+dVt9mk+1UU\nRRvijsSJ7/tKNqyDF484jnE4HCbs5XleYruK+pTk9poQJ35cmaaZKUaIWxRFVW1Wm2+IoogtKugx\nTVO1j1XmNZ/PNxZ9RU+73Rbxt1ud7OI4Rt/3N+xYIiwq24pSD3a0gM/FarVi33lLguoiChYl8Kkl\nQRCo5jKXaptdiigpYwwGg8Qk0+/3sd/vbyOpW7qRRqMRAgC2Wi3mxCnnRzJ5uoiXFHzf3xB3vK2I\nc6/Xq8Ipwcv3fdZGlmVVckD8e00mkzzBJ20rElIkcqMoykyIb7fbqpEDpX5F+WSe5yVyy3iRR3lm\nijkTyv19PB4nctharVambfgFxWw2q8or8fm8CMlrF4pUkJOsgFp8A3+4w3EcbLVaLKJmGIaMjSrz\niuM4sUChPj4ajTYml/F4jJ7nJaKOJX6s9kmY/OhgMEj0Pf7fJf6lchvygrPRaGCn00Hf99H3/SqL\neSVOaRFZs6CqZCvf99GyLHRdF7vdLg4GA5zNZnszDssWD67r1nXIiT27FFHCxlgsFmxQpw0is3Uk\nYQwp0HYUP9lPJhNVQVGZ03A4ZPbJE0p8ZKYCpwQvXgAIRroKQRMobR9lOC9pW9HqrtFoJH7OCype\nVCkIKqV+RYnblKy9Xq9xPB6ziTf9UERNgps0r+l0mthqbTabpY6RBLpt26KLnEJO6/U60R5F/arX\n6yEA4Gg0KktEVuUkPA7TIoqPkFFbG4YhO06UePFcDMPAXq8n5JPW6zV2u11mexIRErykEYYhdrtd\nNuZ5P+/7Pi4WC9Et3MptSJH1vMeyLGy32zgcDmXmICVO1A6O42wczClol6qchGzFL3rbn4SpAAAg\nAElEQVTzHtM02bbxYDDAyWQiEjWu3IbpxYPjOIltbP53zWazjlQXQNxTIUVOoNvtIuK5CKAyA7xg\nkAznV+aFeK52qaOkJw6aBDudTh28hMA7vqLJJI5j5lgFJrxSW5FAoTyQOjCbzRKTaMbxemlbUS6N\nbduZv+dXwoqCSqlfkTPK2haLoohF5fhIAT22bWOv1ysTOUK8giBIbM/S+4uufmkLHkD45GohJ3KE\nFM0pAtmw0+nIjjlRTkLjsEhEEUj0SUatpXnFccza0rZtpUgKieM6FzRp5EUNXNdNRO6KxokgJ2Fe\nJFjCMMQgCHA4HLJ+mFfSZVvboJRHST5wNBolBBW1r6KgqmQrWrh3Oh0cjUY4GAzQdV10HCczuk6P\nQLSqEi9+wTIYDDIXD3EcJ04fC/q5Um2zC/Gk1EjpFS4ln/KK07KsOrYUpBwACZesyE8YhmzSkwhx\nKnGK45g5ItGVLnVygdVUqa1qnMRwsVgkVgskzjKchLStyoQUjzxBVQKlfkW8REtSTKdT7Ha7G86U\nImqdTgcnkwk/Zkp58VvSZHeBhP8NULTTMAyRFV4hJ9rSENkmoGgFOfAKUPYNvHApyxHk0xUqis5c\nXuSfbNuudIyen2wyxoCyH51OpxuRn06ng4vFInPhQgJUoMxGZf9OfPKwWq3Q930WQSPRpcirEHEc\ns8/gBQHljvFbfzXW4xOyFfXjoj5MYnQ0GjF/WsFWpbxoN6MsH3E+nzO7SpRuKdU2uxBPwsZYrVZC\nORer1SoxGD3PEwldVx5ocRyzTpEXrSAhKFGkU5pTFEVMeJimKSzayMkKbIOU2qqObZV0bSLTNHE4\nHDLRLMGr8DNEhRSBBBX1xRIo9ytyhrK5WYvFAnu9XiJCy4erZXjRljQJKdU8N2rHCsKTCSPRtqLJ\nhp6y8UbFRCeTCfb7fXRdl5ypUhvypwpFhQu/LSKQryjMi4QICdoKJwUR8VX+G42BlECUslUcxzge\njxOTv2ma2O/3S/sbLf62Hc2QPQVaJroEeJWCfHyeSEon5EsIqkq2khC3iJiM9G2DFxXHBihOBQjD\nkL2OdrsEUaptdiGehBuJxJFohGM4HLKBnj4eLWkMIdCkU7T6rXurIw2+8KVt21IOk5y453mqnBgv\nmjhV9uvTQoXyOCrsoRf/0V8/QxbkpLaV5Eqn0lSFaBRFiS3vVEREmBefP6AaXeRzmxRPV7FVpQwH\nfiuBHwtpwZTeHuUXYQWcCsWBrIgiTCYTxqfkuwrzosiwoEATAr/lm1rll3Jar9c4GAw2ksdt28bR\naCS0yOSjMtvOr6GUDW4hkguZKHcVTrTFWuav04LKsixmY8kFqRAv3veLgHjVkVKSBZrX8uZlSt8g\nP+m6ruyhtVJts03RJGUM6jSWZUnlPq3X60SCbrvdzlsVV+o8iK9yoMomPhqUgrWlhDjRipMGDNWH\nkoHEir/UVhTdkhFSFL3iHWun09l6/RqJ1WMCgrW3lPsV9XkV8RLHcWI7NGMyF+ZFfYsmLdVyECSE\nLMtSiipKREwZ0ltQRaKJtoxbrRb7u7+e5hS2Vbq+leoWGl+qhPJwMuwlzIvf7iniE0XRxqm9vIc/\ndZga46Wc+AMwtNCVPZBCfrTOAzJ5oEWywCJTSnRV4UR5qKLb/1mCSjJFQogXLQBlhZTIS1V4UYQs\nK3BBY5X6caPRUCmVUqptti2chIzB5xapHPHkt9xoFSRpjFLQFpFoAUKaFARqS5VyCoIgcbqMrw8l\nC772jgInxoscJZWlKAOfFE+Th8L2g1L77auQkt3K4kGOgZxlBkdpXpR/qHiHHiK+ihAV9ItcTrIr\nXcRXYjRLNLVaLXY0Oz1mU2UxhG3V7XYTfbnKcW9evJIQTEGIF/kmkUlX5MRV1pPyy0K2okUWfUfJ\n7RTZhUYl/y5THkZGdFXhhJjMGRRBGIbMx5O4ljgsIMSLLztSBuqbgmJQiVeR3+APeQBIlWkR5QWI\n+ooYDQ0NDQ0NDQ11iKitGp9M8JeSqhTd5GtubGtfmCIwolWcJYp0FnKaTqeJVWu60KYsaDVRtUow\nHc+l/JoypJNXFYvLKbUftYMsyFbbikghosyJFgZagVGxypzInjQvPmlTNW+LT3iWPTpP30vm5CAd\no2+32zgYDDAIAqForWpEivItKPJW5QoYOv1EtspYLQvxopwd8oFF4E89NxqNjeti8p7Ud5TqV3xZ\nk3a7Ley/aF4Q7A+VxiF/eKYMNRU3FgLNbSL5tqvVaiP1I8evVLKVTESqpnyyQl7kN7LabrVaJfJI\nTdNUmXtKtc2uBFShMeg+L36Ai4Yy+aPXJQ6tUuehDioTGkwX6czZEszlxFd47vV6tSSRihxdLeDE\nbEXXnNAkIDJ5UY4U2URBECq1n2DS+AYEHUalfkUlLES3s8jhG4axFV4kkBXu0GPgt74yRHYuJ367\neNtIJbYr+Sz+HkDZvDJeRM1mszy/IsSL39orO3XG126qUClbul8tFgsm0kVTEyRPtVYahyTaRPK4\nZERXFU6IyXpSRaA7VgHOc7eqpm4UgdpFpEYZLazSBZHr5FW2BUzBFT5gIzmXlmqbXYgn4Uaaz+eJ\nI92dTqewQ8RxzF4voNiVeZFYkyk8OZvNEuKwoN5SJqd0LgN/WaxgldhM8JGkAgjZik/yl4lg0Gpe\nQRgqtR8NfNmJYxdCSmZ1m558S6DMi9pVMAdkAxSNzTmSnMuJ+qbk3X1KSK1ilWyVruUmusiSaEdh\nXnwOZVF0kz/luOtrT/jL08sOy/ALtYqchMZhzunETNR0b6kQ8upJ8UgfXNjW6TiCYO4oIm5e0bUN\nXjL5y6PRiNnKdV3RxWKpttmliBJqJLoslb6sZVm5EyAJFcGaTcq8yOkWrZRXqxWORiNstVobp4YM\nw2DbDxInc3C1WuFwOMytFttqtWRvbWcOqiT5T8hWNPHJRpj4JNuarmMpBG8/13WFo527EFKiV/fw\nwnrbtWL4wx8qyZkUreET4gUKhbJtAKoJVWULuwj8FuZfxUSlNuQrKpe1DZ8cX6cY5ssfFCV1875p\nC/eWloLfFi0q3yK44BPhJCUORO62lBFdVTgRiupJ8cn8Eqd/K9lKxnfPZrOtCynEVxFFkdIG8/mc\njX/LsqoWqAbEPRRShHTRTXLMJBroCGoNHbqQF78ioBXcer1G3/ex0+kkVt784zgO9vv9KoXkEqD9\n7rz72BzHwdFoJLTK5EPmVfNr+FozMhEm/oSYBJT6VdYVCyKCiiambd46L1IrhxesEvlDlXhR1ESi\nsCyzc/o6jVTB3EJO1JdquD8vFyRmuL5X2Wfx0ee8NtqmGE6XsMjqs3xxWpWTogK8hMCXj8grKEz9\nT8KnVGrDtD+1bRs9z2P9kJ9jJApMFvESRl49KV6US26H12orus+OTszTxdhhGCZuwNgmr/V6zdql\n2WwWzoNhGCYKWgsIwlJtsysBJd15CGnHbJomjkajxDFrQSjxoo5A95vlRYds22bXdNR0B2AhoihC\n3/cz72Kj5Lper5fZSfg6IJKXkio58CzEccwEXQ2l+oU/M0tQ5YkqweP4lft70QEAPmwvWOC1Fl58\n+4h8Lt0In7ZrRv8r5ESikRYodUalKMpFPDl71+Kziq6AURBRSrz4+k1pIUrRAZF7DBV5SYGP5E2n\n00T/J6ElUXtKuQ3jOMYgCHAwGBTeq2cYRqJ2W0VewkjXk6LSInntXIGTEC/P8wrrtWU9uyhhQYnl\nfPCFxB09fI6goIgq4wWIF0BIIb4qupmOxMisllV5pW+MpgHVbrdZBKhCrkERL2HEcYzT6RQ7nU6m\nE7AsC7vdLttKoJUM1cSR4JTLi7/oVHQ7iBy74B1tRbykkHcJaFpQ7UpIZR0AIIFK7VljPpkwL5H2\nmU6nCeflOI7qVigDv1ipeo8jD9pep1WrACfpvsULJhpfiiJKmRcv6BzHYdtVlGBfw8GV2vw7Lwj4\naJ7CadZa553VaoXj8ZhtmaavY5KI6NXCiRY1/C6N6B2rEpyUbBUEAU6nUyZWut0uuq6b8K+CAY/K\nvKIowl6vVyryer1eHYWg2bMrAaXcSGlMJhPWQJLF8KR5pY8VC27VyaJ2W1Fl4l6vl6h0Syqcv2Q1\nZ2tNqQ3ptJZIrgiBomOCK+RabUWCKm/bb1dCKp1HsFqtEsnDimKilnFIwiOdAB4EwUZl9Yo3qTPQ\nnZuKkbhMpO90VK0CLwL+CpgskSABZV7T6TQz5YAiGBW3TWsdh/zpZOrvfASmIqda5h3E80maDhFJ\nFBethRN/EpZ8g2JxySJOtdmKx3w+r7pQluYVRVFC3NEzmUxUTiOXaptdCajaO7SCEpfmNZ/PK52Q\nq8irNiwWCxwMBpmX3AJkJp0qtyE/4DudTukA4ve2Bdp0a7bKilLx1fIVOAnz4k8o8bVgJATmVngh\nvqr/BXC+BctHdSjaKTkpC3GibWd+BamyzRdFERPrVes1yYC2ZCuKwUq84jhG3/ex3W4nItUF5Raq\n8lIGbe2VVHpX4bQVcSCBWjjRlndJ7biqnPbVVvvMCxB1ZXMNDQ0NDQ0NDXWIqK0an9eJi8ZrKwjD\nEEejUWKvPWO7spKtxuPxRjX2vG0fPmIgcGH11m2VFZnadkQK8VVeEEUOCi6yFUVt/T19AS3xHAwG\nWy+qykelGo2GVC0wPgGetkKqFL6UBUUOJA5T7IRXDdgap/l8LhoJFuX0RtiKaiaqXJAtwWlfbbXP\nvAAR4WeIuFPdtssPS+FnBb/bR15b53RycsL+fenSJf5XlW11cnICf/jDH+Dg4ECYj2VZMJlM4Nq1\na3kv2Zmtzs7OYDwew+effw6ffPIJ3L17V5aTFK/bt2/DgwcPAADAcRz4n//5HwAAeOedd0TfYiu8\nAM5t8f7778NqtYJbt24BAMC//uu/qnKTbsOjoyPodDrw7NkzAACwbRt++9vfguu68NZbbyVe++LF\nC/jmm2/g0aNH8OLFCwAAaDab4Ps+XLlyRZZTIa8d4KLxqoXTcrmE69evw8OHD6HValXlBPCG2Or0\n9LSKP+Bx0WwFsL+8zl+ghRQA7CevfeQEIMnr5OQEHj16BP/1X/8FR0dHG783DAM++OAD+M1vfgOe\n521MjIK8tmars7Mz+P7777c+CR8eHsKNGzeg0WjAfD6vw2HW2t+Pjo7gb//2b+Hy5csVKAFAhTa8\nf/8+fP7550wglcFxHBgMBnD9+nVVTkK8toiLxqs2TsfHx/D222/LjIOfrK0UcNFsBbC/vM5foIUU\nAOwnr33kBLCfvPaRE4AEr5OTE3j//fchCII6xArAG9yGT58+ha+++grm83nm71utFly/fr1I/Ipy\nkuK1BVw0XvvICWA/ee0jJwDNKwt7J6Q0NDQ0NDQ0NN4Y6FN7GhoaGhoaGhqK0EJKQ0NDQ0NDQ0MR\nWkhpaGhoaGhoaChCCykNDQ0NDQ0NDUVoIaWhoaGhoaGhoYi/2fHn7esRxn3ktY+cAPaT1z5yAtC8\nsqDbUBwXjdc+cgLYT177yAlA88pCafkDHZHS0NDQ0NDQ0FDEriNSGlvEkydP4KuvvoLFYgEAAFev\nXoV2u1105YrGnuD4+Bi++uoreP78eaLQ5M9//nP41a9+Bc1mE65du1ZW+V3jNePk5IRdZUNt1Ww2\nXyclDQ2NbUPkQr4an9eJi8ZLGFEUYavV2rhYlp5Wq1V2GbAopwtvKxEEQYCtVgsnk0kVTkK8ZrNZ\nYdvxj2ma2Ov1cL1eb51XFcRxjEEQZF2GXcTrdaKyrSaTSeIicP4xDAM7nY7KhbM/6XEoCW0rcVw0\nW+0zL0DUlxYT9pGXEKfT01P48MMPYblcgmVZ8Mknn7AV8FdffQW+78PLly/BcRyYz+eiEY2d2urp\n06cAANBoNNKXJ6ex1XyDw8ND+Ld/+zd2J2Cv14P79++X/ZmSrU5PT+F3v/sdu9TZMAx2116z2WTt\n9OLFC/j222/hyZMnsFwu2WsfPHgA3W63dl5V8ejRI7h9+zazm+d5orz2cQwClPA6OzsDz/Pg8ePH\nAHDeNjT+vv/+ewAAeP78OXt9v9+HwWCwl+NQAm9UG24ZlWx1cnICt27dAt/367qwuIgTwH7aCmB/\neQGAzpHS0NDQ0NDQ0FCHSNiqxqcUcRzjeDzGdruNruuiaZrYbDax1Wphq9XC0WiEq9Wq7vCcMFar\nFQZBgNPpFAeDAXuIb6PRYCH9iltDQmi32wgA6DhO5vZdGIZo2zYCALbbbdG3rWyrMAwxCAKczWYJ\nG2XZiZ5er6fKSxm+76Pv+8xG8NetGABA13VF3kLaVmEYsu9vmiYOBgOM47j0gxaLRWIbsNPp1Mqr\nCubzOdvaMgwD4zjO+061cFqtVtjr9dB1Xfa0Wi0cj8ey29hFnEp5UXuYponj8TiXa6fTSWy1i7R3\nFV55oK1kCQ4yvF4nLtq2kBCGwyECAFqWhbPZbNuc9tVWSv6h2WzifD7fNi9AxP0SUr7vo2VZQvki\njuPIdiwhXsPhkE38nU4HXdfNzX0oezzPq8KrFNPplA2yoryZMAzRNE0EAJxOp1U4bfAajUY4Go3Y\nZEYiROahNhcQLrUMsiiKcDQabfQ127ZxNBrher1mk6MAhG0VRRFGUcREW6PRUFoUTCYTZudOp5Mn\nqHbiLMMwZGJesN9X4hRFEXqeV9ifDMPA4XAo8zWUbDUYDFi/EWnH+XzO+pygkKm1DUejUcJOEgsr\nUV6vE8q2Gg6H6Pt+FWGpwksIaR/V6XSYH9kCJ6U2XK/XG4GF2WymYs/aePV6PWazfr+P/X5/GwsH\n9uxKQJUag1+xNZtNHI/HGAQBRlGEQRDgZDLByWSCnU6HiQK+Y1U0BkNZ4q9t22z1y3ec6XSKQRDg\nYrFgEzGtzBV5lYIE3mg0Kn0tOVHHcUTeWrgNu90udrvdzMRoElf9fh8HgwGL/qQTkcMwFBUulQbZ\ner3Gfr+f6D+O46DjOOj7fuK1FDESmCCFbeV5HhMBeRFEUSwWi8T3GAwGyrxUEEUR9vt9Juh4LiUL\nHGVOfHTVNE3sdrs4m81Yf/J9PzF+2+321iI/6/WaffecpPrS71AhAivVhnEcJ/yraZqsvQQ4yPDa\nGhaLBfq+z/xNRh9TthVNuoZhoOd5OJlM6hRVyraihbJt2zgcDll/sywLLcuqEm2ppV+tVquNRVT6\nkbRjLbz4sck/jUZDNapXqm12KaJyjcFHTGzbLj2VFEURDgYDZizbtkVOxAjx8n0/YfiC00eFcF0X\nAWBjgpbgVYjVasUGlUhnjeOYrW4EBqBwG47HYxyPx2yVrSoOqP1LhIuSrcIwxG63mxhcrusWDipy\nEALbs0K2ms1miYiJwgmuDZCjpfdcLBbSvGRB7c2vlLvdLgZBwCbpkv6oxCmOY7ZwcByn0EfMZjPW\nn0q2P8s45fLq9/vKUZ3FYsH6oqLoFG7DKIqY3UzTTAhP4pC3JanAqxaEYYjT6RT7/f5GlJtEREYf\nU7YV7/N5sdnpdHA6nYpG8fOgbCtaFNBCebVaoeM4CZ6KkZbK/cr3fdYuhmFgu91mQQXiLZgaUSsv\nxFfCuN1u43w+x0ajkUglkQi+iPACxD0RUojJvBHbttOTQib4jiUgpoR4RVGEhmGwTiJ41HwDvLgo\ngVLHob3zZrMpzImcf50r4cVigYvFQibalQlB4SJlqzAMN7aBPM8T6ltk37psRcJaNIIoCj4imJrU\na3FKhNlstpHX5rousyVtc3W73bK3qtTfbdsWcoJ8xK4uMcyD/I5qVEAwQlypDReLBYt+ZW0/TiYT\n0SiiKC8p0G4DTb6tVqs0tYOEvASnUl7kvyzLwvF4nJnKQcJKYVGtxIkWyoZhJPp7HMdsq4pf8Ev2\nw0r9iu83WaKEhFSNAl24b8VxzOZu8k2Ur5mO6gkEOUR4AeIeCSnE84FFE45pmkIrgSiKmFNrNpsq\nSa4bvPgtGIXOgIivwovpgSDBqxB8nkyz2RQSfBQ1EBA8wrYie9N3VQ2J00RcIlykbDWfz5kz6na7\nUlEgiiBVyNtivMgp0pZKnbkY6/U6EcrmvmNlp0Tc+e0yWt2lxyZN2AITjTQnPpoqEx3gE3UVo2Qb\nvCg/RTD6lgv+O81mszwho9yG0+mUCUnXdXN9ENkoI6JZBCVOQRDgcDhEz/MShzuyRAulTtBhlW2K\nTn7yJTuFYYij0Yht/fP8LMvCbrcrKl6UOFFUpWhhQtEW4iWRF6hsq/V6zfpVekFIY6NCEKKyz6IF\nSl6kmJLQyWatVkuEZ6m22aWIEjJGej9fZPXOJ/BS8rOkMRKgfCzZiE8aNAGVKF9lW/ErTsuySiex\nOI6ZXeuaWAjkbCSccQK0TVUiXKQ4SXzfDfCTZQlKbUWDmwT6NkBjhuv7lcZhFEWJhE3TNHPHIglW\ny7JE3lqaE/WNRqMh8v4bkUjLslS3jDd4zedz9n2r+AbEV4uHgn6h1IYkjmgiLuv71HcsyxJdbAhx\nGg6HOBwON4RIOqrZ7/eZz01/voSIrtTfyyKMq9UKB4PBRlTWtm3s9XosMi/BKxdxHDOxUuZP4zhO\njNNms1lrXmca/LZZGnyah8K2XiVeiHILrtFoxGxc5NsEeAHiHgopQtoZlIGiLXXsofNRltQqXwq0\n916yvVfJVumq5mWrEhJedU0sBNpeqhLBExAu0raqsgVTl63INgUivzKor3EOTmkcUgicTx7v9XqF\nUVVyrv1+X4SqNCcSHBkJ9QlkJcAPh8NaT8dRzkzRqlcUdMiCorlVeJHP4gWkaF+L45j5kEajIbJ1\nKsSJxAXfHt1uF8fjsfCCS+KQTKV5J2MhkgsqvZEVVWs0GmkfKM2JxIiMUJ/NZkxECJxaVbZVkcCj\nw0UV5oFKbSh5oArX67XowY9SbbNLESUsDgj89hU1UtEg50N2GQ0pzYuckuRRagY+34q2YDJQi61o\nsiEH3263M21FHb0keiVtKxr8gsm9mSBHsFqt8sSLtK1kHGQaFfO2GC+yecEWDiIiSwLmH1ERT3ke\nXNRGug2n02licmi1WkLH+iUOMRTxygW1YV5klwQqn18jcZVOEacNXqJCKo5jxqtINPDRmozXSbch\niXbTNKVKa0RRxKItAqUZpPsVfUeZrVnJLd1aJmGRRTsPPhpE81TK70pzoj4hWIeQIYqixG5O3aVR\nyMfYtr3xO1oI06OYW1ypDcluon2MT78p4VuqbXYhnpTFAeKrbQM+dJkHmsxznJw0rzq39xSSJKXB\nn1aiLYM0tiWk6kw4LzgpI20rVQeJKJxwXmorsjnlW+QVJM16RHlTdINzcsJtSNEMPiFe1BlRJFh0\n202UE48iIcUvIGjcK9TmErYVv7WX19fTBV6LFhd8BClj4lTypeRzRBPzCWEYMuGiWOw1Fyq5WNsq\n2ZIF6scyvp4XUXToQ4JXJvhtctX8O+pTvV4vz3cp2YoEcdYOCz/3kqAcj8eygkq5DfmaiqKg9hNI\ntyjVNvqKGA0NDQ0NDQ0NVYiorRofadBqlE48Fa1m6HQUZIcfpXnxSX9V86RoazIDtdkKERMF0rK2\ns7YVkaqS2E2g6AId75XglYuy6EERBE/uldqKj/TkPVTo1XXdxHaPaHJ6lYgUgT8BK1owlMZnWf4S\nB+k2pM/IiujyY14gRC/LaYNX+tQej9lslplYXcSLP8aekUKg5EvjON44ySwKPqeJ6gJlQMln0baj\nSFK7wknNSvMOtWlOrtrGa3k/W5IPJMWJ+rpCdX7Gjc/tzbGzkq3IH2ZFpGazGUspSfd/ieiUchtS\nfxdN4eBPOgtESEu1zS5FlLQ44DtVwakIBppM6hJSPIeqeVLEK6Mz1WIrAr+1l7XFQXlkJfksSraq\nWluHH6g5CfpKkwr1IVmBVzEBnvFaLBYbuU9FToW2GQREHEMdOVKIyROwZbky/EJDYjtNmhNtC+Vt\nc/IOXHGcStuKbDSfz3GxWCTEMtUj4nnlbQ/zW5N1VqfnE2llkuLX63XiHkjTNGvzWfz2seM4hX1L\nNnG4gJOwLxU5XJK+J7Puch/pPKdmsym1iC87/l/CqdBWfBmXIkRRhL7vo+d5GxXGS+ZwJV7kLwVK\nDTEI2kmEFyDusZDiRZRosTh+dZqRs6HEiyb3OvKkclYvlW1F4JMBsxICEV8lBldIrstFlcRuxGTJ\ngZzBqmSrKqUZKiTAK7UhYnJBIJp7VNepPfp8XkzlgfISJMeGNKeyvBESp0WvUeSUy4uiK3wuVPoC\nar5YbR4v/nqljHFTqW+tVismdEVOVGYV8Kzr0AchndSe9xqFumGVx2HZ4ZIgCJg9G41GraUi0uBP\n4ZmmKVw8ktpvW6UiBN+fgS/PkDcnVeWVvo2kbCEvectHGS9A3FMhpSKiEJMJbxlbIkq86tje4yvB\nZkQYapuESWXnndaoszZS2eergp+YMuytZKuiraEiBEHA+FCdmwxU7u9pUDvRBCyCjAKylXjxlcHz\n2pM+U1I4K3EiMVzUhvQaiYrFZZxyeZFAIj9VdEqw6E5MPpJV5/1xhNlsJnQVzHQ6TZyQ3kYRYQJ/\nJVhWpI4mXsnt+Mq24lML0kgfZJJI4lfmlN5CLCseyd/Np8iplBdFh8siigT+AIOA/1XmlS5O2u12\nc9uo5mgnIO6hkFIVUXEcJwyZoZiVeSnkgWxwK7h2prZJmC8AmjWZCNa1KuJUyKtKPhKBdxwSbVgI\n2ZN70+k0UUrDMIydCilETPSXIvBRj1QuTmVefL5MenLh66zVdDKnEDRJmKaZu6ChBYtC/1OyleM4\n6Hle6QIrb4LjbZhjx1r6Fr9iz/KpsgU86+CUd9cfn7siecddZVuly1rEcbxxKbvCJc+1tB9fPDLP\nF5HvFFjYKNuKn2fLbtSYz+dsISqYolCpDbOugcnq78S/pmgnIO6ZkJrP54nLi0XvNeILytERc0lj\nFIK29ySOeG+Av3YmlcdRyyScLiCa5dwlrr1RshWfcK56eTHv1DNWhsr9SmSC9bei6v0AACAASURB\nVH0/IcZN08R+v7+VbdAy0PZsmVApyMOphRcf0eAXEvwhCkkoc+KP9Wf1bz5kL3knmnJ/F0VWjRua\nuLfhs9KgaAsd2FksFhsFPCUitrVwoj5Ei+bZbFZYOVuRkzAv2lK3LCtxXRlxVIh0FvGSwnq9TqSI\nUKSYv9ImYzEly0mIF79lbJom9nq9RP7nZDJJ9CvRgytVefH8+IUwzb3r9Tox5iRQqm12KaKEG4mf\nzFzXRd/32WkZHpTUxif/bSOXhXfQ6fcm0RIEAbsXajAYoOd5Gyew6EnllNQy0Mrq+UgUHyviVMqL\nvm/RREbJ15PJJHFjuOu6GwUhBXkVgk84T/88q5ijZVk4Go12OvjTaDabzBnk5XZRpI36vmCkU5oX\nvzVN0U7B64+yoMyJz6+hax3SbUSn4CQn4q20IY/0dgJ/sq6Oa61EQJF1EunUvwSTpkV4SYPai/Ii\nJU5SiXKS4sUv5MkXWJalfICmDk48RqNRIlpNURcSoIJpFZVtlSXssp5+vy+z4Ki1v6dvaDBNM3GV\nnARKtc2uBJSUMbKuqeAnuSyBInDHUCVeFN5tNBrouu7GaQTRh/6emwBq6Thll/6SExUMTSvbij6n\n3W4zMZl1m7rIk5HLpWwrPuF8vV7jYDDY6F+2be8kv0YEdHIxT5TyIionTF0rr3TBPcELubNQiVP6\nVFPRs43K5qrgF2OLxYJ9h0ajUctF66Ic0pNfo9Gos4CpEtJH5hWv3qnFVnwUynGcohICVXkpg4IF\nWX5VUIDW1q/m8zl2u11WusV1XWy32zgajVTsVvs4JMHH93uFAyml2maXIkraGFEUsWPEec6SIlYV\njVEK/kh6WtiRuHNdl0VYfN/HIAhEVjK12IocQNa+Lx9CF5xclG2VnuCzHqqZRDe8DwYDnE6nGASB\nqhguBU1caRFMkR/JfAwRTpWcJR0YSEd91ut1Yjwo1PpR5pVVRVwBtXAKgiDzeLVlWdjtdmWd+Fba\nMA1+e43G4zbKkBSBonoZCzoZ1MqJonMVLz6vxVb81qJqPTxBXrWAzwnaZamILWGrvCjXTOFUeam2\n0ZXNNTQ0NDQ0NDRUIaK2anwqg/KRFPasK/Maj8csoa6m1UoRL2Hwieb8aY71ep24D0riEkxlWy0W\nC+x2uzgYDHA0GtVtL2VbpSNlrVZLNi9EllOl/k7V3eGvUafpdIrdbpe1M53e2TUvur9Lsj+J8Hqd\n2MkKnT+RZpqmyKnkrfAquEBdFFvjpHAqroyTFC/f95VPZ0vyqg109F8iLWEn/V0BW+e1Xq/rrjMH\niAg/Q8Sd6rZdflgKPyv43T7yEuZ0cnICH3zwATx79izz94ZhwMOHD6HT6VTlJMVrC1C21dHREbz/\n/vvQ6XTg008/Bcdxts1JiFce7t27BwAAd+7c2fhdu92G8XgMly5d2jkvwq1bt+D+/fvw1ltvyf5p\n5f6+Beysv9++fRuOjo7A9324cuVK2cvfuHG4RfykbXV2dgYAIDoeL5qtAPaX1/kLtJACgP3kJcXp\n7OwM7t27BwcHB/D8+XMAADBNEz766CMYDAYiTluEkzSvmqFsq7OzMzg5OYHLly/XTGk7tjo4OAAA\ngJs3b4JhGNBsNqHdbsP169dFv8Mb14ZbxM5sdXp6Cu+8847oy3UbikPbShwXzVYA+8vr/AVaSAHA\nfvLaR04A+8lrHzkBaF5Z0G0ojovGax85Aewnr33kBKB5ZWHvhJSGhoaGhoaGxhsDfWpPQ0NDQ0ND\nQ0MRWkhpaGhoaGhoaChCCykNDQ0NDQ0NDUVoIaWhoaGhoaGhoQgtpDQ0NDQ0NDQ0FPE3O/68fT3C\nuI+89pETwH7y2kdOAJpXFnQbiuOi8dpHTgD7yWsfOQFoXlkoLX+wayEljdPTU/j2228TP2s2myoV\nlTU0NDQ0NDQ0asXeC6nnz5/DBx98sPHzdrsNvV4Prl27tntSGhoaGhoaGhpwAXKkTNMEgPP74gzD\nANd1AQDg8ePH8MEHH8DNmzfh9PR0a59/fHy8tffW0NhXnJycwPXr1+Hk5OR1U9HQ0NDYa+y9kHr7\n7bcB4PwyxrfeeguePn0K6/UaBoMBmKYJBwcHcPXq1a0Jnj/+8Y/w7rvvwsOHD9nFkPuM7777Dp4+\nfcqe5XL5uilpXDAcHBzA3//938Pjx4/h888/f910NLaEk5MTLZQ1NGrA3gspDQ0NDQ0NDY19xYW4\ntPhnP3uVNM/zPTk5gX/8x3+E5XIJtm1DEARw+fLl3LdR4XX37l347LPPAOB8m/HmzZvwz//8z0Wf\nI4vKpzqWyyV88cUXMJ1O4eXLlxu/NwwDbty4AZ988oloTtlFOz2xj5wAKvCiSOJyuYRvv/0WFosF\nvP3223B4eLhVXnfu3IF79+6x/xuGAcfHx3Dp0iXRz1XldSHakNIIvvjii1o++NNPP4V33nmnMi8V\nXL9+HS5fvgz379+X/dML3YZluHv3LvT7/boONL3RtqoZF5HX+QsugpC6dOkSvHjxAgAA1ut1wqmf\nnp7Chx9+CMvlEprNJgRBAACQNQiUGunhw4dw69YtMAwDfvzxR/Zzz/Pg008/hWazKf19BHmV2urk\n5ARu3boFjx8/Zj+zLAsajQb7/4sXL+DZs2fs/61WC/7jP/6jbGKsrUOTyPvXf/3XOsTnG+mUlstl\nQjB9/fXXma+zbRu+//77rfG6d+8e3LlzBwzDgIODA3j06BE8fvwYer2eymQry0u4DW/evMm28t9+\n+21wHCfx+ytXroBlWQBw7gcExqi0kPrFL34hSrcQi8Vig78KLxk8evQIAAB++9vfgmVZcHx8LCsa\nahuHT58+BYBzOwC8EncKqMVWy+USrl69Co1GAw4ODl6rf98iLqJg2Vde5y+4CELq3XffhefPnwMA\nQBiGGwPt9PQUfvnLX8KLFy9gNBoBAMDt27fTb6PUSIeHh3Djxg1otVowHA7hiy++gMlkwkQVOcFP\nP/0UOp2O1Pcq4VVoq+VyCdeuXYOXL1+CaZrQ6/XA8zy4cuXKxmtPTk7gyy+/hAcPHsDLly+3Fr0r\n4ggAzD4VRNUb5ZRu3boFDx8+zPydbdsAcN6/HMdhbRdFUVEEQ4nX0dERAAC8//77AAAwmUzA8zw2\nqVSJStHfcRN15TZ86623EouaMrRaLZjNZkUvkW7Dt99+G16+fAm9Xo8diJHB48ePYblcwnQ6hevX\nr9fGqwwnJyfwD//wDwAAbHE6Go2y/GURKrXhwcEBHBwcZC4YgiBQPYldi63u378Pv/vd7wDgPBp7\n9+5dAADo9/sqnIp4XUiftWVcRF7nQMRdPkpoNpsI54bE+Xye+ZrpdIoAgJZloWVZGMdx+iVKvIIg\nQABAx3HYz9brNQ4GA7Qsi/Gizx4Oh7her2W+njSn1WqFpmkiAKDrusKft16v0XEcBABsNBoYx3GW\nnYo4CbfhYrFgHBuNRsJOAICdTgfDMBR9uzJeylgsFrhYLDAIAgyCAGezGQ4GA/Z0u110XZe9ToJT\nKa9Op4MAgLZtY7vdxsFggEEQYBRFG691XRcBAGezmehXE+IVxzE6jsP6xWAwSLxJu91GAMBeryf6\nuRhFEfb7fez3+2gYBvq+L8JLCIvFgtksCAKcTCaJ9hoMBthqtdB1XTY++/1+2dtKtyHZK88flaHX\n6yEA4Gg0qpVXGTzPY2PQtm3mt3L8gCyvQszn8w1f0Gw2mZ8AAAyCQOVrFXGSshWNySyeq9WqTl5K\n4H3VdDrd6PPEezKZqHCq5Et5xHGMk8kEPc/DRqMh+mdb51WE9XqN6/U6Ydu/+o5SbbNLEaVsDJpE\nygYaObccB6XEKwxD5nSy4Ps++r6f+GzDMLDT6YgOPClOcRwzB+h5nqwDxCiKGNder5c3QVZqQ15E\ntVotjOMYwzDETqfDHBUvqCQclBCn8XiM4/GYDQTXddlDtpN5aKKpW3SORiNhkUITb1roFECIF3Gg\nPp7+jiRcDMMQEuzj8XhjgcEvQkQ4FWEymSAAYLvdLn0ticCSSaWIUy4viffOhGDb1+pLZ7MZa0tq\nT/IFJYJOlFcuaKFLIsX3fdbXeJ+gas8CTlK2Iv+wWCxwNpuxhTnZbTwe18UrAZrEeXHUbrfRdd3E\n3CLyGIZRNi/UPkev12s2F9LY4EWoIGrnxYNE6Gg0wsFggJ7nlc4Jb5SQ4gdaanWbgO/7RY2nxCuO\nY/aeZQiCILHio4jRdDot+jMpTuSAKaKkAn5yzJkgldswS0SlQaKKt1O73c6L+IjwSoBWbCJOx7Zt\ntG07IbZ6vR4OBgO2uiuJaCjbaj6fCzsamohEBIQMLz7amzeJUVsVTfpBECQcPtmS+gLXtpV8Q7/f\nF40ysfar0K9yeZGwHQ6HotQTEGzP2nxpFEVMENBCg+chGZWS4jSfz9EwjNw+JOrfFTkJ24oWzaZp\nsp9FUYRRFCU4tlotmV0HIU60UMuK3qcXJTS2PM9joms2m2G32xX1EbX0qzAMcTQaJQIdad9Ki2VB\nKPGKomhjN4FsRP1O5eEWraXaZhfiSXnwE0QHWhzHTBwAQHqLRJkXTQaigycMQ+z1eomQNU3Y4/FY\nlNfGd4vjmDnDEnGWCxpwfAfLWGUp2UpERPEoElSS22iZGAwGTDBOp1MMgkBkUkXE88FJNirZglTu\nV3x/FbEVTXiCKOUVRRGbOEzTzOWwXq8Zz9VqlYgghmGYWIHatp3om+TcuYhHJd8gGgmi7wYAIgJB\nug1pQdPtdkWpJ0CLmVS0rjKvPFA7uK678TuFqJQUJz4CXsTtdQspina2Wq3M3/u+z/ybaZqiXKU4\npUWnjM9qtVqiNlS21Wq1wsFgsBElMwwDPc9Dz/PQ9322vS+52BDitV6vsdFolIrOLGHHC9DRaIRB\nEOB8PmdzRU4/LNU2uxRR0oOfwH/Jsq2NZrPJVtmpfBJlXqSsZffI4zjG8Xi8ETo0TRN7vR5N0EKc\n+ChLifPd4DCdTrHT6SSEHUW1KMyegrStZEUUjzAMN8RdTpRKilMcx6wvSERyEPHVRJnnVAU4CfV3\nckgizlJS0Jfy4ieOsu9JEZh2u43tdps5Smoz0zRxOBxutPt4PEaA823oEl5CEB2LFO0THCvSbUiR\nHIH+kQlexNbJKwvkN0gIp8FHpQQhxGk+n7N2KIp4yfh3BU7CthLZPl+v10ywUL/OymkU4JUL8j2W\nZQkv3mnhZxhGGZ8iToW80mLDNE3sdDo4nU4z25Z2ZyS2a4V5UeSJuDSbzcRuwmQywSAIhOxH7U72\nk5wPAXEPhRSfTEcPH7UoC+l3u122wklFWpR5UeiyQiIkzmYznM1miUEIAEW8EhgOhzgcDgtXdojn\nA4r2qj3P2xAnjuPgYDAom4iEbUXRI1URxWO9XmOv10tEFVPRN+n2C8OQcZNZ7ZLAEYj8VervOX01\nExSNEYxGlvKi/kSJ4UWgqBS1CZ8H1e12cx0WTeJcJETZViQ+DMMofW2GgCuCdBtSREkikXYD1C9p\n+6gOXmnwOZVFkQGJ/l7EK4HRaMREQdH2Dj8huq67cXgg68ngWdlWZAMRP+/7PhsPlmUx/56BSnOO\nqFCntJasiKMEp1JeNBcbhlG6+JNZJMryoh2asmhnGfjvo9CG7NmFeMo0Bp/PJPOU7bfySjW1slDu\nPKSsK4Sd2eqMD0eWJLIlwCdqF/FIrxpIPI1GI5lTckK2IgFVh4gipPuFYBsWgiIveSvyNERW0QKc\nhBw4TfgieQTUtiL5QSK8+NC7SPidVm78pFfmJOsUUjJRJskcJuk2lBF1eeAnmrpPhBKojR3HKezL\nFJWqGMFLoMAXJ0BjoOI8UMlWklvBiHi+zcXnGBKv1N8rcVqv12yxIrLlSnOUYDJ8JVvRgs40zcLx\nzy8UBCHNa7VaMUErE+iI4zjxPUpOQ5dqG31FjIaGhoaGhoaGIv7mdX0wVR5Oo9Fo5P6Ofr9rEB8q\nYCeD09NT+N3vfgcHBwfsZzVWzd3A9evXE1farFarOq/3YEgX2zQMAz7++ONK1yocHBzAzZs32f8H\ngwEriFcFnufBV199BQcHB3D9+nVYLpeFPOn6j08++aSuayJyQX1A5HJpKv5a10XU1J/pYvAy/Mu/\n/AsrbHnv3r2iQpIMP/zwAwAA/PznP1dk+Qr0vbOKzqbx3XffCb9WBe+88w5YlgUvXryAk5MTpTFm\n2zYsl0tWrb6gwrkSjo6O4N69e2AYBvi+X9iXr1+/Do7jwHK5hMPDQ6G2LQPvx6mgchbeffddcF13\n4+eXLl1K+Pvlcpm4xaFOfPPNNwBw3gYiY/74+Bi+/PJLWK1WiZ8HQQBPnjypbL9Lly7BeDyGGzdu\nwJ07d+CDDz7I7R9nZ2cwnU4BAODXv/51pc8VwaNHj+DGjRvw5MkTuHbtGjx9+nSD28nJCbx8+RIs\nyxItIKyEK1euwN27d+HOnTvQ6XRgsViUft7Z2Rnjb5pmJn9piIStanwSITng8i3qAtVGgs2wvnI4\nk88lkQHVIQEukS0jIVeIE+WxiPCgE4Igtz8twonxiuMYR6NRos4KAOQlr5eC39KjLQEJXqXgjxYX\nnbTic4HqSuouA31e2ZaCYIKyMC8+5041x6AMGfWSlG1F+WQi23V1J+ZngbZ16ijKmbN9o9y3+PwR\nEb8VBAHb6hDY3hPiRCc8qc9KbPFkgi8tk9EHKo1D2oIsGwfz+XyjxE2z2cTJZJKXVF3JN1AfKSp3\nQ9uyu6zXFMcxy/c1TXMjbUSmtEsdvCinrCxFgq+jaNu26PxYqm12JaAyjcHn1yhUuc4EHcGEzVwi\n5UaiSV60HsZ6vU4cCaequDn5OUKcptMpGzBlCYV8wr1inRthW1HS32g0SpxOtG1bWFClRZQCLyHw\n++l5SbU08dddr6kINCGL7PGTjQXGSykvvpaR7KlGUWQcyVa2Vc5p3A2s12sZwVnEqZCXwqmkBHiR\nWXdhXBIG6Ql4tVqxwyjdbje32GOdte9EJ7ki8GMXIPPUZqVxSBzzvvdkMtnIh/I8T0REVxYstADM\nsx/lztZdZkCEG41v27YxDEPml2TnzKq8wjBk/SNvPKZFVE05w4D4moUUOXAQT5IrBR8hETw6X9pI\nVBFY5EREutaIwPeSEizUWYqUtIzokuRUaivf96UElYSIKuIlDJq8slZRiK+EikSdrspOSfC6EESs\nt1o3CQ6JY9PCoErNGTXdlG0lGmWi8brtFbpCnZwEBISsEq/FYsHszhcoLCpO6DgOdrtd9H1f5ECG\nFCeej0rkc7VasXFZcMJUeRzyfpV8QhRFbHHI+zPTNLHf79dekLMIRQvAOI7ZuKjhhggpXvT5vJgi\ngaI4NirxorkkXTaCBB61o+M4dV3jxp5diqgNY1A4VDCcXAoKJ+ZsFyo3kkjxvNVqlajwKlH9VooT\nTbqu6+aGetOiS2GCrDzQsgQVVVQm3pIiqoiXFEiMpCdamoAlt5prsRWtcMsgca2MEC++z0peE1II\nfis6JRKUbCUTZVIolqnUhnTarI6inDm+RYlX0ZUilmUx4TYcDlVLukhzIj9P407kcynSnT4VXPdV\nTfz9jVnFlHn/pXAquRafRX2NFoAk+OhUrGQZjso+iwe/jUy2Ir8iGa2tzItEHZWNIBHOi6ia50NA\nfM1Cqq5q3QS+Qm7GRKPcSOTEsyZYOjrO1xSpqfPkcqFOUVZugDqVwtZDbQMtLajIRnxb1XB/nBSi\nKGKc+HwsEli7XEUhvsoXzLvPkV5DtcEEI41CvPg70CzLqiUqRWH2nOipkq34opFUR4jqzKW3zWmr\nQyLKrdSGJLzrKMqZIxCleVHeGz2u62K/38fpdFpb+oQsJ0IQBIl8Stu2sdvtsgrT/GWxnuclxExG\nWQFRTqW8SHinI3Z0vVfFeak2wUL+ia4/QXw150nm79bm3wm0bZYW8ZI5upV5rddr1m/6/X6iD7Va\nLVX/Vqptdimico1BHTnr0lRRpPfQMxqwUiPR+xLm83lmDZEa1W4u+FB5o9HIzRkRKYYnyUl5oE0m\nk8yBJlnJuDZO6bv4JpNJ0d2DKpykeNGAn8/nOJlMWEQnb0tGIDIjzItvF8EClrng8xBy+p6SrdIC\nQeSRSAJXakNeAKeLCIs+fNtm+A5hXlTUk4SJagK8IJT7exRFOBgMNi62znuazWZpTlwJp1JefD6r\n5IXzVXhJg68tRb6TFoSS7V27f0d81Qf58S85H9bCi49+koCqWOOwVNv8DBFhh8j8sLOzM/jggw/g\n6OgIWq0WO2ItirOzM3j//fdhuVxCp9MBAADf99Mv+5ksLx6XLl2CFy9eQBiG8O///u9w7949AHhV\njuHBgwfQarWkeJfwKuS0XC7B8zx49uwZ4/HRRx+BaZrsNX/5y1/g4cOHYJomO4ZekVMpLxEcHh7C\nZ599Bu12W7a8gZKt8nD37l1WKoLQbrfh8PCwDk5SvK5duwZff/117u9N0wTHceDatWvgOA689957\ncPny5Vp4UakAx3Hgxx9/hH6/D8PhUJQ6w+npKXz44YewXC7Btm0AgKzjyEpteHh4yMof/PjjjzCf\nz9nvqHxA+oh9FEWiR6+V2vDs7KyWsg6ExWKRPoa91XFYAbWMw6OjI3jy5Ak8f/4cjo+PAeC8TMav\nfvUrVhKhpI+LcCrl9fbbb8PPf/5z+OSTT+CTTz6pu1xMrT7r6dOn8MEHHyR+Zts2GwMVOQHU0K/I\nD1BpEAnUxuu3v/0tPHr0CDqdTpYWkEURr/MX7IOQAjivy3H16lV4+fIleJ5XWveEwNeEsG0bFosF\nAECWA63USFevXoXlcslqxwAA9Pt9GAwGAABV6g0pD7SzszO4d+8efPnll6U1rlarlUxNnTfagfO4\ndu0aAAATMbPZTFYQ12Kr27dvw4MHDxKCCQBERVMtvKj+zY8//gie58FoNBKeVJ4+fQrdbheePXsG\ntm1DEAQAAFm8a2/DPD5kQwEot+H169dlFym5uHfvXrq23E9mHNYAJVudnp7C48ePwfO8bdWMq91W\nd+7cAQBgi/lutwvj8bgOTpV48Tg9PYU//OEPr43X6ekpfP7550oLwgyUCild2VxDQ0NDQ0NDQxF7\nE5ECSFbLbjQa8PDhw8JV5dOnT+H27duwXC5FKpRWUrsfffQRPHnyBADOowTj8biuyuS1rFiePn0K\nR0dHcHZ2lvj5tWvXoNlsyq62fjIrYdpSuHr1Krz99tuyIfIiTgASvE5OTuDs7Ewl8pQHJV5PnjwB\nz/Pg5cuXYJom3Lp1Cz7++OPcaObh4SEcHBywitPNZhP+/Oc/F32PNyaasQNcNF77yAlgP3kpcyIf\nT+kwQRDIRF+LOFXiVQMuIq/zF+yTkAI4z9e4fv16Ivfnxo0b8N577wHA+RbacrmEp0+fsu2YRqMB\njx49KivzXqmRbt68CZPJBO7evQv9fr/s5TLQTkkcW7PV4eEhfPfddypte9FsBVDC67vvvoPbt2+z\nhQPAqxwtwsuXLxNX1RiGAbdv34bBYFAm2nV/F8dF47WPnAD2k1dlTsfHx/D//t//k81DArh4tgLY\nX17nL9g3IQVwrrgfPHgA9+7dY3e5ZYFWzALOG6BiIx0eHsKVK1e2cXeXdkri2Kqtzs7OVPIkLpqt\nAAR5PX36FP70pz/BkydPcnPwGo0GdDod+Pjjj0XzqXR/F8dF47WPnAD2k1ctnI6Pj3eSP7kjXERe\n5y/YRyHF48mTJ3B0dMRWvz/88AM7udRqtWQmvovWSPvICWA/ee0jJ4A3iNfJyQmLEhMUtoyLeL0x\ntqoRF43XPnIC2E9e+8gJQPPKwt4JKQ0NDQ0NDQ2NNwb61J6GhoaGhoaGhiK0kNLQ0NDQ0NDQUIQW\nUhoaGhoaGhoaitBCSkNDQ0NDQ0NDEVpIaWhoaGhoaGgo4m92/Hn7eoRxH3ntIyeA/eS1j5wAFHkt\nl8uy4rIi0G0ojotmK4D95LWPnAD2k5c0p7OzM3YRb0VcNFsB7C8vANi9kNL4CeL4+BieP3+e+Jll\nWdsobnrhQRcH08XJjx492tZlqhoaGhcI4/EY7ty5w/xmTVeUadSAvS/IWSMqq92jo6NtdN43cnX3\n3XffwRdffAGz2WxDRBFM04QbN27Ab37zGyYcFHldaFsR+LsmCY7jwH//93+LVg3fCq+bN2/C73//\n+7qE7xvdhjXjovEq5XR8fAyHh4cbfdx1XQAAeOedd+rmJMRri6ilv9M9nC9evADbtgEAYLFYqNrr\notkKoGZep6enMrbbu4KcF7qR3n33Xeh0OnD37t16GJ2jtomFLi7+y1/+AgDnTstxHHj33Xfh17/+\ntcxkrGyrk5MTuHXrFrvEFuA8+tRoNBKv+/777xMCq9lsgu/7ZZP1GzsJ071ZL168AM/zYLFYAADA\ns2fPwLZtODw8lN3uq43X3/3d34FpmvDw4UPwPE+GgwyvC9+GW4A0L74CveRFtjKQbsMsv5CFVqsF\nv//971W4X7Q2lOL08OFDuHXrVuJn7XYbDg8P6+QkzYsHXaZ8dHQEAADvvfeerNDbSRseHBzAV199\nBZPJRPRPSoWUTjbX0NDQ0NDQ0FAFIu7yeZ2ozMt1XQQAdF0XoyjaNi8hRFGEg8EATdNEOFftuU+r\n1cLFYlGFUyGvIAgYD8MwsNfr4Wq1yn19GIY4HA7Rsiz2N77vq/CSQo1tV8RJqg1t22ZtFMcxRlGE\nURSxPmcYBk6n053yQkT0fT/Rh7rdLna7XYzjWOZtRHgpYb1eM04V2nUrPiuOY4zjGGezGQ4GA+x0\nOui6Lrqui71eD0ejUeH4UOGVbi+RxzRNxovnp8ArE2EYsv5tGAZ2Oh0cDAbsabVaG5xc18UwDIs4\nVLbVjlALJ8dxEACYvySfORqN6uSkZCvf97HRaGT2rWazibPZ7LXwykKv10MAQMuyZP6sVNvsUkTt\na4cW5tVut1kHsSwL5/P5NnmVYjKZsAEFAOg4DvZ6PRwOhzgcDnEymeBg9pf5tQAAIABJREFUMEjw\npsGoyCmX12w2Q8MwmBNcr9ciXwERz4VEt9tl/HzfzxNUlftVHMdoWRaapontdhtHo5HIhFaESv0q\njmPmJB3H2RAocRxjp9NhtpFwnLWMQ8/zmMCj9iWuijar1TeQ0AQAbDQadXNSnljSY67oIfGS4U+k\neZGQsiwLXddNtJnM47quir0SICFJk2yRX6CFA7+wMk0TJ5NJEQ8RTlJtGIYhBkEg8yeqvIQxnU5Z\nm8ZxjEEQYBAETJgqzEO12Gq1WjHfRVwMw2D9me9PJcK8Vl5pUN9Kj0mJOapU2+xSRFUyRg2ozIsm\nNHJOhmGwiXgLvAoxHA4TkaaywRRFEXNQjUajbPUubKswDDEMQxaJ6nQ6ZdRzQZMADcgMh1a5X/X7\n/dzJw7Is9DwPx+OxzISs3K/iOGarcdu2C9tkNBolokLb5MWD2jUMQ5zP52jbNosuSE50ZbykwY8B\nProiGbkr4iTMK45jHI1GiYUNL0r6/T6ORiM2CY7HY+x0OolIsmEY6dW7NC+adEuEUCbm8znrZ81m\ns+ilQpzIN+YtEvKwXq8Tk56gf62lDZvNJrOfRCRFhZcwSKyk7TAYDJg/l4wQV7YVv/vQaDRwMpkw\n4UxYLBasDQVtWbt2CMMQHcdhNjRNk/kvCT9Rqm12JaAqGaMmVOZFk/BgMGAhQno8z1PdWpDmNJlM\n2OeOx2Mp7qZpioTMhW3VbDaZ8/E8T4hLEfjJ0bbttIOo1H6LxYKJ4Ol0ir7vY6fTwU6nwwZXlrDy\nfZ8Jxgwo9yuK9liWJbSNMZ1OE1G/usRwHubzOZsECVmrO8mtvlp8w3w+Z7agCCbPaTAYyLxdJVuR\nwOQjY+PxmNmqCHEcY6/XY/7ENE1+pSzNiyIVKkKKvguNvQIIcWo0GiwapSBuE4sHATFVub/zkV8+\n8uo4jhL/El5CoPakaFQaFPmRXMBWshXvR4vmPeImMS/Uqh14sUdtGYYh2/0Q2JkR4QWIWkhJ8aJJ\nnsKU0+kUTdNMKHOFrQUpTmEYsk4sKqJIeOVEeWQ4JXjR6rdooKuAX0GkHGil9iPBlxfRCcOQiass\nYUUTTKfT4Z2HUr/iJ07BvDVEPHdifGRxGwKPQCvevLD8aDRifVFiq6+yb+BzytLceCEukcuobCuy\nEbVHhQmXRSf7/b4yL5p4efErgzAMaxFS9D7Ux1XBR6lLfFel/p7+nKzoom3bZTmcMryEQIsDrk8k\nwO8GSHBTthW/VVskkMieqYXB1nilMR6PmW9qt9vYbrfZ/DQej9nPa+AFiHskpDzP20h4rPJkOP/K\njUSdg1f/q9UqsVdsGIbsdocUJxpYgts7iZW7qPAq4JTgxe+DV9zeTGA2m+FsNmMCTYBXKfjcEdHI\nIS+saLsxYwBK9yua7CWE7QYvPlRtmmYt+TVpUPsWcZzP58yxCm71VXaUFDnI2y7iV6K2bYsIVWlb\nxXGciIANh8PKCwnq81w0SZqXoBDKBf19ifgp5USCrkp0jEBilcZuzvhV7u98hIX3Y3Ec43j8/9u7\nXiA3kuv9rupXpWPyocwhxUg2OgVljeQwOUQ2ioyioCghXnSlkMgXEBnpLiFKkHxIrhDtIdlIa7TL\ntIdkE9kmY6PxBc0uej+w99pvRjPTf6Yla533VU1V4tOuvn3d0/31+9djHI/HiYNVrVbD0WhkOt7O\n851CY5VKpVCM0IG2UqmUPcxoedHaVRSqDcNQiVCLPacULw4eLcoSoGRXi3dEq212JaC0xiCVWPah\nHI6MiVd6kIryD+I4VqEay3CHEaflcmn8YhHW67Wa0IbJfjpOilcURYpLpVLxXQmHiKgWL7aRO40f\nf7EdcnoSnrcM2xvPKwpB0e8q472IoihR7ZQh4EvN9yiK1Njq5nEURZlzP+fnSr2D3HOgqwglD6TB\n4cbaVmT7arXqLTmZwmofU0jFcazGsQBaTj6FFOIHUU9hUAtOhX8I927qwmOTySQhqIIgwOFw6Bpi\n14KEusnaTeLBMF/KyVZUqKM7XNFBx2HcS69Z9F4WvfN8jnvwWAPiHgkpxA9Jdd1uVyVl6p75fK4m\nd61W22qogxaHokRM7lI8ODgok4+UAJUKm7zwtImRPVutlo6DKSfFi4tKHwtlFmhxYKdEp/GjmLgL\nzziOE4tnhufNaF6Rt4Eey5NaLnh+TdmwEIdL0jKf+xSezRA7zpx4WNskjJE+3JycnOQVZRjbir+H\ntmFZHcjm7H21HkM64GiEUCF8CKnVaqV+j0uYMQxDnE6n2Ov1EqX1JO4z7O4030ms2CRsT6fTRLVa\ntVpV88LiAF+IMAytDs08Ud4gX8rJVnSIyVsT0tWEHtNctPZarVZqngRBoH0vafw8pLoA4p4JKdps\nbOKqtFBWq1XvPVnSMD3t8eTTarWqq1gw4tRqtZTa1nky+GcNKvRsOCle5EGkHj46cDe56eZDLy77\n/dbjR6d8xxdbJemTMMiA1lY8vApgnQhtBO7R/Tl3odR8J/FpG7JdLpeJjS8j3OfEyXKjUDD8GSNb\ncS+LSVh2Op3idDo1fv/oJM+SYJ3GsKyQornqcChV4Kd+nVeTRFOWcOLziLcryeBlbSsKUxnsHZmY\nzWYJQUV/a0qsO813OhzZFPDwg4Zmj3CaV7TXZh0CKXeKxs5xjXPixUP5jUbDSDvQu+ahIhQQ90xI\nIdq5M8kYhifDUrwQk6cEHaIoSlQMUC6VBa8EeNl50UufLu93LOHV2opO5XQSy0MYhtjv9xO2ME3y\ny8hJsxo/7pXLS9YsAs+dcPVmrFarRGuIMu0hdFzpe35+d0rNd5prLh6XKIoS1Zw83OfKiTYWXZsI\nDpqjGdWfaRjZim+aRXM+nT9VqVSw1+vlvrcUtidvS5mqPUS0DVtsgMa+jJBCTFb0Fm3s6QrotHDy\n3USYe05MDqZFSHusM9Y26/ELw1CtO7bvH0/y9h2dobBeFidedEHrpQOseaUPkKZeRaoINRSqWm0j\nV8QIBAKBQCAQuMJEbXl8tOAnsyI1Tp4X3yX92l/ys/rVKV/evNPRzZoA90gV5V1lNSDz2DRR8SJF\nn+eRWq1WqtoNUqdN07g/nTZcPVLE0cAjkQlKcNWELgttxXN0qBrRJ+jUSadFdsJynu9lEpYXi8VG\nV2PKIfnZQ2LNyaWLs+XPaG1FIWK6niNvPvEmq7zSk55Wq5VYDyhkSZ6blCfeaQxN1gmTnydPWQaM\nONENC6DxQpMHxib/zYJTghe9K/xaqsPDQ6ubGDh4jlVOnpX1+JGHzqI8PwHae2heWXAq5EVzOAtU\n3MELPBwquY15UUoJcbLoCYWIH97ner1elhcg7mFoD1E/kciNmNEJuAileSF+cG/mLVJ8ITWs6DHi\nxMWRbmOgZHM+0XxX7VFiLPXoIMzn8427szqdDp6cnKjPmvLhDVA1vBIIwzDhHncRL7xdgmaRLbRV\nOuEZcnIMXEAVR+l7+kx4FYEEqE0YMi2gqC0DE1A6XpngFZemGwvv4m+4wGptRWtOUU5gFEXqHa3V\naiqcT00AeXibSuj53MgIWzqNYVkhxdte5KxfTu9h0cGYF004CghjW9Hfxecr3QFoYzN6Tyj/skzq\nBoFXxrkWMvBeTzlrrdO8KhJS9L1xHCfSS9rttk2I2YhX+qBG3zOZTIy/i+fwlaxyBMQ9FVJhGKpF\nJ+3N4eXjnvo1GfNCLM4d4ZfMWlT0GHGi/BoAuwTgdBWh4clLayveb6ZarW5cXEl5IXxh4i0cqtWq\ndgKTLdkcMLIVF2wuizLfiA1Ox0bzKp27ZilsN8DzvwAy+7o4z3eyncn7lSWgSDxZ9vvJ/Tt5Do1J\nHgTxt6g41NqK3r28eyB5by+qHk4jiiIcjUYbzV7pHfJViUbvoWtbBl9CikDjVzQeNqIrB062ogMe\nHw8TQcULSHxEHAj8Sp0yWK1WCS9fiqOTreh3mXj35/O52sMt7qU14hVFUeZtBraiit6Tkh5rQNxT\nIYW4OaGoqoOM5bHDrBWvvCaFYRhqF1JLXgnwv9/2JeNVhEEQlPGSJXhlVdeY9FYh92+eIEz3zDLw\nsijwnk/kkbA9mVu2SzCeVzSGvLGnS0Jw+o6+nARsp/kex7HiV8RtPp/nCigNnN5BbreiLuq0bvju\nqExCiipPOdbrtXq/Go2G0ZjOZjNsNptKQFmG0ArtZdJItQi0Ofm6PJx7FXUewhJeqVLr+3K53PAc\nt9vtzLHhf4/BgciIE3lz6PeWSYBfrVa4WCwS4a+yVw8hWrcMwDAMEwUnBp54J15RFOF4PC70VGUJ\nKxrvktcPAeIeCyleDdHpdBLqepellWnwRQbxQ44KX0gtN26nF81WSHJvmcHkMbIVvwur0WgYc+K3\nmWeBu/kNc0YU+IvLH7rahe7NywMXcL6rhQgnJyeJcl1boZe+o89nZU5RrzQq987qoePBfa/FcrlM\ntBVJbzZ0CnfYiLS2ormeDmXzK3tMRRSHj2rCNOg9d2k+i6j3vrlw4t23i0LtJbxSXtb39Xqdeede\nu91Wnguyb7PZLBMWSoAEetZBOYqixGXXlJc6GAyw1Wphs9ncaMOQ9ZRp9Ir4watuE/KnXCQAoxSL\n0mMYhmGuqEp7q+idNmjfo9U2uxJQ1hMacdO7UEJEoS9ePLxGi2iZhdSWE3mlXJoB8pApcS1z3UIY\nhkXu/0LkXWbKXzyLLuIJUL7CeDzGTqezcWdWWlhxceXQLsFpXq3Xa6sGcgSLO/pKLZb8PcvqlxME\nga2A0vEyQvrC5H6/rw4ZZE/T65MMOClefIOn958L4lR+mi84jSEJbcek7ULvm4ZXIUxzWx29Ut72\nHcTL8T48PNwoGOCe/ZJpEgnwA0qtVsNms5lZqGPy1Ov1xHVpJLpGo5Gu4EO7ZpE9TEJ1/P0wfCe9\nj+F4PN5oxcLtlCVcLXkB4p4LKcTk7dYle/B44UUbTbvdTiSPllhIrTnRRlKpVIwTl2ezmY3g8zqG\nWcjKBeC9kAAywwDOnEhYpceNP/x+NouxdLYV9xIaVHfa3tHnxIu779MCKggC1RCxhGjwMq9435r0\nxeEO3IxslXeNFb8Q1TOcxpALIRcY9IhzHkPu7ckL8zl6pbayZlHuFu+FZ1M9asIpy2GQPrTwO2S5\nR4oOspYNRZ1tRftfzv2eiHjpZR0Oh2oMLfbtre476/UaR6NRZhPVEp5hQLwCQmq5XPpqYuiFF79d\nHuAy7GjTCMyCVy7iOE4sSI1GA8fj8cbmenJygsPhMJHLdHBwUCaPxcuEpr+BV6fMZjO1UKXDJwa8\nrLFcLnE0Gqnv4uLKd1hIB15dmRdydbijz5oXNZzlpzUuoDyJBW9jyBNaaUF06VBdwGmDF3kweZPV\nLcJpbtF8Is+G7ul0OonNmcT9NoQUYnINbTabmRuyg1dq62tWFEU4GAx8XcSrMJ1Ola1nsxkuFgvX\nhpZlOWltxfMzyYFA3MnZwd9J39XivpAWVSW8+4B4BYSUR3jhxTe0LQs8Iy5ZYau8U43FArCTMeS3\nuqdP99u48FaH5XK5lZ5bJuD5ZulLf+fzuTrdbXMM+dzegoDS8XIC75vm0LdGxymXF1UObRlOcyvd\nZdr1KWj1UHoMF4tFonqRxnA4HOJisUgUF2wrV3FHuEqcjHjFcYyDwaAw9NhoNFxSPj7KGBbkmZrw\nAkTpbC4QCAQCgUDgjM8QcZfft9MvS+Gzgv9mzOvo6Aju3bsHg8EAHj58WJ5VPi8jTufn53B0dAQ/\n/PADvH79Gk5PT6FSqQAAwMHBAdy8eROazSZ0Oh0fnIx5meDt27dw/fp1uLi4gEqlAg8fPoR+v+/C\n68rPK4DLudXpdODi4gKazSYAAPT7fbh79y5cXFzYzjlrXn/4wx/g8ePHEAQBfP311/CnP/0JPv/8\nc5s/oQwv5zE8Pz8HACjDdSfz3QGleR0fHxt90U8//QRnZ2eJf2s0GgAAcPfuXVNeVrY6Pz+HR48e\nwb///W949+5d7ufa7TYcHR3pft1VG8N95ARgwev9+/fw7NkzePHiBQAABEEAAJf7Ds2dj8FrCyji\ndfkBEVIAYMHr+PgYXrx4Ab1ezwMlAPiEXzQT/OUvf4Hz83P4+uuv4csvv9R9/JO31dnZGdy5c2dj\nY+l2uzCZTLbK68aNG/DHP/5xWwKK8MmPoUdcNV7OnEgoPXv2DFarVeK/3bp1CwaDgW5O/s/YygOu\nmq0A9pfX5QdESAHAfvLaR04A+8lrHzkBOPJ6+/Yt/Pa3vwWAS2FleCJ35kUeHYBSXh1T/E+MoSdc\nNV77yAlgP3ntIycA4ZUFEVIMV22Q9pETwH7y2kdOAB7CVY8ePYJ+v+8icGQMzXHVbAWwn7z2kRPA\nfvLaR04AwisLeyekBAKBQCAQCD4ZSNWeQCAQCAQCgSNESAkEAoFAIBA4QoSUQCAQCAQCgSNESAkE\nAoFAIBA4QoSUQCAQCAQCgSP+b8fft68ljPvIax85Aewnr33kBCC8siBjaI6rxsua0+PHj6HRaLh2\nwub45G3lEVfNVgD7y+vyA9JHCgD2k9c+cgLYT177yAlAeGVBxtAcV42XNaf79+9DHMcuDWfT+ORt\n5RFXzVYA+8vr8gMipABgP3ntIyeA/eS1j5wAPiFe79+/h+fPn6ura27cuAH1et3kWh9TXp+MrTzi\nqvGy5nTt2jX473//C8vlEgCgjGfqk7dVHs7Pz+HJkyfw+eefm96pulNbnZ+fmzYUvmpj+OEDV1FI\n8bvurl27Bs1mE+7cuQM3btwo+jEvg3R2dgYvXryAFy9eQLVahV/96lcAAK6bShGvfZ04+8hrHzkB\nfAK83r9/D3/729/gu+++y/zv3W4X/v73v9vMfRlDc3jl9a9//QtevnwJw+EQALZyybMVp+PjY/jN\nb34DAJcXEwNAGc/UVRvD0pzevn0L//znP+Hx48fw7t07CIIAXr16ZTKuO7PV0dERdLtduHfvHvz1\nr3+F69ev7wUvS2iFFCDiLh8vGA6HCJeGTTzNZhOXyyUul8usH3PmFUURDgYDDIIg83vpaTQaOB6P\nMY5jmz/Hi63CMMTxeIydTgebzaZ6+v1+nj1cOHkbQ0dYcwrDEFer1cfgtK+2Mua1Xq+xVqup+X1w\ncIDdbhe73S42m03170EQ2MyxT9JWW4I3Xuv1GiuVihrHg4MDDMPQNy8rDAaDjTXUYa3ScdrXMXTC\nyckJdjod7HQ6CbvR2I5GozKcvNuq0WgkOA6Hw53ziqIIoyjC2Wzm+iu02kaq9gQCgUAgEAgccSVD\ne7dv34bnz5/DcDiEa9euwY8//ghHR0fw7t07qFQqAHDpxu52u/zHnNyGp6encO/ePZUbUq/X4caN\nG9BoNODNmzfw6tUr9bmLiwsAAAiCAIbDYfr781DK9Xt+fg7ffPMNfPvtt+r7s9BsNuHRo0dwcHBQ\nhpMxry3B2lZPnjyB+/fvJ/6tVqvBL3/5y8S/3bx5E37xi1+o/99oNODatWtwcHCgc5V7t9XZ2Rn8\n9NNPiX/76quv4IsvvrD5NaV4vX37Fm7dugVv3ryBRqMB0+l0I3T+6tUr+POf/wxPnz6FarUKp6en\nuvB6Ea99nFcAW+b14sUL+P777wHg8h1ttVpb4UVrJkcQBDCdTuH27ds2v6qIlxWnW7duwenpKdTr\ndXj58iUAXNrg+PjYlk8RJyteZ2dnUK1WdSEoG5S2FeVA/eMf/4CzszP175VKBe7fvw8PHjyA169f\nw71790zDezuZ7xS6DYIAWq0WPH78GABA7YuTyWQnvO7cuQMAAE+fPoXJZGK6L3N8eqG9KIqUm5CH\n0OI4xsPDw4S7c7FYmLrnMjGfz5XLtNlspn/fBqbTacKV2W63TcJ8zrZar9dYr9cT3zeZTHCxWOBi\nscDpdIqHh4dYrVaVzabTqcmv/mTc5IvFojAcq3sMQg1ebLVcLrHb7RaGjxuNBk4mE9NfWYoXhQ6a\nzSZGUWT02UajsdX5vkXsdL7HcYyTyQQPDg4QALBarWK1Ws2ynRde4/FYhWBXq5UK+9O8MgwHmfAy\nBl/HwzDEIAjU3HcMwXixFc3lTqeDJycnLjxMeWkRhiH2+/3EmhAEAQ4GAxwMBhuhWdp7DOy3k/ne\nbrcRAHAwGCAi4mw2U3sRAGCv19s6r36/v7GOGu6BprwAEa+ekJpMJmqBz8Lh4aESVI1Gg/8na140\nEQDA6qWaTCZqwjQaDd1G5GSrKIqUiKrX64UbfhRFiZi6ThAWcNrXDS8Xq9UKAQBrtZr6t/V6rcQm\nPePxWC1QPB9u27aK4xh7vV7iRa/Vaok8N77pmYx3WV6z2SyxyZn8DZRLZbApb21e0bjSWLbbbWw2\nm1ipVFzfQa/z/eTkBLvdrjqc0UNr1jZ48dyo9AbCc5Ta7bZWMBvwMkZ6HR+NRjgajbLW7bKcrHjx\njR5YPpnFAcaUVy4oDyrNQ8dhOp2a2m/r8325XGauIcvlEiuVipqTKdHnlRfZgx6yaaVSwfl8bvOr\ntNpmlyLKyyB1u93CBTuOY4zjWG2ETABZ84rjOGF8m5PSarVSm0uz2VS8MuBkK+4FMFkAW62WeiFL\neA28biwOsOa0Xq/V+NmARLTB6cXZVqvVKuFR7PV6hYnx0+lUzalqtaoTU868SLjZeCpIfAVBoPuo\nE6c4jnGxWOBsNsPBYICHh4fYbDYT9st7qtWqK6dS8z0MQwzDEIfDYSJpn8Qw/e/VapU37qV50Vh2\nOp3M/869BIYCvYiXMdLreHrddvBKlbbVycmJOsgMh8MN73AQBDgcDm0EZxEvBfJQTiaTRETD1jNm\nYb+tr++0fmYdEApEszdeJNhojtE8o8ODpZjSaptdiigvg0Qv/Xq9LvwcufTIrViGFw8Z2pxM1uu1\nmtj9fh/7/X7Wx6w5zedzNRl0duD8gyAwrdTZ+ovmCCdONHY2oIV+PB67ctJ+4XK5VPOZxLYOcRyr\nRapWq7kI9EJeYRi6eimUONAsULmc4jjG0WiEg8EAW60WtlqtDW9c0RMEATabTez1ejgYDFR1765P\n6NPpVB1c0vz6/T6u12u1keR51n3wor8/CILCcVyv12oDNwz/l7YVrYtpAVnCK1V6DGmT5SEnEjhc\n+FYqFez1ekZrbxGnKIo2Qnd8nrhUVRraz8hWURQVrS+5CMNQeZ2K/oaM9cLLexhFkTq4ZB0gaD+s\nVCplDw7q2ZWAKr0JUwgmHabJA7mOu90u/ZOXRSklzrQ4OTlRkypn4Kw50UZq4jEg92alUrEJT1rb\nKo5jtfHZPP1+PxFSSz/D4ZC/yE7jZyq+OTKEeB5KzSsuplqtlrGYovwaftryxSvtpTBtH0E2ywlT\n6XghYnZJPD2NRgObzaaaF4vFonAhJC9Zu93WUS+9Zq1WK+z1epk5bp1OZ0Nc0iayLc/BarVSJ3KT\nk3c6xNzr9bx70QkU9slax0t4pUqPIYn2vO+dz+cbArndbusETy4n/rfWajWs1Wo4mUyshQsH/51l\nWwEdHh6qfavb7RqPBwmVPC8ogd51X3s04uXfT+NYFH2hg7KBZ1/HCxCvkJAij47BQo2I/oUU/52p\n36sF92hlLOpWnChJEwC03gIScYaeFRNOhbZKJ/uXfTImudP40enEpp8UCWeDuVZ6XrmIKdqMKEnX\nNy8edqxWq0aLKB10Dg4Oij6m5ZQWU5reM7mg03lGUqspp0JbRVGE4/F4IxxTr9dxPB6r/jVpkEc5\nCALdWDuPIQltm3UK8XKNS/ebyjiAlJrvGRtoAo5eqVLznSe/69bV1WqlDrPpoicLXoj44W8lW/sA\nrV3tdjvvEKG1VRzHG/liNGd7vV7moZxC2TR/dAKFQqlsnEuvpTz6ojs4czGl2Ru02mabosnZGFlo\nNBpqwTI5YdEEZRuhF168ko+q8nQbH3d1Zrg7rTjRIlwUEqAJTQLCRHim4GQrfhoajUYbCd15z3Q6\nVZOan858LeC0qRgkjitkCPE8eJlX6Zy6vE2Yg+fdWHg6jXlFUZQouNB55ygsqMlJMuJEHkn6bpdk\nX1pUDYSYsa3m8znO53PsdDqJpPFqtYq9Xs8oVJCuZvLBi8M0pJeH5XKZCGVlbDSl5pXO88PXEYsK\nq1LznbyXmlCrgq7oyYAXIm4KlhINSRW4mHFdG3jieqPRwNFotJGPWKvVsN/vq7nB31kDL3BCvJry\nKgJVp9pEX8jDSPtNjvjSaptdCSjrjSWKIrXR0iQ3PAEgYmZSuhdeiJu5LSYl4nQ6yPAOWXHKEIgJ\nkGuTFivT3JsUnG1F/FqtlvGXcXtS8nyOPZ040ctiU/ZqsbB6m1e8kzgtYEXziucPZYhEb7zS1V1F\nuVP0uQJYcSojpnwXDPB1iJ5Wq4XT6dT4HeObnEEOjPUY2ob08sDLxuv1evrvc55XURSpA2XR3ObF\nC4a2LTXfab8wTdvQFT0Z8FLQRCycwH9nRohNayv6eRJHhNVqhYeHh5nFE7yFhakgTK0XzmPIoy82\n60Q69SVHgGm1zS7EU6Yx0vk05DXQPbqJRh4i2pjZScrbxoJ4OaH4SUK34Y7HY6WYU6EGp40l74Xn\nC6BpMnoGnG1lm+PAPXwGYS0nTrTo2bxghmGqIk5O84pXm+i+f1dCCtGoB8zll3oWUojuYoo82Aan\nU2NbkUeKxsg2dGZzYrfhRese/c22vDgo5EJPhv2c55WN54f+ll1ce0LCwFQAZOwvtrwUeMTCt1fK\nMgqieHGvYd46vlgssNfrbYQATUOy5JFiuXJOY8iLumyiL/zQoUl90WobuSJGIBAIBAKBwBUmasvj\ng4gfEmV1D1Xp8Efnpt9FjwrEZD8KkzJKXnWYOo1ZcaLvzWmloMJDdMKyCbEZcDKylWmyaDp5f1td\nsbmb2hRZjTwtOVnPK16GTt2ui+bVtnOk0uBJ1XmnVPrvBXDi5OKVolPqNkJoWbmSNpwMw27GvHh+\nimtuFOKlZ4tyYXy2bCGQd9ikgMAyvOc832k/Mug3lvi8SfW4KSeJuJZ1AAAJ8UlEQVTeSNqgOMII\ndME4bHpqtLaiOVBQ+ZfAbDZTlymbhpQzvP7WY8grmE3z2xCTLRJKFqMA4kcK7dEGSr1iSGSUKftE\nvDROwVUD3jYW4k9NvUwmji8hRd9dVFpKSXPkcnVw85eylUl4j7eTyBOFFrwKoROfWaDEaYNGnl7m\n1XK5VDar1+tFiY/q81xwbYsXgYRlwXUmGMfx1oQUop2YIi6GjVidbGVbbUnCwHADNuYVhqEaF9qY\nXIUUbeaUF+W7/YFtCM0ivOc83+ngZ7pO6qoODXklQO87zVmXHlJpkAjK+J1aW/GwuKercjZAaShM\nyFiPIc+NshGgJi0SGLTaZpuiKdcYtImmE9nKgvdByfCGeNlYqGrO5mSM6E9I2ZyG+EJvWblX2lZF\nXik+Tp7aMhTCogIv+WV6UVDEydhW6TsdTTZB3jl4W9eLcPBFL2/BorlZr9eLflUpTqZiioSfhouO\nk5YXF8C64gAqerDoGG/MizY84hIEgfUGyO+l1Pysk61oTAy63ytYeKWcx5A2VeqdZfp5w75KVpyK\nOoK7IqNKVGsrXqDjcDedEUhUl23IyddPEz1Bh4VareajQTUgfiQhxROmfQkpnn1v2fjSeBHn6teW\nNw85GrpZc2GTzL1YLFw4l7ZV2itFHbl5/xWP10AUguaGbZjTsJFnKVtRAYJFeFP9TLVaVa0ufPNK\ng989mNdGggSrJpG6NCcTMUUHHsMxL2WrdLVl1niQiLD0NljzCsNQbfSVSsVYtPH7Eku0ZSiEreeH\nYOiVch5DHiKnS4HzxoiqDgH0ffw0vDKRd0edC6j6mfdW+nl9Md6jiw5OrqAE9oyDtvMYmoopHk3y\n1KAaED+SkKIXqqBhmBW4EbfR6RkxWaXnMrGo4ZpFyDEX9GKYNnDj9jH0AHnZhEnA1Ov1RIVZtVp1\ndRc7ccrxBGphGIZwthUvUTYVufTuGAhpL2OI+EGU6LygdIp1rICxQlpMpQVVToWsLSdjXuv1WuWV\nZPVAo7G2FBHOvHj1bqfT0W765CVuNBpby1WkNaDVaiVuLyjqMbderzcamPq8Egnxw113PAeQXwPD\nx9K235QLJ5ubK7KwXq9VNR2vqAuCgNYyra2ocpNSZcqm3RC2mX7DLynOWoNcWyRoeAHiRxJSJvkW\npuAnes1m5DxI/MTpIvx4ODBjUlpzopwIm5eNJ3YbTCJvmzBfnCgJ3qbDuCGvQlgkjidg2MjTyVZ8\nTlBOVBGWy2XieopdjqFJjx1+ktZs2l44IW52QOc2sbjip4iTFa8oitR8p01ruVzmXaK+dV6mV/3Q\nQcPD3WO5iOM40dbD9XFIgLcaw8VikWhCS0+73Vbl/p68+7mw6Hy/8XNZdzweHBykw3NGtuJrt6uo\nS4PbNkOMlh7DrH2OvPYuLRIMeAHiRxJSiMlBcgnvRVGU6Ia9zTvRuKvcpjs2YjJPKUcpO3HizQFN\nJjn3ZOxyE+Yu3IIwlCmcOFESZxAE6qRrsqEZNvJ0tlW6NxPdI8effr+/0WW6ZG6G1RjyDbBIAFvk\ndniZV4Q8MdXpdGxOnt7mexRFar2ggyJ5oz7GRbyr1arwQmLLkJ6OVy7CMNyY2/QU3dGZbvxYcBmu\ntzFE/ODV4f2d+OMhLFQI0yR7uqaI24nuxit71166EbZjT0IF2q/JI7WtMeRiajqdJqJBtlEJA16A\n+BGFFE9sNAgHKNAN8bQBVSqVrYereFk6bRYmgmA8HiueDvceacEr36g1BPcGRFGE0+k0EVbb9caC\neOkdcK0iMuSl/0GD0261Wk0s4nSCKdGsTYswDBMHgiJug8HAxo5eF6WiMHI6Z8uRlzOyrpOxvBbI\n63ynC7zTY2hZWOGNVxzHiTlGBQr8omLDkJ6O18eE1zEkUJ7RcDhU64FpmwQNr0IUVXhSuDHdDDMI\nAhwOh17bffAWCrVazalZKM/bo5YuZQSeCdIHLOLvuA9ptc0uRdSGMdILIIkBPhHW6zVOp1OV9JY+\nwVuEiUoNUhzHG5fyttttHA6H6ib6xWKB8/l8o4W+5k6+UhNnOp1mXi6ZfoIgsEnu3sqi5AHOnAaD\ngVMnfYOTuhdbkejNOrE7XvXhhZeu0oyH1kuK9NLgawl5EQxP0d7nexzHqq8O8dnlVU1ZGI/HCe8K\n91RZhtuv0trgldd0OrW9RNuZE40PrdsWoTtXThu8aN+i76UCBpO5HMcxDgYDtT8ZCDGvY2jb69GR\nFyBKZ3OBQCAQCAQCd5ioLY9PJiaTiXKbmjzNZtNn6byV2l0ul+qUqXuCICiTj2SMKIpwNBplelkO\nDg5wNBrZujR3crpzwFY58YuyF4uF8hBtI9l8ByjNizfYrNVqOBqNlG3G43EiZLzN/BobpF36JTl5\n4dXtdl37AnnndXJykrj5AMCsy7ghr4+Jq/YeakHe3nq9vpEDxfOgPHLK5UXta/je1uv1cDabbVRa\njsdj7HQ6iUhJt9s12YO8j+FgMPDRC0urbT5DxC3KtA3kftn5+Tk8fvwYfvjhB1gsFnBxcQEAANVq\nFW7dugXNZhMAAO7evQs3btxw+e7PXHjl4f379/Ds2TP48ccfAQDg5ORE/Tfi22q1yvDa6cCk4NVW\nHiG2MocXXqenp9DtduHly5eZ/71arcJ3330H3W63LC9vtnr48CF88803UKvV4PXr12U4AXji9fbt\nW/jyyy9tf2wrvN6/fw8AAPfu3YOLi4vE2lWS15Wf71uAs63Oz8/h+vXr8O7dOwAACIIAHjx4AL//\n/e8BAFzmk46TltfR0REcHh7CmzdvjL6o2WzCw4cP4fbt21vltWUU8br8wL4IqR3gqg3SPnIC2E9e\n+8gJ4BPi9eTJE3j+/DmsVisAALh58yZ89dVX8Lvf/Q6++OILH7y82urhw4dwfHwMx8fHJh//nxjD\nLHgWeJ+0rRxRylbffvst/Oc//4EHDx5Ap9PZNicAQ16np6fw9OnTzPfr+vXr8Otf/xpu375t6/S4\namP44QMipABgP3ntIyeA/eS1j5wAhFcWdjaGZ2dn0Gg0TD561WwFsJ+89pETwH7y2kdOAMIrC3sn\npAQCgUAgEAg+GUjVnkAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAg\nEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQ\nOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJ\nBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgEAoEjREgJBAKBQCAQOEKElEAgEAgE\nAoEjREgJBAKBQCAQOEKElEAgEAgEAoEj/h/qjfKa+/L9/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5e5c72490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Checking multiple training vectors by plotting images.\\nBe patient:\")\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "nrows=15\n",
    "ncols=15\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        plt.subplot(nrows, ncols, row*ncols+col + 1)\n",
    "        v = x_train[np.random.randint(len(x_train))]\n",
    "        plt.imshow(v.reshape(28, 28),interpolation='None', cmap='gray')\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), (60000, 1), (10000, 28, 28), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2d = np.reshape(x_train, (len(x_train), 28*28*1))\n",
    "x_test_2d = np.reshape(x_test, (len(x_test), 28*28*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(train, test):\n",
    "    estimator = KMeans(n_clusters=3, n_jobs=20)\n",
    "    return estimator.fit(train).predict(test)\n",
    "def kmeans_pca(train, test):\n",
    "    pca = PCA(n_components=3).fit(train)\n",
    "    estimator = KMeans(init=pca.components_, n_clusters=3, n_init=20)\n",
    "    return estimator.fit(train).predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 12.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = kmeans(x_train_2d, x_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4994\n"
     ]
    }
   ],
   "source": [
    "print(labeled_cluster_accuracy(y_test, kmeans(x_train_2d, x_test_2d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py:893: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=20\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 6.53 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = kmeans_pca(x_train_2d, x_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4995\n"
     ]
    }
   ],
   "source": [
    "print(labeled_cluster_accuracy(y_test, kmeans_pca(x_train_2d, x_test_2d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "EPOCH=500\n",
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single fully-connected neural layer as encoder and as decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 128  # 3 floats -> 3 floats represents 3 classes\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's also create a separate encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Crossentropy loss, and Adadelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_autoencoder = x_train.astype('float32') / 255.\n",
    "x_test_autoencoder = x_test.astype('float32') / 255.\n",
    "x_train_autoencoder = x_train_autoencoder.reshape((len(x_train), np.prod(x_train_autoencoder.shape[1:])))\n",
    "x_test_autoencoder = x_test_autoencoder.reshape((len(x_test), np.prod(x_test_autoencoder.shape[1:])))\n",
    "print(x_train_autoencoder.shape)\n",
    "print(x_test_autoencoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.3380Epoch 00001: val_loss improved from inf to 0.27442, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3376 - val_loss: 0.2744\n",
      "Epoch 2/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.2755Epoch 00002: val_loss improved from 0.27442 to 0.22834, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.2740 - val_loss: 0.2283\n",
      "Epoch 3/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.2540Epoch 00003: val_loss improved from 0.22834 to 0.21001, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.2533 - val_loss: 0.2100\n",
      "Epoch 4/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.2408Epoch 00004: val_loss improved from 0.21001 to 0.19867, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.2400 - val_loss: 0.1987\n",
      "Epoch 5/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.2260Epoch 00005: val_loss improved from 0.19867 to 0.18599, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.2254 - val_loss: 0.1860\n",
      "Epoch 6/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.2114Epoch 00006: val_loss improved from 0.18599 to 0.17432, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.2107 - val_loss: 0.1743\n",
      "Epoch 7/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1976Epoch 00007: val_loss improved from 0.17432 to 0.16309, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1970 - val_loss: 0.1631\n",
      "Epoch 8/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.1851Epoch 00008: val_loss improved from 0.16309 to 0.15349, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1847 - val_loss: 0.1535\n",
      "Epoch 9/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1746Epoch 00009: val_loss improved from 0.15349 to 0.14537, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1741 - val_loss: 0.1454\n",
      "Epoch 10/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.1653Epoch 00010: val_loss improved from 0.14537 to 0.13850, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1649 - val_loss: 0.1385\n",
      "Epoch 11/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.1576Epoch 00011: val_loss improved from 0.13850 to 0.13336, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1573 - val_loss: 0.1334\n",
      "Epoch 12/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1510Epoch 00012: val_loss improved from 0.13336 to 0.12880, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1508 - val_loss: 0.1288\n",
      "Epoch 13/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1455Epoch 00013: val_loss improved from 0.12880 to 0.12456, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.1454 - val_loss: 0.1246\n",
      "Epoch 14/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1410Epoch 00014: val_loss improved from 0.12456 to 0.12146, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1408 - val_loss: 0.1215\n",
      "Epoch 15/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.1371Epoch 00015: val_loss improved from 0.12146 to 0.11854, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1369 - val_loss: 0.1185\n",
      "Epoch 16/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1337Epoch 00016: val_loss improved from 0.11854 to 0.11567, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1335 - val_loss: 0.1157\n",
      "Epoch 17/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1305Epoch 00017: val_loss improved from 0.11567 to 0.11343, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1305 - val_loss: 0.1134\n",
      "Epoch 18/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1279Epoch 00018: val_loss improved from 0.11343 to 0.11159, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1278 - val_loss: 0.1116\n",
      "Epoch 19/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1256Epoch 00019: val_loss improved from 0.11159 to 0.10963, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1254 - val_loss: 0.1096\n",
      "Epoch 20/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.1234Epoch 00020: val_loss improved from 0.10963 to 0.10812, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1233 - val_loss: 0.1081\n",
      "Epoch 21/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1213Epoch 00021: val_loss improved from 0.10812 to 0.10676, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.1214 - val_loss: 0.1068\n",
      "Epoch 22/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.1196Epoch 00022: val_loss improved from 0.10676 to 0.10555, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1196 - val_loss: 0.1056\n",
      "Epoch 23/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.1179Epoch 00023: val_loss improved from 0.10555 to 0.10421, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1179 - val_loss: 0.1042\n",
      "Epoch 24/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.1164Epoch 00024: val_loss improved from 0.10421 to 0.10362, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1164 - val_loss: 0.1036\n",
      "Epoch 25/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1150Epoch 00025: val_loss improved from 0.10362 to 0.10213, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1150 - val_loss: 0.1021\n",
      "Epoch 26/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1138Epoch 00026: val_loss improved from 0.10213 to 0.10135, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1137 - val_loss: 0.1013\n",
      "Epoch 27/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.1125Epoch 00027: val_loss improved from 0.10135 to 0.10067, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1124 - val_loss: 0.1007\n",
      "Epoch 28/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1114Epoch 00028: val_loss improved from 0.10067 to 0.10014, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1113 - val_loss: 0.1001\n",
      "Epoch 29/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1103Epoch 00029: val_loss improved from 0.10014 to 0.09951, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1102 - val_loss: 0.0995\n",
      "Epoch 30/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1093Epoch 00030: val_loss improved from 0.09951 to 0.09877, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1092 - val_loss: 0.0988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1082Epoch 00031: val_loss improved from 0.09877 to 0.09820, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1082 - val_loss: 0.0982\n",
      "Epoch 32/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.1074Epoch 00032: val_loss improved from 0.09820 to 0.09738, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1073 - val_loss: 0.0974\n",
      "Epoch 33/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1065Epoch 00033: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1065 - val_loss: 0.0974\n",
      "Epoch 34/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.1057Epoch 00034: val_loss improved from 0.09738 to 0.09670, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.1057 - val_loss: 0.0967\n",
      "Epoch 35/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1049Epoch 00035: val_loss improved from 0.09670 to 0.09656, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1050 - val_loss: 0.0966\n",
      "Epoch 36/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.1043Epoch 00036: val_loss improved from 0.09656 to 0.09596, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1043 - val_loss: 0.0960\n",
      "Epoch 37/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1036Epoch 00037: val_loss improved from 0.09596 to 0.09557, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1036 - val_loss: 0.0956\n",
      "Epoch 38/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1030Epoch 00038: val_loss improved from 0.09557 to 0.09513, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1030 - val_loss: 0.0951\n",
      "Epoch 39/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.1024Epoch 00039: val_loss improved from 0.09513 to 0.09511, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1024 - val_loss: 0.0951\n",
      "Epoch 40/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.1019Epoch 00040: val_loss improved from 0.09511 to 0.09452, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1018 - val_loss: 0.0945\n",
      "Epoch 41/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1013Epoch 00041: val_loss improved from 0.09452 to 0.09446, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1013 - val_loss: 0.0945\n",
      "Epoch 42/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.1008Epoch 00042: val_loss improved from 0.09446 to 0.09410, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.1008 - val_loss: 0.0941\n",
      "Epoch 43/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1003Epoch 00043: val_loss improved from 0.09410 to 0.09367, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.1003 - val_loss: 0.0937\n",
      "Epoch 44/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0999Epoch 00044: val_loss improved from 0.09367 to 0.09355, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0998 - val_loss: 0.0935\n",
      "Epoch 45/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0994Epoch 00045: val_loss improved from 0.09355 to 0.09348, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0994 - val_loss: 0.0935\n",
      "Epoch 46/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0990Epoch 00046: val_loss improved from 0.09348 to 0.09299, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0989 - val_loss: 0.0930\n",
      "Epoch 47/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0985Epoch 00047: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0985 - val_loss: 0.0931\n",
      "Epoch 48/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0982Epoch 00048: val_loss improved from 0.09299 to 0.09282, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0981 - val_loss: 0.0928\n",
      "Epoch 49/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0979Epoch 00049: val_loss improved from 0.09282 to 0.09239, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0978 - val_loss: 0.0924\n",
      "Epoch 50/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0974Epoch 00050: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0974 - val_loss: 0.0925\n",
      "Epoch 51/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0971Epoch 00051: val_loss improved from 0.09239 to 0.09228, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0970 - val_loss: 0.0923\n",
      "Epoch 52/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0967Epoch 00052: val_loss improved from 0.09228 to 0.09218, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0967 - val_loss: 0.0922\n",
      "Epoch 53/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0964Epoch 00053: val_loss improved from 0.09218 to 0.09184, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0964 - val_loss: 0.0918\n",
      "Epoch 54/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0961Epoch 00054: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0961 - val_loss: 0.0919\n",
      "Epoch 55/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0958Epoch 00055: val_loss improved from 0.09184 to 0.09178, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0958 - val_loss: 0.0918\n",
      "Epoch 56/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0955Epoch 00056: val_loss improved from 0.09178 to 0.09164, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0955 - val_loss: 0.0916\n",
      "Epoch 57/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0952Epoch 00057: val_loss improved from 0.09164 to 0.09135, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0952 - val_loss: 0.0914\n",
      "Epoch 58/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0949Epoch 00058: val_loss improved from 0.09135 to 0.09135, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0949 - val_loss: 0.0913\n",
      "Epoch 59/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0946Epoch 00059: val_loss improved from 0.09135 to 0.09127, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0946 - val_loss: 0.0913\n",
      "Epoch 60/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0944Epoch 00060: val_loss improved from 0.09127 to 0.09102, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0943 - val_loss: 0.0910\n",
      "Epoch 61/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0941Epoch 00061: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0941 - val_loss: 0.0910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0939Epoch 00062: val_loss improved from 0.09102 to 0.09090, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0938 - val_loss: 0.0909\n",
      "Epoch 63/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0936Epoch 00063: val_loss improved from 0.09090 to 0.09088, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0936 - val_loss: 0.0909\n",
      "Epoch 64/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0934Epoch 00064: val_loss improved from 0.09088 to 0.09066, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0933 - val_loss: 0.0907\n",
      "Epoch 65/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0931Epoch 00065: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0931 - val_loss: 0.0907\n",
      "Epoch 66/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0929Epoch 00066: val_loss improved from 0.09066 to 0.09054, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0929 - val_loss: 0.0905\n",
      "Epoch 67/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0927Epoch 00067: val_loss improved from 0.09054 to 0.09046, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0927 - val_loss: 0.0905\n",
      "Epoch 68/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0924Epoch 00068: val_loss improved from 0.09046 to 0.09032, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0924 - val_loss: 0.0903\n",
      "Epoch 69/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0923Epoch 00069: val_loss improved from 0.09032 to 0.09027, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0922 - val_loss: 0.0903\n",
      "Epoch 70/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0921Epoch 00070: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0920 - val_loss: 0.0903\n",
      "Epoch 71/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0918Epoch 00071: val_loss improved from 0.09027 to 0.09015, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0918 - val_loss: 0.0902\n",
      "Epoch 72/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0916Epoch 00072: val_loss improved from 0.09015 to 0.09010, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0916 - val_loss: 0.0901\n",
      "Epoch 73/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0915Epoch 00073: val_loss improved from 0.09010 to 0.09001, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0914 - val_loss: 0.0900\n",
      "Epoch 74/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0913Epoch 00074: val_loss improved from 0.09001 to 0.08987, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0913 - val_loss: 0.0899\n",
      "Epoch 75/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0911Epoch 00075: val_loss improved from 0.08987 to 0.08983, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0911 - val_loss: 0.0898\n",
      "Epoch 76/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0909Epoch 00076: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0909 - val_loss: 0.0899\n",
      "Epoch 77/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0907Epoch 00077: val_loss improved from 0.08983 to 0.08976, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0907 - val_loss: 0.0898\n",
      "Epoch 78/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0906Epoch 00078: val_loss improved from 0.08976 to 0.08975, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0906 - val_loss: 0.0897\n",
      "Epoch 79/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.0903Epoch 00079: val_loss improved from 0.08975 to 0.08959, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0904 - val_loss: 0.0896\n",
      "Epoch 80/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0903Epoch 00080: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0902 - val_loss: 0.0897\n",
      "Epoch 81/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0901Epoch 00081: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0901 - val_loss: 0.0896\n",
      "Epoch 82/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0899Epoch 00082: val_loss improved from 0.08959 to 0.08947, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0899 - val_loss: 0.0895\n",
      "Epoch 83/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0898Epoch 00083: val_loss improved from 0.08947 to 0.08946, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0898 - val_loss: 0.0895\n",
      "Epoch 84/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0896Epoch 00084: val_loss improved from 0.08946 to 0.08934, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0896 - val_loss: 0.0893\n",
      "Epoch 85/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0895Epoch 00085: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0895 - val_loss: 0.0894\n",
      "Epoch 86/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0894Epoch 00086: val_loss improved from 0.08934 to 0.08930, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0893 - val_loss: 0.0893\n",
      "Epoch 87/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.0892Epoch 00087: val_loss improved from 0.08930 to 0.08927, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0892 - val_loss: 0.0893\n",
      "Epoch 88/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0891Epoch 00088: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0891 - val_loss: 0.0893\n",
      "Epoch 89/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0889Epoch 00089: val_loss improved from 0.08927 to 0.08911, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0889 - val_loss: 0.0891\n",
      "Epoch 90/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0888Epoch 00090: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0888 - val_loss: 0.0891\n",
      "Epoch 91/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0887Epoch 00091: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0887 - val_loss: 0.0892\n",
      "Epoch 92/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.0885Epoch 00092: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0886 - val_loss: 0.0892\n",
      "Epoch 93/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0884Epoch 00093: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0884 - val_loss: 0.0891\n",
      "Epoch 94/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0883Epoch 00094: val_loss improved from 0.08911 to 0.08890, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0883 - val_loss: 0.0889\n",
      "Epoch 95/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0882Epoch 00095: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0882 - val_loss: 0.0890\n",
      "Epoch 96/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0881Epoch 00096: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0881 - val_loss: 0.0891\n",
      "Epoch 97/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0880Epoch 00097: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0880 - val_loss: 0.0889\n",
      "Epoch 98/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0879Epoch 00098: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0879 - val_loss: 0.0890\n",
      "Epoch 99/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0878Epoch 00099: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0878 - val_loss: 0.0889\n",
      "Epoch 100/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0877Epoch 00100: val_loss improved from 0.08890 to 0.08876, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0877 - val_loss: 0.0888\n",
      "Epoch 101/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0876Epoch 00101: val_loss improved from 0.08876 to 0.08872, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0876 - val_loss: 0.0887\n",
      "Epoch 102/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0875Epoch 00102: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0875 - val_loss: 0.0888\n",
      "Epoch 103/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0874Epoch 00103: val_loss improved from 0.08872 to 0.08871, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0874 - val_loss: 0.0887\n",
      "Epoch 104/500\n",
      "55552/60000 [==========================>...] - ETA: 0s - loss: 0.0873Epoch 00104: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0873 - val_loss: 0.0889\n",
      "Epoch 105/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0872Epoch 00105: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0872 - val_loss: 0.0888\n",
      "Epoch 106/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0871Epoch 00106: val_loss improved from 0.08871 to 0.08865, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0871 - val_loss: 0.0886\n",
      "Epoch 107/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0870Epoch 00107: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0870 - val_loss: 0.0887\n",
      "Epoch 108/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0869Epoch 00108: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0869 - val_loss: 0.0889\n",
      "Epoch 109/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0868Epoch 00109: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0868 - val_loss: 0.0886\n",
      "Epoch 110/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0868Epoch 00110: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0868 - val_loss: 0.0887\n",
      "Epoch 111/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0867Epoch 00111: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0867 - val_loss: 0.0887\n",
      "Epoch 112/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0866Epoch 00112: val_loss improved from 0.08865 to 0.08852, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0866 - val_loss: 0.0885\n",
      "Epoch 113/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0865Epoch 00113: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0865 - val_loss: 0.0887\n",
      "Epoch 114/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0864Epoch 00114: val_loss improved from 0.08852 to 0.08848, saving model to weights.best.simple_model.hdf5\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0864 - val_loss: 0.0885\n",
      "Epoch 115/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0863Epoch 00115: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0864 - val_loss: 0.0887\n",
      "Epoch 116/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0863Epoch 00116: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0863 - val_loss: 0.0887\n",
      "Epoch 117/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0862Epoch 00117: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0862 - val_loss: 0.0886\n",
      "Epoch 118/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0861Epoch 00118: val_loss did not improve\n",
      "\n",
      "Epoch 00118: reducing learning rate to 0.5.\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.0861 - val_loss: 0.0886\n",
      "Epoch 119/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0860Epoch 00119: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0861 - val_loss: 0.0886\n",
      "Epoch 120/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0860Epoch 00120: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0860 - val_loss: 0.0887\n",
      "Epoch 121/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0860Epoch 00121: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0860 - val_loss: 0.0886\n",
      "Epoch 122/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0859Epoch 00122: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0859 - val_loss: 0.0886\n",
      "Epoch 123/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0859Epoch 00123: val_loss did not improve\n",
      "\n",
      "Epoch 00123: reducing learning rate to 0.25.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0859 - val_loss: 0.0886\n",
      "Epoch 124/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0859Epoch 00124: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0859 - val_loss: 0.0885\n",
      "Epoch 125/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0859Epoch 00125: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0859 - val_loss: 0.0885\n",
      "Epoch 126/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0858Epoch 00126: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 127/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00127: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 128/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00128: val_loss did not improve\n",
      "\n",
      "Epoch 00128: reducing learning rate to 0.125.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0858 - val_loss: 0.0886\n",
      "Epoch 129/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00129: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0858 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0858Epoch 00130: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 131/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00131: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 132/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00132: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 133/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0858Epoch 00133: val_loss did not improve\n",
      "\n",
      "Epoch 00133: reducing learning rate to 0.0625.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0858 - val_loss: 0.0885\n",
      "Epoch 134/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00134: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 135/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00135: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 136/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00136: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 137/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00137: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 138/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00138: val_loss did not improve\n",
      "\n",
      "Epoch 00138: reducing learning rate to 0.03125.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 139/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00139: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 140/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00140: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 141/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00141: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 142/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00142: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 143/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00143: val_loss did not improve\n",
      "\n",
      "Epoch 00143: reducing learning rate to 0.015625.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 144/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00144: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 145/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00145: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 146/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00146: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 147/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00147: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 148/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00148: val_loss did not improve\n",
      "\n",
      "Epoch 00148: reducing learning rate to 0.0078125.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 149/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00149: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 150/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00150: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 151/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00151: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 152/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00152: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 153/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00153: val_loss did not improve\n",
      "\n",
      "Epoch 00153: reducing learning rate to 0.00390625.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 154/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00154: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 155/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00155: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 156/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00156: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 157/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00157: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 158/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00158: val_loss did not improve\n",
      "\n",
      "Epoch 00158: reducing learning rate to 0.001953125.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 159/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00159: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 160/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00160: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 161/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00161: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 162/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00162: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 163/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00163: val_loss did not improve\n",
      "\n",
      "Epoch 00163: reducing learning rate to 0.0009765625.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 164/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00164: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 165/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00165: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 166/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00166: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00167: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 168/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00168: val_loss did not improve\n",
      "\n",
      "Epoch 00168: reducing learning rate to 0.00048828125.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 169/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00169: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 170/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00170: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 171/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00171: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 172/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00172: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 173/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00173: val_loss did not improve\n",
      "\n",
      "Epoch 00173: reducing learning rate to 0.000244140625.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 174/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00174: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 175/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00175: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 176/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00176: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 177/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00177: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 178/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00178: val_loss did not improve\n",
      "\n",
      "Epoch 00178: reducing learning rate to 0.0001220703125.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 179/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00179: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 180/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00180: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 181/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00181: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 182/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00182: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 183/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00183: val_loss did not improve\n",
      "\n",
      "Epoch 00183: reducing learning rate to 6.103515625e-05.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 184/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00184: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 185/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00185: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 186/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00186: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 187/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00187: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 188/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00188: val_loss did not improve\n",
      "\n",
      "Epoch 00188: reducing learning rate to 3.0517578125e-05.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 189/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00189: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 190/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00190: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 191/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00191: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 192/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00192: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 193/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00193: val_loss did not improve\n",
      "\n",
      "Epoch 00193: reducing learning rate to 1.52587890625e-05.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 194/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00194: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 195/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00195: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 196/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00196: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 197/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00197: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 198/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00198: val_loss did not improve\n",
      "\n",
      "Epoch 00198: reducing learning rate to 7.62939453125e-06.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 199/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00199: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 200/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00200: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 201/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00201: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 202/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00202: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 203/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00203: val_loss did not improve\n",
      "\n",
      "Epoch 00203: reducing learning rate to 3.81469726562e-06.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00204: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 205/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00205: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 206/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00206: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 207/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00207: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 208/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00208: val_loss did not improve\n",
      "\n",
      "Epoch 00208: reducing learning rate to 1.90734863281e-06.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 209/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00209: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 210/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00210: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 211/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00211: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 212/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00212: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 213/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00213: val_loss did not improve\n",
      "\n",
      "Epoch 00213: reducing learning rate to 9.53674316406e-07.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 214/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00214: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 215/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00215: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 216/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00216: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 217/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00217: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 218/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00218: val_loss did not improve\n",
      "\n",
      "Epoch 00218: reducing learning rate to 4.76837158203e-07.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 219/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00219: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 220/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00220: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 221/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00221: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 222/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00222: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 223/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00223: val_loss did not improve\n",
      "\n",
      "Epoch 00223: reducing learning rate to 2.38418579102e-07.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 224/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00224: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 225/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00225: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 226/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00226: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 227/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00227: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 228/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00228: val_loss did not improve\n",
      "\n",
      "Epoch 00228: reducing learning rate to 1.19209289551e-07.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 229/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00229: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 230/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00230: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 231/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00231: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 232/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00232: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 233/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00233: val_loss did not improve\n",
      "\n",
      "Epoch 00233: reducing learning rate to 5.96046447754e-08.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 234/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00234: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 235/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00235: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 236/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00236: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 237/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00237: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 238/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00238: val_loss did not improve\n",
      "\n",
      "Epoch 00238: reducing learning rate to 2.98023223877e-08.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 239/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00239: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 240/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00240: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 241/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00241: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 242/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00242: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 243/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00243: val_loss did not improve\n",
      "\n",
      "Epoch 00243: reducing learning rate to 1.49011611938e-08.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 244/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00244: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 245/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00245: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 246/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00246: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 247/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00247: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 248/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00248: val_loss did not improve\n",
      "\n",
      "Epoch 00248: reducing learning rate to 7.45058059692e-09.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 249/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00249: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 250/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00250: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 251/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00251: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 252/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00252: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 253/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00253: val_loss did not improve\n",
      "\n",
      "Epoch 00253: reducing learning rate to 3.72529029846e-09.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 254/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00254: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 255/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00255: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 256/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00256: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 257/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00257: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 258/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00258: val_loss did not improve\n",
      "\n",
      "Epoch 00258: reducing learning rate to 1.86264514923e-09.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 259/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00259: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 260/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00260: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 261/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00261: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 262/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00262: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 263/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00263: val_loss did not improve\n",
      "\n",
      "Epoch 00263: reducing learning rate to 9.31322574615e-10.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 264/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00264: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 265/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00265: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 266/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00266: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 267/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00267: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 268/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00268: val_loss did not improve\n",
      "\n",
      "Epoch 00268: reducing learning rate to 4.65661287308e-10.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 269/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00269: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 270/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00270: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 271/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00271: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 272/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00272: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 273/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00273: val_loss did not improve\n",
      "\n",
      "Epoch 00273: reducing learning rate to 2.32830643654e-10.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 274/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00274: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 275/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00275: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 276/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00276: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00277: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 278/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00278: val_loss did not improve\n",
      "\n",
      "Epoch 00278: reducing learning rate to 1.16415321827e-10.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 279/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00279: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 280/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00280: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 281/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00281: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 282/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00282: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 283/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00283: val_loss did not improve\n",
      "\n",
      "Epoch 00283: reducing learning rate to 5.82076609135e-11.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 284/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00284: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 285/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00285: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 286/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00286: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 287/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00287: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 288/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00288: val_loss did not improve\n",
      "\n",
      "Epoch 00288: reducing learning rate to 2.91038304567e-11.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 289/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00289: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 290/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00290: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 291/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00291: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 292/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00292: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 293/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00293: val_loss did not improve\n",
      "\n",
      "Epoch 00293: reducing learning rate to 1.45519152284e-11.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 294/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0858Epoch 00294: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 295/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00295: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 296/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00296: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 297/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00297: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 298/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00298: val_loss did not improve\n",
      "\n",
      "Epoch 00298: reducing learning rate to 7.27595761418e-12.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 299/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00299: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 300/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00300: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 301/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00301: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 302/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00302: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 303/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00303: val_loss did not improve\n",
      "\n",
      "Epoch 00303: reducing learning rate to 3.63797880709e-12.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 304/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00304: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 305/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00305: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 306/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00306: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 307/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00307: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 308/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00308: val_loss did not improve\n",
      "\n",
      "Epoch 00308: reducing learning rate to 1.81898940355e-12.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 309/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00309: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 310/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00310: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 311/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00311: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 312/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00312: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00313: val_loss did not improve\n",
      "\n",
      "Epoch 00313: reducing learning rate to 9.09494701773e-13.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 314/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00314: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 315/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00315: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 316/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00316: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 317/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00317: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 318/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00318: val_loss did not improve\n",
      "\n",
      "Epoch 00318: reducing learning rate to 4.54747350886e-13.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 319/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00319: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 320/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00320: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 321/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00321: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 322/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00322: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 323/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00323: val_loss did not improve\n",
      "\n",
      "Epoch 00323: reducing learning rate to 2.27373675443e-13.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 324/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00324: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 325/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00325: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 326/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00326: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 327/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00327: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 328/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00328: val_loss did not improve\n",
      "\n",
      "Epoch 00328: reducing learning rate to 1.13686837722e-13.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 329/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00329: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 330/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00330: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 331/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00331: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 332/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00332: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 333/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00333: val_loss did not improve\n",
      "\n",
      "Epoch 00333: reducing learning rate to 5.68434188608e-14.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 334/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00334: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 335/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00335: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 336/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00336: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 337/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00337: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 338/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00338: val_loss did not improve\n",
      "\n",
      "Epoch 00338: reducing learning rate to 2.84217094304e-14.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 339/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00339: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 340/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00340: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 341/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00341: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 342/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00342: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 343/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00343: val_loss did not improve\n",
      "\n",
      "Epoch 00343: reducing learning rate to 1.42108547152e-14.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 344/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00344: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 345/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00345: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 346/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00346: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 347/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00347: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 348/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00348: val_loss did not improve\n",
      "\n",
      "Epoch 00348: reducing learning rate to 7.1054273576e-15.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 349/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00349: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 350/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00350: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 351/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00351: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 352/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00352: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 353/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00353: val_loss did not improve\n",
      "\n",
      "Epoch 00353: reducing learning rate to 3.5527136788e-15.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 354/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00354: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 355/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00355: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 356/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00356: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 357/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00357: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 358/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00358: val_loss did not improve\n",
      "\n",
      "Epoch 00358: reducing learning rate to 1.7763568394e-15.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 359/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00359: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 360/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00360: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 361/500\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00361: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 362/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00362: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 363/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00363: val_loss did not improve\n",
      "\n",
      "Epoch 00363: reducing learning rate to 8.881784197e-16.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 364/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00364: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 365/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00365: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 366/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00366: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 367/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00367: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 368/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00368: val_loss did not improve\n",
      "\n",
      "Epoch 00368: reducing learning rate to 4.4408920985e-16.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 369/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00369: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 370/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00370: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 371/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00371: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 372/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00372: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 373/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00373: val_loss did not improve\n",
      "\n",
      "Epoch 00373: reducing learning rate to 2.22044604925e-16.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 374/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00374: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 375/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00375: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 376/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00376: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 377/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00377: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 378/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00378: val_loss did not improve\n",
      "\n",
      "Epoch 00378: reducing learning rate to 1.11022302463e-16.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 379/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00379: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 380/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00380: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 381/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00381: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 382/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00382: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 383/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00383: val_loss did not improve\n",
      "\n",
      "Epoch 00383: reducing learning rate to 5.55111512313e-17.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 384/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00384: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 385/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00385: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00386: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 387/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00387: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 388/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0858Epoch 00388: val_loss did not improve\n",
      "\n",
      "Epoch 00388: reducing learning rate to 2.77555756156e-17.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 389/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00389: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 390/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00390: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 391/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00391: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 392/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00392: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 393/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00393: val_loss did not improve\n",
      "\n",
      "Epoch 00393: reducing learning rate to 1.38777878078e-17.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 394/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00394: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 395/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00395: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 396/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00396: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 397/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00397: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 398/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00398: val_loss did not improve\n",
      "\n",
      "Epoch 00398: reducing learning rate to 6.93889390391e-18.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 399/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00399: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 400/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00400: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 401/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00401: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 402/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00402: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 403/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00403: val_loss did not improve\n",
      "\n",
      "Epoch 00403: reducing learning rate to 3.46944695195e-18.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 404/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00404: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 405/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00405: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 406/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00406: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 407/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00407: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 408/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00408: val_loss did not improve\n",
      "\n",
      "Epoch 00408: reducing learning rate to 1.73472347598e-18.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 409/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00409: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 410/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00410: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 411/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00411: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 412/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00412: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 413/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00413: val_loss did not improve\n",
      "\n",
      "Epoch 00413: reducing learning rate to 8.67361737988e-19.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 414/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00414: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 415/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00415: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 416/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00416: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 417/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00417: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 418/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00418: val_loss did not improve\n",
      "\n",
      "Epoch 00418: reducing learning rate to 4.33680868994e-19.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 419/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00419: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 420/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00420: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 421/500\n",
      "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0856Epoch 00421: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 422/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00422: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 423/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00423: val_loss did not improve\n",
      "\n",
      "Epoch 00423: reducing learning rate to 2.16840434497e-19.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 424/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00424: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 425/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00425: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 426/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00426: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 427/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00427: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 428/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00428: val_loss did not improve\n",
      "\n",
      "Epoch 00428: reducing learning rate to 1.08420217249e-19.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 429/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00429: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 430/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00430: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 431/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00431: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 432/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00432: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 433/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00433: val_loss did not improve\n",
      "\n",
      "Epoch 00433: reducing learning rate to 5.42101086243e-20.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 434/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00434: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 435/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00435: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 436/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00436: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 437/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00437: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 438/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00438: val_loss did not improve\n",
      "\n",
      "Epoch 00438: reducing learning rate to 2.71050543121e-20.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 439/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00439: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 440/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00440: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 441/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00441: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 442/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00442: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 443/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00443: val_loss did not improve\n",
      "\n",
      "Epoch 00443: reducing learning rate to 1.35525271561e-20.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 444/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00444: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 445/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00445: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 446/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00446: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 447/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00447: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 448/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00448: val_loss did not improve\n",
      "\n",
      "Epoch 00448: reducing learning rate to 6.77626357803e-21.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 449/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00449: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 450/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00450: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 451/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00451: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 452/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00452: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 453/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00453: val_loss did not improve\n",
      "\n",
      "Epoch 00453: reducing learning rate to 3.38813178902e-21.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 454/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00454: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 455/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00455: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 456/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00456: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 457/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00457: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 458/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00458: val_loss did not improve\n",
      "\n",
      "Epoch 00458: reducing learning rate to 1.69406589451e-21.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00459: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 460/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0856Epoch 00460: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 461/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00461: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 462/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00462: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 463/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00463: val_loss did not improve\n",
      "\n",
      "Epoch 00463: reducing learning rate to 8.47032947254e-22.\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 464/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00464: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 465/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00465: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 466/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00466: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 467/500\n",
      "53248/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00467: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 468/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00468: val_loss did not improve\n",
      "\n",
      "Epoch 00468: reducing learning rate to 4.23516473627e-22.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 469/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00469: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 470/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00470: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 471/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00471: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 472/500\n",
      "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00472: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 473/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00473: val_loss did not improve\n",
      "\n",
      "Epoch 00473: reducing learning rate to 2.11758236814e-22.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 474/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00474: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 475/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00475: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 476/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00476: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 477/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00477: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 478/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00478: val_loss did not improve\n",
      "\n",
      "Epoch 00478: reducing learning rate to 1.05879118407e-22.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 479/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00479: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 480/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00480: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 481/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0857Epoch 00481: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 482/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00482: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 483/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00483: val_loss did not improve\n",
      "\n",
      "Epoch 00483: reducing learning rate to 5.29395592034e-23.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 484/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0856Epoch 00484: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 485/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00485: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 486/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00486: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 487/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00487: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 488/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00488: val_loss did not improve\n",
      "\n",
      "Epoch 00488: reducing learning rate to 2.64697796017e-23.\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 489/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00489: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 490/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00490: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 491/500\n",
      "54016/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00491: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 492/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00492: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 493/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00493: val_loss did not improve\n",
      "\n",
      "Epoch 00493: reducing learning rate to 1.32348898008e-23.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 494/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00494: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 9us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 495/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00495: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 496/500\n",
      "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00496: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 497/500\n",
      "53504/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00497: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 498/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00498: val_loss did not improve\n",
      "\n",
      "Epoch 00498: reducing learning rate to 6.61744490042e-24.\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 499/500\n",
      "53760/60000 [=========================>....] - ETA: 0s - loss: 0.0857Epoch 00499: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 8us/step - loss: 0.0857 - val_loss: 0.0885\n",
      "Epoch 500/500\n",
      "54528/60000 [==========================>...] - ETA: 0s - loss: 0.0857Epoch 00500: val_loss did not improve\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 0.0857 - val_loss: 0.0885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb50f175bd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_cp_name = 'weights.best.simple_model.hdf5'\n",
    "autoencoder.fit(x_train_autoencoder, x_train_autoencoder,\n",
    "                epochs=EPOCH,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                callbacks=[ModelCheckpoint(simple_model_cp_name, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "                           ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, factor=0.5)],\n",
    "                validation_data=(x_test_autoencoder, x_test_autoencoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_autoencoder(autoencoder, x_test, y_test):\n",
    "    encoded_imgs = autoencoder.predict(x_test)\n",
    "    decoded_imgs = autoencoder.predict(encoded_imgs)\n",
    "    \n",
    "    n = 10  # how many digits we will display\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "    encoded_imgs = np.reshape(encoded_imgs, (encoded_imgs.shape[0], -1))\n",
    "    print(\"Accuracy = {}\".format(labeled_cluster_accuracy(y_test, kmeans_pca(encoded_imgs, encoded_imgs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xm8V3W1//HFr5tDZmhqoDkcR5wR\nRQUbxCnEzDlxKpVKqFTEbgSYgpSoZAyZA1oOpCKW4gjikKh5EcUQNUFLBNJwFtMEvXb5/XEfrvv+\nLM7efs+X77S/5/X8a335bM7Z57vn/fistTqsWLHCAAAAAAAA0Nj+X71XAAAAAAAAAJ+MlzgAAAAA\nAAAFwEscAAAAAACAAuAlDgAAAAAAQAHwEgcAAAAAAKAAeIkDAAAAAABQALzEAQAAAAAAKABe4gAA\nAAAAABQAL3EAAAAAAAAK4D/asvD666+/oqWlpUqrgiwLFy60N954o0MlfhbbsH6eeOKJN1asWLFB\nJX4W27E+OBabA8di8XEsNgeOxeLjWGwO7e1YXL58ucef+tSnPP70pz9dj9WpCI7F5lDqsdimlzgt\nLS02e/bs8tcKZenevXvFfhbbsH46dOiwqFI/i+1YHxyLzYFjsfg4FpsDx2LxcSw2h/Z2LM6fP9/j\njh07erzhhhvWY3UqgmOxOZR6LJJOBQAAAAAAUABtmokDAAAAAEBRPfnkkx7PmjXL47Fjx9ZjdYA2\nYyYOAAAAAABAAfASBwAAAAAAoAB4iQMAAAAAAFAA1MQBAAAAALQLWgfnsssu83jw4MHJckXuVoXm\nxkwcAAAAAACAAuAlDgAAAAAAQAEUPp3q7bffTj7fdtttHs+dO9fjOXPmZP6MNddcM/m85557enzg\ngQd63KNHj7LXEwCAepsxY4bHXbp0ScaYNo62ePHFFz3efPPN67gmANA2+lz4wQcfeDxs2LBkuauv\nvrpm64Ty6fVo0aJFmct17do1+bzuuutWbZ2qjZk4AAAAAAAABcBLHAAAAAAAgALgJQ4AAAAAAEAB\nFKYmzvz58z0+99xzPb7xxhsr/rvuvvvuVn9X586dk+WGDx/u8YABAyq+HkhpLQczs8mTJ3v8wAMP\nJGPPPfdcqz9j9dVXTz5rDiX1IIBseq598MEHk7F58+ZljmXp2LFj8nmXXXbx+NBDD03GjjnmGI85\nTlfNoEGDPI513rTNKmC2ct3BkSNHerx06VKP20PdiGuuucZjPSeZma2xxho1XhvUy/Llyz1+9NFH\ny/oZm222mcfUk6qPJ598stV/1+PczGzgwIEe630Kqkefw7XWbanPeuXSe6J4f3TiiSd63Cj7ATNx\nAAAAAAAACoCXOAAAAAAAAAXQsOlUI0aMSD5rWlMebQmurcJ79eqV+X8WLlyYfNbW5FOmTPE4tiz7\n/ve/7/G1116bjE2dOtXjIrcvqwdN2xgyZIjHeS3jSnXyyScnn0nNQHun6RKjR49OxnRa8SuvvFLR\n3/vOO+8knzUNK6ZkDR061GNNBzr//PMruk7N6tZbb/VYp5DH6eT9+/f3uFGmC6P2Lr/8co/jvZee\nByZNmlSzdaqHeHzo/UNMoT/22GM91uMoTslH49KyDRMmTPB42rRpyXKVTuHQ1OLDDz88Gevbt6/H\n+nyDttPyCWYr34Nk0edRvZZi1WjKVHw2K+d+U4+jvPuXvJR/TY+MqZLjxo3L/PlaXuWwww775JWt\nEGbiAAAAAAAAFAAvcQAAAAAAAAqgrulUWuHdLK32r9WoI53Ges455yRjlU6PGTt2rMexE5ZO64/T\nrvbdd1+P//jHP3pMatX/0mmrcRpdVrV/reZvlk5f3nvvvZMx3Zd0yqROc8aq01Sc8ePHJ2PaTUxT\nFmNanG67mPao1eDp4FA5el7TdIm86cVdunTxOHZm0e2Wl7qqYtcbneIau0Po9eCCCy7wOE65bQ8d\ncspRajqypgjPnDmzWquDBqDXWb2XiWORdnjs3bt35VesgcROKOqDDz5IPus5S2M9b5qZnXHGGR5r\nFz5Su2tD7z01Xd8s/7kji6bLxe6nefQY0+tuvPbp55jCod0ESdv7ZFoqI9LvVrttmqX7ReySW+r9\nDlamJU/y6L4dU+gr8f3rveisWbM8nj59erKcvgOIqbaaBqnPNDHluNLneWbiAAAAAAAAFAAvcQAA\nAAAAAAqAlzgAAAAAAAAFUNeaOLGVnrYb01ZhcaxeuZ+xDoTmvR100EHJmObL6VjMsV5jjTUquYoN\nTXN7ta5RzC3v3Lmzx5r/eNJJJ2X+bK3xYZbmGOt2om3uqtMWi7pNSm3XGOW1ltZaHtrCT1s+onVL\nlizxWOtHmWW3WNQaDWZpzYBKn3djfTBtyxhbNGoeuo7F+gFZ5472JrZB1euRfkeR1mmI323e+ReN\nKdadGjZsmMfaRrwt9tlnH4+bvcbfU089lTkW66koPXZiO2qtO6VxvL/UenC0li5fvDc888wzM5fV\n5w69Zuq2MKv8tVDr9MRzt9YajHU4evbs6bFe7/L2zfYsfn9KjzFt625mNnToUI9j/bA5c+ZUaO3a\nH71+xHqGffr08Vi/47z7l0qsh+4H8byr55J4/dRjTu+vd91112S5adOmeVyJ51Fm4gAAAAAAABQA\nL3EAAAAAAAAKoObpVDotTVOkzNKpjLGNWyOmwWirMG0jbmbWrVs3j3WKurYENFt5al4ziSkvWW1u\n4zT9MWPGeFzqdO2JEydmjmlLT7RdTKuILeE/Ftu8a8qcjsUUQp16OHny5GRMW/rp/qNtAM3MpkyZ\nkvnz24s4VVino8ZW3DolVaexNuq0fW0jqdPNNbXDLJ3uevrppydj7amFb15L8cGDB3scvxNNIdBr\ntVmaatfsaTRFptO8Y1qFprxqO+R4H6LnknifFlMum1lei/GYcqH3qJraotcws7RdsY7F5fTzZptt\nlozptVXXY/PNN89c3/ZE9+dx48ZlLhePDz031vIct+2222auk96/xvP6BRdc4HE8XyvSq/5XXjpV\n165dPY7p3HkpbXo/Ev8fShfvPTW9VM+Feu4zW/ldQa3E9dDrot5HxdIFei8bU/HKOX8zEwcAAAAA\nAKAAeIkDAAAAAABQALzEAQAAAAAAKIAOK1asKHnh7t27r5g9e3abf4nmEGprvthaWvOPNW+siDRP\nUlupx/ZoL774osdZtTy6d+9us2fP7lCJ9Sp3G5aq1BxdrcNRbutazdXX+h9m6fesrZbrqUOHDk+s\nWLGieyV+VrW3Y9Yxa5Yet9Vua5nVWjq2M9d9KLYqrKRGOxb1/LHXXnslY1oHJ27DqVOnelzk+iYx\nB13rTWhtLbO0TkKRjsVSZV1zzNLzYd41R1vWai03s7Q2Q2zbWw+NdizWkm4bbVNtVnob3UsvvdTj\nWBtpnXXW8Tjepy1YsMDjStRgabRjUe8XNtpoo2RM6zYuXbp0lX5P/F3XXnttMqa1jRYtWlTSz4t1\nJbQ1dmxhXkmNcCxqDUatHaO1n8zS+hpFr2Gi9QqzahWalf5s1WjHYqW1tLQkn/W4mjdvnsdao8gs\nvd7FFvVaC2vmzJke16s2YyMci5Wg50Zt0x3rO2qd2Vinpl6WL1/ucTwna42cWEdUn3dKPRaZiQMA\nAAAAAFAAvMQBAAAAAAAogJq0GNf2bDo1N059KnoKldJpmjrdLk511pSgIk7t1Cn8ZtVPoVI63TjS\nVpFoO52aHKfT67ardutKPSfoVMN4rtBpxbEFbhGPqzw6VTOvjbhO49QW7GbN04Y9TlXVdKrHHnus\n1qtTV6W2Fc/b9jo1uVu3bpljmqah1zdUTkwDHjZsmMd6vou0HXW8Rsbj5WOxjbie82MqZrO3sZ41\na1bmmKYbVoKmscVrqX6O20dTrzQ9KC6nn2MbeU2v6t+/v8cxlaRRxXvprPNfvEfNOgaKSO/F9Pof\n78N128e2xs3u7bff9jimJWqqXd5+r9/fxIkTkzHdD/UaGY83tI2eGzWdTdt3m6Vp3r179/a4ntcp\nvceaNGlSMqapYbH9eDxXlYKZOAAAAAAAAAXASxwAAAAAAIACqEo6lU5fM1t5OtHH2kvKy7e//W2P\n4xRQnf5flLQPneYdO2Mo7VxktuopVNpVxSz97mIHgmp2YmhG8ZiN07LVqFGjqr06rdK0jeHDhydj\n2jEgTqsuynFVKp2m+9xzz3msaRRmZjfccIPHzZI+FeVNgW6UrnTVpNNv9doSuyDmnaeVHmM6TdnM\nbNy4ca2OaZojVo1OG4/nMe3Ip9e7vDScUo/7yZMnZ47pFPX2IE5xV3vuuWcN1+T/xBQg/axd+PSe\nyCw9ZvVaEcc0LkonQ035jvT81EzpU3n0uI9plHptiPd2zf795KVHxpThUsR7T+0EOXr0aI/jNbdZ\n78FqQZ/n4jlO00m1Q1uj3JfEzo/63iN2OtPzcKmYiQMAAAAAAFAAvMQBAAAAAAAoAF7iAAAAAAAA\nFEBVauLEnDVtV7n33nt73OytKj+W15Yy1sgpAs37jG2NdftWuv30r371q8wxzYU0WzkPEfli3rAe\ns7He0MiRIz3WGgG67c2qe3zHfGPdJ+MxNX/+fI+L0j5Vxb9H891128Rc90aqX4DqyGqrG+vNlZOP\nH3+G5p5r3RD9dzPqkX2SmKuvNa7y7gcOPfRQj7WVbSWudXk10Nrb9nz00Uczx3r16lW7FSmRbv8B\nAwYkY/o5/l3jx4/3WI/hefPmJcs10nVE6yLG54yOHTt6HOsxtjex/qReJ6ZPn56MNXtNnLg/K60B\nV6pYY1FrSOkxpudoM1qOV4rWADNLr6d6XxLrQsVzY73os8vQoUOTsbx6bFmYiQMAAAAAAFAAvMQB\nAAAAAAAogKqkU82dOzdzrJbTUZcvX+7xBRdckIwNHDjQ42pPF+3UqVPmWGzt3Kh0mre2QYupNlnt\n5Mul2zBO21fbbLNN8rnS7eV0ymQztgpcunRp8lmnJmtrW7N0mmKcsqi05bWmFMa0Kz0nlJruFLeB\nTnGN6zRt2rQ2//xGktdKVadmVvtv06nCcX/RtMo4lbua8lIfmjFdV1uKm2W3FS+1pXieUltjxmni\neiw247myFLG9vX5fedexLl26eBzbjVY67UGPnZgWrefuIp4z20rvM+bMmZO5XGy/XSRx3XWb6z5Z\nTopJreSlG/Tp08fjWp53dN+55pprkjFNgaxlin/efpq3fzejxx57LHOsa9euq/zzNW1KW5bHVBlN\nS6XcQ/nid6epk1pWI5bzqNexGOm5abvttkvGyimvwkwcAAAAAACAAuAlDgAAAAAAQAFUJZ0qTs1V\ntZyaqylUsYvHhAkTPL766quTsWav1l6Oa6+9ttV/r3ZXKJ2emrdf6XT1atO0BbO0Y0JR0wdiBxL9\nHKf4PfDAAx7r9ObY4WrRokWtxjGdQKvGx4r+pcqbFrt48eKyfmY9aZpl7ByjKYya4hJTCHWad0w7\nevXVVz3W7g3PPfdcslzeMad0f6llOlVeV53evXvXbD1qJasblVm6L1TjPKRpUxMnTvQ4nh90HdtT\nlxi934jp25qSqqmqZum07zPOOMPjal9LYncfdfjhh1f1dzcaPT9qZ8aYWlTU63trslKTGrED18cW\nLlyYOaapiJUW0yO1U6res8TUc/2OK11qIE8z7aerSruTRpVIHdSfoSk78fyqHVTHjh27yr+3vdLn\nLbOVnzs+Fo/Ffv36eawlFuop3guUg5k4AAAAAAAABcBLHAAAAAAAgALgJQ4AAAAAAEABVKUmTmw/\nq9ZZZ51q/EqndSHy6gdorQdtTWiW5qVrTn+5eaaxzoRqaWkp62fW2pQpU1r99/79+1f19+p3F1tT\nV1rMt9Y6LkprT5g1f/5xzBvWz7G9sNJcZK37EHOFtZ1tufK2Qd75qFFNnz7dY63REG200Ua1WB0z\nS2tBxVo5lWjVWSqtwRLrOmiOsba6LjJtKx7rz1S6rXiptA5Ez549kzHN9z/xxBOTsaK3q451p7Se\nV951XmtGjRkzJhmrV7vTrGu6WVrboT2INcM+1sj1YVZVVqvpRv6bs+7JzFauVVgOvWe58MILPY6t\nw0ul9f/iMRXrEFZSXu2gzTffvGq/t1FoPcC8ts2VqImjRowY4XGs16fXzPgMQcvxlemxozVZ8+og\nqh49eiSf+/bt67HuH2b1e4aLdXvKwUwcAAAAAACAAuAlDgAAAAAAQAFUJZ0qb1pjqS1rS6WteM2y\n29tqOkcU24GOGzfOY21FFqdUxulaWfKm82233XYl/Yxai235dBqrbt9KT0eMatmKL25fbZ9er7SF\nItPUiTXXXDNzuUqkFOZNH65EulatZbVNzBPPR5q6uueeeyZjuj/rdoppUeuuu67Hmp4UU+JqmSaj\n6a6RHrPNkuZYz7biWXRfi2kBOg06pls2SmvPttDrgu5feeI9kLa712Oq1rQ9q6Z/xVanjZxSUw1Z\n6VSRTsMv2vklphBk/c21TI1tq5133jlzbO7cuSX9DE3HGD9+fOZYHk2N0utRvG/We8V4LtTyAJVO\np9EU3KjaZQkaQdYzV2xDX+ljWJ+H4nOCPleeeeaZyVgt28/X05IlS5LPv/rVrzyOz19Z7writerY\nY4/1eODAgR43auq2fgdxP1199dU9ziujoJiJAwAAAAAAUAC8xAEAAAAAACgAXuIAAAAAAAAUQFVq\n4uTVoMhrw1mOmKOutVs091NbhUex9Z/W1dH1ja1Uhw8f7rG2losmT56cOaa58o0kr3ZRzCstMq0R\nkFdroxKt5tuzmCuuKpG7qi25o1JrVzUSPSfF85Mef7VsTVntbZhH65ZpW/FYe+Scc86p6nrUQqxn\noHnTmjNt1hg11WLLbG1dHWtM6N9WlBbwej8Q60mMHj3aY61rEK+fep8ydOjQzJ9/+umne1yNYzur\nVsbhhx9e8d9VJFk1yLSOhZnZ1Vdf7bHWYjBr/HoMsQaO1lzQWh71rNn0Sbp165Y5pvu21oCLNS/z\nnkH0/KrbN15Xstp0x1pSWh9Mr1tmZsOGDfNY96ty6XUi1qzTv6tRnzkqKasmTt7+Uy6ty6o1lvK2\nqe4XZmajRo3yuBlawOt1X9uDx787j97n6rNZrHtbtOexvO/gwAMP9Dgew1mYiQMAAAAAAFAAvMQB\nAAAAAAAogKqkU8UphdoiVada56U45dFWZHHKkbYfK3WKYky3mDlzpsfaCi62QNO/K6Zz6NRandoX\np//r9KlGkteyWVsXF01sSX/EEUd4/M477yRjum2yWtejNJq2FpXTAnzGjBnJZ50q3gztchtlnbUt\nrU5Dj2k9lU4fiGkfMQXlY7E1ZyOnApQqr6V4bDvZp0+fkn6mXnfy0mE1PatTp06Zy+Xtn5qGEK+Z\nOi1az69FmRIdp7pfdtllHo8dO9bj+HdrWk5M59B0D41j63a9pyg3RTRrinZ7SLHI8+yzz3qs39GE\nCROS5fQ6c/nllydj+lm3T//+/ZPlND22luerOXPmZI41yvXmk+h6agqYWXqfHcssKL0/iCn0uq0q\nkc6ozyAx9VXPEX379vW4Lc8Een3O+5u13XUtU7DrJavdfNeuXcv6efo96znfLE2pzStDocd9LL9R\nhBSq2B5c06TiuVDLmqh436j3CvE8WcQyCK2J31ve/Z0+Z5JOBQAAAAAA0ER4iQMAAAAAAFAANUmn\n0qncOpU4TpnP61ahnVEGDBiQuZxOUSx3ippOcdXpkDrl0Sydvhgr/8fPHxs8eHDyuVGnkbe0tGSO\nvfrqq7VbkQrQFKp99903GdMpuDHN4IYbbqjuijU5nUaoqWox3anUKeWakpWX3qZTh80a9xgrgqyO\nVNXoiqRV+/O2r6bhFiUN4JPosRK7ken1c9myZcmYpldlXXPM0mneeVO+YweVLHlTgvPoNGtNHcrr\n7lgUep6J9yj6OaaCasqOHgOxi4V+1mvVkCFDkuU0DSvuL7p9dWp7UTqFVYteg/TcE89Dej7UTjRm\naWqnHovxuMzqgFTtdIK8Y7vcNJN60u6wZmmHNU3RjseHbtNq3xvoM0g8x2mKsD5LaGqfWbpvalqP\nWfo3671sTFFvhq6NbZHVnSqm4GWJ6bC67bJShczS7oVxexf9XiXep+t3lPedaPpZfIZuhvT31ugz\n50EHHZSM6bNQTJku5zrMTBwAAAAAAIAC4CUOAAAAAABAAfASBwAAAAAAoACqUhMn0jow2rI75qrm\ntdbTnFGtAxBzlquZ2x3XT3NXY3s/bQ+m+euxXkejysuR1laVsX1ao7Qv1Dx03Udie1fNHZ42bVoy\n1qz5mrWSlSfblnoqWgdi0KBBHse6Hlo/IObKo3xZNXHKbSmuOf26Pc1WblOpdJvG60Yz0PNmNerD\n6Hk6ngPVwoULW40jrTmwdOnSzOXy6nBoTZwTTzwxGStCy9VyxdoI+nnMmDEex/bW+lm3Ybz30LbJ\nPXv2zFyPffbZx2PqhpVGz3ux1bC2mNd6j9qK18zs7rvv9ljrSsQ6HHrfGFtha22JUu9TYi0mpbU8\niiLe6z/wwAMeN2L9kXjd0mcEvV8dNmxYstzAgQM9jse6/j+9l9Xvwqz93cvqM4qKzzV6nGqdt6ya\nOmYr19XRe5Nmri0WrxH63cXvRJ/RZ82a5XFercOi05qdRxxxhMdxX9LjVK/35WImDgAAAAAAQAHw\nEgcAAAAAAKAAapJOpSlEWVOCzdJ2WzHlQqcN6jTTOKW1lnSKYmyXrlNjdQpuUaYtx+mX2vZWp4HG\nqZ/akr3atI3b6NGjkzGdqq9i286pU6d63N6mnFZbVirOvHnzks869Tm2Y9VpmSpO/54yZYrHRTnG\nimDu3Lmt/ntbWtLquVGnlMfzv7be1TbiZiunXqFtNF2rUVJe9fydl5LVnuSl1elnTTONra71HKqp\nO1Fs94pVo9cdvZeNbWR12v0VV1zhcUyn0vNjTMPX9CptMx3TEvVeWVvbmqXT+pshfbERU6jy6LOL\n3pfGtOK8NGPdvloOoBm2Z1vEe029b9T7imOPPTZZLivdN7Zo1+eJeDy3V/pcG59/Na1Mz2txO2kK\nanw2a3TxuNR7Wz3X6jFqlh6nlbgXYyYOAAAAAABAAfASBwAAAAAAoAB4iQMAAAAAAFAANamJo7nC\nmjsXc+C01orGZmleo+bYNWr9i2Zrpab5+FktMqNRo0Yln8vJ/9Mcyph3qbUAYstppbmKsf10o+4/\nzSCrlXHMzc9rQ6z5pP379/eYGim1EesXfUzPx2ZpbvPEiROTsax2nTFXWM8lRcuPRttpDTLqkbVN\nXs0VvWZeeOGFydikSZM87t27d5XWDnm0XonW/op1wPQ+K7Yp15pIWbGZWceOHTPXI7YFRm3p96/3\nqNrqOoqt5s855xyP2/M5NK8luNbHifeanTt39njw4MEec3/ZNgceeGDyWeuy6fUp1rzs2bOnx1p3\nNf4/rbFT7Wc2rVmW98y5aNGizJ+hf0usE1vp45SZOAAAAAAAAAXASxwAAAAAAIACqEk6ldK2ZDNm\nzEjGdMpUnKqk6TxMta89nfqpU8x0m5mlKREx1Up/Rt40X03DyUuTUrHltLYEZH+pD50erlNVs9pW\nm63c2rG9tcpsNFlt4s8888ySf4ZuUz2PN1vKKdAI9B4rTuXWtsakEjc2TVGI6QpjxozxWFOtYtvb\nvCn/8Z4J9aPpVJr+Y2Z2yimneMz9UOtmzZqVOabPGvo9m6XpaZwPK0ef9TTVTZ/L4udYQiV+/lh8\nntPrXXx+yBLfPSxcuNDjvHOmapQ29MzEAQAAAAAAKABe4gAAAAAAABRAzdOpVKyOP2fOHI+1CrTZ\nytPgUD86tTdWhdfK+rFTQl4F+Sw6FfLwww9PxrRbESlTjU0rsvfq1at+K4Jcy5cvTz5ndRiL3an0\n2MzrMgCgfkgZaA7a6VPvjeN9snaDmTBhQjLWp0+fKq0d2kqPy9ilDJ9MOwqZpV1oBw4c6HF77uBV\nL7pvazq9WfoMF58XtcupPjvGDlfx86rSZ854jtR720a5r2UmDgAAAAAAQAHwEgcAAAAAAKAAeIkD\nAAAAAABQAHWtiRNpvmLMnUNj0vZuZmaTJk1qNTZLcxdj7Q2lrdtoqQjUzrJly5LP06ZN87ilpcXj\neNwDABqL1gqkbiCa1dVXX518pvZNMWhtr0GDBiVj+nnJkiUexzqNWkv3nXfeKen3xnq8nTt39rho\n50lm4gAAAAAAABQAL3EAAAAAAAAKoKHSqdDcijZNDWhv4jTkAw88sE5rAgAAkI/0qeamaVcam5n1\n6tWrxmvTWJiJAwAAAAAAUAC8xAEAAAAAACgAXuIAAAAAAAAUAC9xAAAAAAAACoCXOAAAAAAAAAXA\nSxwAAAAAAIAC6LBixYrSF+7Q4XUzW1S91UGGzVasWLFBJX4Q27Cu2I7FxzZsDmzH4mMbNge2Y/Gx\nDZsD27H42IbNoaTt2KaXOAAAAAAAAKgP0qkAAAAAAAAKgJc4AAAAAAAABcBLHAAAAAAAgALgJQ4A\nAAAAAEAB8BIHAAAAAACgAHiJAwAAAAAAUAC8xAEAAAAAACgAXuIAAAAAAAAUAC9xAAAAAAAACoCX\nOAAAAAAAAAXASxwAAAAAAIAC4CUOAAAAAABAAfASBwAAAAAAoAB4iQMAAAAAAFAAvMQBAAAAAAAo\nAF7iAAAAAAAAFAAvcQAAAAAAAAqAlzgAAAAAAAAFwEscAAAAAACAAuAlDgAAAAAAQAHwEgcAAAAA\nAKAAeIkDAAAAAABQAP/RloXXX3/9FS0tLVVaFWRZuHChvfHGGx0q8bPYhvXzxBNPvLFixYoNKvGz\n2I71wbHYHDgWi49jsTlwLBYfx2Jz4FgsPo7F5lDqsdimlzgtLS02e/bs8tcKZenevXvFfhbbsH46\ndOiwqFI/i+1YHxyLzaE9H4srVqzwuEOHitzr1QXHYnNoz8dis+BYbA7t7Vhslmuh4lhsXdG2danH\nYpte4hRBORtK/09b/h8AIPU///M/yef/9//I2m0kXN8QcQ8EoFnE81mWf//73x5/6lOf8pjzX/Np\n1m3K3TUAAAAAAEAB8BIHAAAAAACgAHiJAwAAAAAAUACFr4kTcx81x1HjSPPjNBcyjmXFAICVxRo4\neo7Wejmxdo6eh6mjs2ridbGAiO4QAAAgAElEQVQSRf24/hVfqceiivdHHJsAGkm83un5TJ8D43IL\nFy70uFOnTh6vvfbayXLxHAg0Cq7GAAAAAAAABcBLHAAAAAAAgAIoTDqVToNbvny5x7NmzUqWu+qq\nqzx+8cUXPY7T4z772c96vPvuuydj3bp187ilpcXjTTbZJFlu9dVXL2XVUSUffvihx++9914y9ulP\nf9rjtdZay2OmggOr7qOPPko+//d//7fHccqyHnPvv/++x3oeNzP73Oc+1+r/MUvPtUxtbp1+77o9\nzNJzpU41jylSa665psd8z81Ht7ceYzH1fNmyZR7HfUnvpfQ62x6urXrstIe/F20Tr315KaiVSHFF\n6/R89sEHH3h87733JsvddNNNHi9dutTjoUOHJst9+ctf9pjrYn3F69Hbb7/tsV6PPvOZzyTL6fm6\n1HN3PC5LPU5reTxzFQIAAAAAACgAXuIAAAAAAAAUAC9xAAAAAAAACqBha+LEGid//vOfPR45cqTH\nc+bMSZbTfDnNe9NcObO0TorWzjEzu/322z3+j//4v69ojz32SJbr16+fx1tvvXUyRr50ZWjdjF69\neiVjs2fP9ji2SNXtfdJJJ3k8fvz4ZLk11lijAmsJNIeY079o0SKPf/CDH3g8d+7cZLnVVlvN4y23\n3DIZ69mzp8frrbeexxtvvHGy3DrrrOOxtvs0S2uYde7c2WOt4dLe5eVhv/LKKx7fdtttHuv3amZ2\n7LHHeqw1itB8surjmJn9/e9/9zjeH22wwQYea/3AZqkRqOfAf/3rX8nYm2++6XE892gNBr2v0HtI\nFIfuB1pXxczspZde8ljvQ2OdN32OiddWvUft0aOHx9tuu22ynB5X1M5pXfxe9JjTbfLCCy8kyz3+\n+OMeL1myxGM9/5mZ3XzzzR7H7YPq+NOf/uTx1772NY/jMaY1irReW7x/0fN1rKujtMZOrKuz7777\nenzyyScnY3qfq/uj3htXA28aAAAAAAAACoCXOAAAAAAAAAVQ13mecXqhtngbNWpUMjZlypRWl4ut\nw3fccUePd955Z4833HDDZDlNf9Lp+WbpdKrp06d7rFPqzMwmTpzo8f7775+M/frXv/a4Y8eOhmwx\nFequu+7y+Fvf+pbH77zzTsk/U6fL3XPPPR6/9tpryXKa0kEK3KqJ21HbPOpYnF6u3zvThWtPp3xr\nOo2Z2X333edxVqqqWXqOe/nll5MxbW+t5+HYqlOnNsfp6xtttJHHXbt29XiHHXZIlovpQe1V3D63\n3HKLxyNGjPA4Hm+vv/66x8OGDcv9mSi2jz76yONnnnkmGfvNb37j8VtvvZWM7bXXXh5r6qSmSpoV\na3/Re1Gdrq/HjZnZueee63E8z2kK1U477eTx2WefnSy33377eUy74sby/vvve3zppZd6fP311yfL\nLVu2rNVY0+3M0mNM74fM0nQqTf3QlA0zsyOPPNJjTSsxS8tCFOl4qzb9LjSNRreVmdkbb7zhsd5z\n/O1vf0uWGzdunMcXXXRRMsY9R/n0vBvbuo8ePbrV5SLd1gcccIDHX/rSl5LldPvG87pub02h1XcN\nZma/+93vPI7nBL23vfLKKz3ee++9k+UqXQKAox4AAAAAAKAAeIkDAAAAAABQAHVNp4rpMQMGDPD4\nqaeeSsa22247j7VL0W677ZYst8suu3is05ZiqoeK1aN1inn37t1bXQczs8GDB3t80003JWNTp071\n+Omnn/ZY0wLaM+14EaebaZV43W5xuqhupzjdTpfVfSR+/0xBbTv9rrVLkU77NUs74mgKVewUp2mP\n/fv3T8Y0TXHdddf1mO3WNvH4uPHGGz0+7bTTPI7dWLQ6vx6nhx9+eLKcdmzYdNNNkzHtrqHTy2OX\nAU3HiOdT3c/uv/9+j88666xkue23397j9twVJnZf0NSAOKVcaRrwqaeemoxp9zBUhx6n1Ugt1dRG\nPXZ0+rdZemxusskmydjuu+/ucVGPsXg+1FQXjWfMmJEst2DBgsyfqd/tI4884nGfPn2S5TSV/8EH\nH/T4C1/4QrIcqcXVoc8dQ4YMScbuvPNOj999912PY0kETenV8gvxvkRTOOJzhp6j9XjTEg5maZrx\nb3/722RMU066dOnicbzHas/0HPXFL34xGdMUSD0n6LFslqbRxO9W06uapUNfrWi6kpbRyBO/f72X\nPeSQQzyOx6Ju30GDBiVj//znPz1+8sknPT7//POT5bRjVl7HuoMPPtjjn/3sZ8lyer9diVQ8noQA\nAAAAAAAKgJc4AAAAAAAABcBLHAAAAAAAgAKoeUKz1kQYOHBgMqb5wQcddFAyds4553i8/vrre6w5\njWal52hrflxePRXNWTv++OOT5bT+QqwRoXVdNH9W/70t69sMtF3xEUcc4XGsw6G5w9ry+Mc//nGy\nnH53sTbGo48+6rG2KW9P33e1aO7qUUcd5XHMI1aaKxxzWrX+1Xe/+91kTI/v3r17e3zJJZcky8Wc\ndaTn2m9/+9vJ2LRp01r9P1pTzMzsiiuu8FjPd+W2xs07726wwQYeb7HFFsmY5j0/9NBDHt9zzz3J\nclo3KdbyaE+0joKZ2auvvlrS/9N6EVofwszshBNOWPUVQ65K10GJLY+17qDW3tDWylGsoaQ15rQ1\ncpHrlOl9gdYBiy1m9W/U85VZek+j59fFixcny2k722222cbjz3/+88lyek954YUXJmPUPMmndY1i\ni/df/epXHsdamXr90GeQ4cOHJ8tpbRXdd/KO31inbOHChR4/9thjHsc6TM8++6zHzz33XDKm98ej\nRo3y+Otf/3qyXJGPzVWl2yTeo2r9Pm1P/fDDDyfL6fPEhAkTkrFTTjnF45122mnVVrbJxXs+vX5o\n3T6z9Hn79ddf9/irX/1qspw+F+Tdl+p+EI8HPffuu+++Hu+zzz7Jclo750c/+lEydu2113qs997x\n/KPvFGLdwXKu/+33yAYAAAAAACgQXuIAAAAAAAAUQM1zS3QK0uTJk5MxbWGq6VNm6bQ3nXJU7vTj\nUn+GjsVUHG21OW/evGRsq6228linNMe/S6dANpvbbrst+dyvX79Wl9PpcGZmkyZN8jgvTUan5g0d\nOjQZO+OMM0r6GfhkzzzzTPJZ09NUbN9+3nnnefzNb37T43gcvfXWWx6PGDEiGdMpirpfaGqeWTod\nOba4bi/ee++95LMeVy+++GIypq3btc3qdtttlyy35pprelyJVI+8866m3HXu3DkZ09QCnfqqqQlm\n6XlYp8abVaadYyPTlrjf+c53MpfbcsstPV577bWTsX/84x8ex9aYRx55pMe6X6C+dKq5mdnJJ5/s\nsbZLNUvTR/Q8uWjRomQ5TaGKrVQ33nhjj4uappF3LtO/KV6r9HjRdrNm6T2fpvPElE9NG5g1a5bH\nf//735Plxo0b5/F1112XjGlquW7vmJJV1O1TDr0f7NWrl8f6HUeaAmdmdsEFF3is+3kl0vBj++kd\nd9zRY20PrvdKZunzw9SpU5OxK6+80uNhw4Z5HFtpd+vWzeP21rpe09huuOGGzOW09bPGZmnr6jlz\n5iRjv/nNbzweP3582evZHsR9T9OfvvSlLyVjP/3pT1uN48/QlNdq3q+apc+SWmrALE3H0/IFMY1S\nn3FiGRY955Sq/ZzhAQAAAAAACoyXOAAAAAAAAAXASxwAAAAAAIACqElNnAULFnis7dlie7/Bgwd7\nHNvDNnpub6wtcO+993q81157eaz5k2ZmI0eO9LgZ2l9fddVVHmuOrlla5+KWW27xWNtsmpWey6i5\nhrGF7g477OBxzBPHJ3vttdc8Puyww5Kx9dZbz+NrrrnGY93PzUo/ZnW/uOyyy5IxzZPV9uO6fmZm\nPXr08Pj5559Pxpq5ForWw9h1112TMW2HePTRRydjmrutLXUbRTwXbrbZZh5r7nGsiTNz5kyPtSW6\nWXPvB2ZpnYxY40Tbct50000ex5acp59+eqvLmWW3UEZtaM0PbU+8xx57JMtpDY1OnTolYzfeeKPH\nWv8qtkHV+5Tly5dnrlMz1tfQ7/nxxx9PxrTmQqwBp/S46tOnTzKmtcr0HkbvnczSc/Qrr7ySjJ17\n7rkea70IvTabpfeX8Rqg5/0i3nvGdsWjR4/2WOvgxL9t7NixHn/ve99Lxur1nKEt4+P6rrXWWh5r\nrQ2z9D7o97//vcexnpnWFtS2zu3Byy+/7LHWTjQz22KLLTxef/31PdbtYWZ2//33e7zbbrslY1ob\nS59pG/2ZtdHE76tv374ez5gxw+NYO0zHtG5frEFVjniOyWtTftRRR3n87LPPenz++ecny73zzjse\nx3b1er4uFXsZAAAAAABAAfASBwAAAAAAoACqMocyTkHS6aQfffSRxwcddFCynE7ljtO8i0bTebbd\ndluPn3766WQ5bYm70047VX/FKuzOO+9MPp955pkex6nWmkKlLRVL9eGHHyafL7/8co+feuqpZEyn\nDuuUSZSmf//+Hv/rX/9Kxu644w6Pu3fvXtHfG/cZbWeu2//UU09NltN0uh/96EfJmO4njTr9P2/a\nZqSpMrvvvrvHb7/9drKcnndi68tGTKFS8fvQqauafqepVWZmv/vd7zyOU6dj2/JmoN/T9OnTPY77\nz09+8hOP89JLtcXuH/7wh2Tsvvvu81jTTHQfxKrR1tQ6hd/MbNKkSR7rFPKY7qQt5B966KFkbMMN\nN2z195500knJZ21dHFuMa3pHM9LrnaakmqWp3+Xeo+q5TKf8f//730+WGzBggMdxG2sas6ZRxrTy\nH/zgBx7/8Ic/TMY0befiiy/2+MQTT0yWa9R7cU0lNjMbM2aMx+uuu67H+l2ZmR144IEeN+L9QFwn\n/RyPPS1Boceppk+ZpalmMWWj2dN+NN0ptntebbXVPI4pVErLZegxZZamHevzXdeuXdu+snDazvui\niy7yWNNHzdLzmp67Y6pkOdpyftDj6D//8z89fuGFF5Llbr31Vo81BdIsvU8r+fe2+X8AAAAAAACg\n5niJAwAAAAAAUABVSaeK0/q1k4JOH/3tb3+brkwVK+TrNGWzdBp6NX6vTkEdMmSIx5oeYpZ247n0\n0ksrvh7V8OKLL3qsHYPM0urscdrm1ltvvUq/98EHH0w+Dx8+3GNNHTEz+8pXvuLxGmussUq/tz2I\nU+b/+Mc/ehynoMZUl1r5zne+4/E//vGPZEz3Ne3AYpZWhy9ip7K33nor+aypmu+//77Hcaq1Hi+N\nnj7VFjptVafcmqXpujG14N13363uitWBpnto14O4n/fs2bOkn6fpNjHd+fbbb/d46NChHt9zzz3J\ncs0+Pb/SdL/ceeedPV68eHHm/zn00EM91hRms7RblaYL5Ild3tQXv/jF5LN2cWlGmlKo5xMzsxEj\nRnhc6ndbLp3Kr12xzNLUK02FmzJlSrKcdrzStBKz9NpxyimnZK5Hv379SlvhGtB7j5jKr/fxei+t\n6VNm1U2hasu9UTnrEf+PXtfPOussj7V0gVnabS6mm2vqWTPStOp4bSq1hIV+79ql0czstttu83jQ\noEEex+tiETvANQrtBBifoa+77jqPdd+OKcJ56XKVkJX2qKmqZul+8MADDyRjzzzzTJt/L3dbAAAA\nAAAABcBLHAAAAAAAgALgJQ4AAAAAAEABVCVJ7957700+az0azfleb731qvHrW1XPPH3NyY25zdpG\ntC1thmtN2ztrrYRY/+jII4/0OLaBLmcb6Hfyy1/+MhnTehCbb775Kv+u9iy2EdeWprEmzte+9jWP\ntaVz7969k+U09zNvX87LI8/6f5r/bWZ2xRVXePzaa68lY9pm97DDDsv8XfUU/049Zx511FHJWNxW\nH+vTp0/yeZ111qnQ2tVeXptV/W5ibaRZs2Z5vPHGGydj9arlVE3HH3+8x/odde/ePVmunLpg8Xyr\ndbLmz5/v8ezZs5PltCYL/pfWilu0aFEypvXbdH+O17Azzjij1Tju56XS4yi2JNaafmPHjs0ca0Z6\nTYu1FLfffnuPG+X+TO8pjzvuuGTsmGOO8fi5555LxvQ41WuKtuo2S2tL1Pu+Smv3xfa8ev9x8MEH\ne1zL7RR/l15zqn1/r3U4Yq0XrR80c+bMZCzeNzQD/a61Nl48d8WanqWINcJ23XVXjydNmuSx1tYy\nW7k1djn072qU80+t7bbbbsnnLbbYwuPnn3/e4xkzZiTL7b///h5X+7vT82S8D9f94Mknn0zG8mrT\nZf6uNv8PAAAAAAAA1BwvcQAAAAAAAAqgKulUOqXMLJ26NHDgQI+rPTVT20NqS1SztCVxTHGqNP35\ncSqetoVtZHPnzvX4pZde8vizn/1sstz48eM9rkRLvQULFnisqWdRnFK+bNmyVpeL66SpQvH/6BRj\nnRIXWzk3w7TGmG6x6aabevzCCy8kY7rPaqvb2HJVW9EecMABHnft2jVZTqej9ujRIxnLagsYzx2a\nkhCnWWsaiK5vI223ONVa98VHH3008//p33DyySeX/PP1s6Ylvvfee8lyn/vc5zzW6akxjVJbQOq2\nrgZd92uuuSYZ06nTmvLTLOK1VduY6rlY0wvNyrvWdurUKfl8zjnneDx48GCPjz766GQ5bZMZrw/N\nRvdFTfV49tlnk+X0uhineeuxpN95THHSNNZKeOONNzyOKV5bbbWVx4ccckhFf28j0nvFpUuXehyP\nm3jtb3S6/l26dEnGTjjhBI9/+9vferz22mtXf8XKlHd+/+EPf+hxtVP+9LjXVMlx48Yly/Xt29fj\nctMeS6X3AjFFqr2lU2ka5PTp0z2O96gxNacU8ZzQr18/j//whz+0GpulKbClXhebMQV8VcXz0913\n3+3xdttt57GmVJqlzwVf//rXk7Fqni/ic8Zmm23mcby2lnO/xEwcAAAAAACAAuAlDgAAAAAAQAFU\nJZ1KpxWbpakaWiG6GnT6mXZVuOiii5LlbrrpJo+nTJmSjGl6R6XFdAXtAqTTMs3q2wUiTuPTqaua\n4hK7Qn3+85+v6HpopzOd8myWTlO75JJLkrElS5Z4vO+++3qsaTdmaZrYiBEjkjHt5qB/l6YLmKVp\nQ0UVUwqnTZvmsaZOmJk98sgjHuvU89jFSqc+X3311Zm/e7/99vM4drYrlaZ/xZQ5PcYaVV5qV5wC\nrKlWOrVXu6qYpdNO47GjXXA01fSVV15JltNpyXp+iueHb3zjGx5XO51K07/i/vL+++97rPtmkem2\n05Qms3Q7DBs2zOOYClVOOtXqq6+efP7Od77j8eWXX+7xX//612Q57RKj54pmEPd77YSn6YwPPvhg\nstxnPvMZj/V8Z2Y2atQoj1taWjyudrq5plDFVOKf//znNVuPRvDyyy97rH+vppWZNVd6oF6v9W+O\n+2cjbX8938fzux5jlaDHeuwIqefh6667zuPXX389WU5T0eM9ajXTuWPqlj6DxfN6o9H7DP2O2vJ9\n6f3Nu+++63HHjh2T5cpJj4zHg56zhwwZ4nG8H9OURU39M8suPdFIKf/1pPtE7D6rqYJ6vxqfR378\n4x97XO0U4bwuYvo57o/lpM81ztkZAAAAAAAAmXiJAwAAAAAAUAC8xAEAAAAAACiAqtTEie2K1113\nXY9jDlilPfbYYx5rfmLMS/v73//ucc+ePZMx/X+a+1pujRrNp42tebWFb63zH/Py7z788MPk85tv\nvunxzjvv7PFZZ52VLFfpOj6aK6q1FszS71JbgJulrdy7deuWuX7aLltr4JilOZVa3yXWxOnVq1fm\n+hdF3Pe23HJLj2+++eZkTPcbraGidQXMzN566y2PTzrppFb/j5nZQQcdlLkepdpnn308vuyyy5Kx\nap9zqkHrTsUaXXqO0/x2PaeZmf30pz/1WNsJm5ntuOOOHm+zzTYea70Zs7TOgB6LixcvTpbT2hGx\ntlelaypcf/31HscaY7pvbr/99lVdj2qJ5+U77rjDYz0Pm5ntsMMOHp9++ukeV+Nv1Xogt9xyi8dd\nu3ZNlpszZ47Huq+ame2xxx4VX69aitdFrfWlrcNjHa711lvP44svvjgZ22CDDSq4hvl037r//vs9\njvuL1rhqD+666y6P9bofa+hl1a4oAq0XYZZuf90vBg4cWLN1+iTxXKjXHa1/Zpaed/R+IN5T6PUp\nHqd6DR00aJDHDz30ULJc/N0fi/eXV155pcennXZaMqbtkCstPmdoXbV4r9zIyr0f1Jpkug994Qtf\nSJarxHVS6+ocd9xxHs+aNStZTmvkxPpNWk+tyOeYTxJrM8Z7RaXXWr226ndllt7b6v6irbxb+3/V\npPtcPIfpeTju33rfX6pi3NUCAAAAAAC0c7zEAQAAAAAAKICqzNvSqcNm6ZQpnSIV2xqXI05r1HbS\nOo1pwoQJyXLaUvMnP/lJMnbeeed5fNttt3l8xRVXJMvtvvvuHudN2bzhhhs8jtP/Ne2n1tP94zQv\nnVr65z//ORlbuHChx0cddZTHMRWtEuuh3+V3v/tdj7XFrVn6HZf63cUpxdqaOm9qnypiek5b5U1j\n1bGNNtrI45gWoKlqW2yxhcexReBee+1V9np+TNO/4nlF16soLRt1WuVFF12UjD3xxBMea4pLnMqt\n6W1x+m7fvn1b/Rnx+NBj4tJLL/U4njP1vFuN89gHH3zg8ciRI1tdP7P0ezvzzDOTsUq3oG2rUttH\nxrRETW+IraA1xbSW07C33XZbj48++uhkTFOtNMXLLJ3m3uitbj+m2y22LdX7Dz23xGNAP8eUuFqm\nU2lq98SJEz1ee+21k+U0Bb49ePbZZz3We9Q//OEPyXKaXl+ktBSzlc8/+neuttpqHsf790aix1G8\nVun9+fe//32PY3mHBQsWeHzGGWckY3/605881tbU8XjeZJNNPNZnhOeffz5Z7thjj/VY08bN0vbj\nlbhmaqqKpgeapdt+//33T8Ya7Z6oEt+FpvLrz6vGvq3fn16fdT8zM5s/f77H48ePT8b0GlrPEhtt\noftb3r2HPlfGtut/+ctfMv+f/sxp06Z5rM8VZun966233urxV7/61WS5clKVyqXbLd4z6L750ksv\nJWNf+9rX2vy7mIkDAAAAAABQALzEAQAAAAAAKABe4gAAAAAAABRAVZLoYy7a5MmTPX700Uc9ji2j\nS6X1Efbbb7/MsS5dungc66lonmTM6dc6L9p2WvNbzcx+8YtfePylL30pGVuyZInHP/vZzzyONRy+\n/OUvW73EnGKtVXLBBRckY/PmzfP4gAMOqOh6lJr3GZcrp515zLfVXPi8mjhaM0DbM+P/xJxTbf33\n1FNPeRzrLWjL+nLde++9Hsc6Wbvssssq//xa0/00fj877bRTq8vF/VeP71h/JOuYi7nNmkv/17/+\n1eO4rbXNZiXEv+W6667zeOnSpR7H4/nII4/0eOONN07GapkTXQr9brU+Razf9sorr3gcW3Ted999\nHh9//PEe67XPLP0+db+I51Dd/nnfl667tlU1S+sxzJ07NxkbNWqUx0OGDPE41q1o1FoAcT1PPPFE\nj5955hmP77nnnmQ5rXMU7xVOPfVUj3/0ox95HI+pcq53sQ6K3s/84x//8PjAAw9MlmvU779a9Ni5\n/PLLPdaW02bpcaW1Ds3MunXr5rFe4xrlu9Tzplm6b+jfVevajG1xyCGHeKznErN0W915550ex5p7\nY8aM8VjbrJul20rr+N1xxx3Jcvp96f+J94ba0lrP42ZmN910k8fHHHOMrarHH3/c49tvvz0Z03pw\n8brYjPTv1XpPsUZbJWgdnNGjR3usLe/N0mt8vLaWc26vtfi8qG3ste6NWXr+0zpT8blS69vE+/Rt\nttnGY33mPPvss5PlvvnNb3rciN+jvpMwS2ta6j21WXn1Vhv3bA0AAAAAAADHSxwAAAAAAIACqEo6\nVZ8+fZLPOj3zpz/9qcf77LNPspxOe4t0StKkSZM8fvrpp5PltD3bI4880uo6RJ07d04+axvUHj16\neKzTj83MfvjDH3qsU7/MzBYvXuyxplbF6V6nnXZa5npVW1wXXc/Y5vb111/3eMaMGR5r63GztPVs\n/PmNMK04tnvTqXlx6rlOndf9rNHSMhqVTt/W7z2m9uQd91k0vc/M7Mc//rHHcTvWM2WxEkptHR2P\nt0rvp9q6/Ytf/GIyplPPK+Fvf/tb8nnEiBEe6/cRWzJeddVVHjf6car7qaZJ/fOf/0yW0/Nm3Lc1\nTVHTk+O5Vqcta2pVbLu+0UYbeRxb0X7lK1/xWK+nmupmlk6tjtOsdbq5/q6TTz45WS5vn691uod+\nl3G9WlpaPL722ms9vvDCC5PlfvnLX3qs09DN0nTr8847r9WfbWY2dOhQj/v169fq+kUxLfHhhx/2\nWFtkDx8+PFmuEa7VtbTnnnt6rOUA9PsyS9vDamqPWXoMr7/++h7HVJyuXbt6XO3p/5oCEdO/9Ni8\n7LLLqroelbLddtt5HNMSZ86c6fHFF1/scTxfaJvpmMKhx5wei6Ve3+L21PvLQYMGJWOaOqnPTG1J\nqdBzyYABAzyOKRyHHnqox/F+K15Tiij+DfqstsMOO3gcn01L9d5773k8ceLEZExThF599VWP9X7J\nLE231P3TLL0ON+q5N66X7mPXX399Mqb3+FpCJR6Leq791re+lYzps+Qmm2zisbYUr6e840afd+K9\nrF4P9Fpgll43SsVMHAAAAAAAgALgJQ4AAAAAAEABVCWdSqcrmqXTpv/yl794rFWrzdL0qjhVSVOc\nxo0b53Gc4tW/f3+PdbpwW+jULa0gr6lgZmYvvfSSx08++WQyptPv1K677pp81ql+tRantul2ip0x\ndFquThmcPn16stzWW2/tcaNUCtd9SdMyzNLK8rHziHby2HTTTauzcgUQj8VSp3tqNwad1l/udF79\nefE40qmdvXr1SsbitFaUTrebnp9jKuyGG264yr/rzTff9HjgwIHJmG5fTZ2LaSCNcs5pTTxu9Pyr\n++gPfvCDZLkXX3zRY/lS988AAAwXSURBVJ2aHH+mTsmOKTvaGSPvO9KU2phioZ/zuljldfnT9fiv\n//ovj2PnR/1bGnmb6rppB8ORI0cmyx1++OEeX3nllcmY3ttoevKCBQuS5fTe5uabb/Y4Ts3v1KmT\nx5r6bGZ2/vnne6zfcSWO3yLTY1E7vsWOQppWfs011yRjV1xxhcd6HGm3U7P0Pkv3i5huo12Eyk2x\n0Gvrbbfdloxp2o52PGwk8e/We7SY1qIdnrQzZjyOtAxCvKfX/aDUNOa89T3ooIM8jp15NDXv5z//\nucex65auh/4fszRV/Pnnn/c4pmTpvlVO+nqj0/t4s/Taos+jeam4+owT04POPPNMj+M2UPr8E9MX\ntXNZvGZmHd/l3ntXQ/zu/vWvf3kcU6r1PKnP4ZqSbZY+j8UU/bzfXS+6PeJ9jnbF1U55N954Y7Kc\njh188MHJWCw1UYrG+GYAAAAAAACQi5c4AAAAAAAABcBLHAAAAAAAgAKoSk2cmL/29a9/3WOtZ9O3\nb99kOc33X3fddZMxzS/UGgGx/s43vvENj8vNH9T/p20M77zzzmS52bNnexxbpGqO5pZbbunxAw88\nkPm76k2/ywMOOCAZ0xbb2j5N27ibpXn7tcy91XxWM7NFixZ5rPnGsd2ntpfX2gRm1MH5WKn7aNwG\nkydPbvVnxHpR+r1ry0Ezs3fffdfjPfbYw+PYKl7PF7fccksy1kjHWKOLOdhaW2Xx4sUe77bbbsly\npdYP0Joo8XjTeh0vvPBCMqb1Jg477LCSflej0/1Sa6toXr2Z2e9//3uP436vufX6M+KxqDWF9P/E\nY0Nz2c8666xk7IknnvBY67/06NEjWe61117z+O67707GtA5LzI9XRT9m4z2QHi/x2FF6vhs/fnwy\npu3Z77//fo/32muvZLnPfe5zHusxa5Yef1r3rto1BxqptsMn0e9C69fEz2PHjk3GtFX8L37xC4/j\n9UjrHmk9o0suuSRZTuvUDBkyJBnT+zM9FuN21O891vfRdrbl1GKoB91vtGaQWVoj5ze/+Y3HcRvq\nfW6sTVJpm222mcdTp05NxrSlstbN0P3DzGz33Xf3eMyYMcmY1v7R83+sf6TtrZuRthQ3M3v55Zc9\n1mvfr3/962Q5reuiNa7iz1N6vJmZnXbaaR6fe+65Hleillsjnyf1/BGf17U+k95Dbr755slyn//8\n5z2O565G/Nv1fKrXajOz+fPnezxlyhSPtT6TWfreI9aWLOc6zEwcAAAAAACAAuAlDgAAAAAAQAFU\nJZ0q0nQWbVOqUwHN0nbbcYr2O++843FLS4vHsZWnplxUYjqW/ozPfvazyZi299OptGbpVKvvfve7\nHld7+uaq0L9vv/32S8Y0leyZZ57xWNOszNIpZbvssksyptMLy9k2cUq2tjrXtsNmaZtQbf0W08S0\nNV5sMd6elbN9YgqHftZ2fDqF1czsyCOP9DimBugUcE3DiucHnaqs6QRom7jd//nPf3qs3/HSpUuT\n5TRlNH7/c+fO9fiEE07wWFN3zMw22GADj2MKQl4KSrOJU2o11aHUtIeY3lbq/9tkk008ji184/k3\nix7rOq3dLD2eNV01rl8jTqWuBZ2qH9sfn3HGGR5remE8VvQaHNPvlLYV1zS3amgP21PvnzStQtvo\nmqXlADSlRtMVzdLz5rHHHpuM6X3kiSee6LG2QjZL95OYpqP3eHrMViINpBbiPqXXoHPOOcdjfXZo\n7f9Vk/6u7bffPhk77rjjPD777LM91lSM1j4rPW41JUufTZqVXo/ic5Wm8OhzQnxeXL58eas/L6YH\n3XTTTR7vu+++yVijtL+uNU1L1FR4M7P999/fY01n++Mf/5gsN2zYMI/jeVJbk9frO45txLW9fGwd\nruUBFixY4PFJJ52ULHfUUUd5rCnN5Wqfex8AAAAAAEDB8BIHAAAAAACgAHiJAwAAAAAAUAA1qYmj\ntUa0Nebee++dLKftL2Pr6q222srjq666ymOtgWNW23xe/V1HH310MqY1CYqYM6ltDc3S1u1aD+PN\nN99MljvvvPM8Hj58eDKmtYw0lzfmHWoeseZTxhxE/V2a92qWtq679tprPT7kkEOS5Yq4bRpVzEvW\nWliPP/64x0uWLEmW0+P+L3/5SzJ28MEHe6zH/c4775wsV5Q8/kb30UcfJZ8nTJjgsdaZmj59erKc\nHovxeNbPmnf+hS98IVlOWzFqfRw0hlJrSeixqDV2zNI6LFn/J/6u9lBPpRRac0Vrn5x66qnJctOm\nTfM4tqZWWuch1lBC5cT9d4sttvBY73Nj/aI5c+Z4rNdBs7SepJ6jr7zyymS5rHOvWXpvtWzZMo9j\n7cciWnPNNVuNzdLvIX4n1TzXxJ+9zTbbeKy1MWINH71X1lojZum2b/ZrZtxWum/HluBa00afUeK9\nidYg++pXv+pxrDPWyLVMG0GsUdm/f3+PL7nkEo/j/eWll17qsV63zMwGDhzo8fe+9z2PS63v1xZ6\nLtTzbqzTM2/ePI/j/qjPnP369fNY656ZpftcJc43PMECAAAAAAAUAC9xAAAAAAAACqDmc2h12tUL\nL7yQjPXu3dvj2JpUp3h27tzZ43qmw+hUqNVWW61u61ENccqaTmfTqZ+aYmGWts+MLVJ1ar3+jJg6\np60wtcVxnG6sU/C/8pWvJGM33HCDx80+zbRR5E0X/tvf/uZxbDGubR5je2rdD0mrqD5toWiWplzk\ntSvWbRjpcartOe+6665kOaYsN594zGrajk5HjtfxUtuZY+V7D00Zjinrmk7QsWNHj9tybtVtwzm5\ncuL5T0sFLF68OBk75phjPNa2vZoWFcV7Ok2RzzouzZpvG1fi79HjqNzUT23xPm7cOI87deqULKdp\nPlqaor3JuyZsvfXWyecBAwa0utyuu+6afNa0fE1bo8xC28R03LPPPttjbRX+61//Olnuvffe81if\n+8zSVODLLrvM4+OPPz5ZTt8b6H4Qj5X333/f44svvjgZ0xbpefeyeo6OpVx+8YtfeLzLLrt4HK/P\nld632FMBAAAAAAAKgJc4AAAAAAAABVDXlgQbb7xx8vmZZ57xeMGCBcmYTuXX9Jhmn/rZKLSryCmn\nnOKxpkeYmc2fP9/jp59+Ohl7+OGHPX7uuec8jikcsYL8x2LXBO3E8M1vfjMZYzpkY9GUmpgyFT+j\nfuIxpl3qNMU1dhnQ825MX7znnns8jl3F0L6UmnrAdbx8+t3Fc2tWGki5Px+1EVMDbr31Vo91+v9L\nL72ULPf22297HLfbpptu6rFen9m+n6yc+8v4veq1NnawwcryUm61G5VZ+oyi+3b8Gezr1bH++ut7\nrGU1Bg0alCw3efJkjx988MFkbObMmR5rxzZNrTJLnwP1ehfTErUrrj5/mmWXCoid7bQT5MiRI5Mx\nTZuqZXdNnnQBAAAAAAAKgJc4AAAAAAAABcBLHAAAAAAAgAKoa02cSHPHttxyyzquCfKsvfbaHnft\n2jUZ69Kli8faQtHM7Oijj/Z46tSpHs+bNy9ZTvO499xzT49PO+20ZDltXQdg1WmbTbM0Z1lrL3z4\n4YfJcmuttZbHsd0kgMbAsdl8tF7OVlttlYzltWWmVTyKTPdZrXvT2mfUj7bl7tixYzLWv39/j7WO\nkZnZv//9b4+XLVvmcayZqvVy9Hdp+3KztCbOG2+8kYy9/vrrHre0tHjcq1evZLmsujf1xEwcAAAA\nAACAAuAlDgAAAAAAQAEwtxarJLbs03QMnXpmZrbeeut5vNNOO5X0MxtlyhrQHunxp+0WY+tFAI2n\nnFbIaB5590/cWwGopXjOyUvp1NRfLeERxRStLFtvvXVJyxUNV3gAAAAAAIAC4CUOAAAAAABAAfAS\nBwAAAAAAoACoiYOqyWtnSntLAAAAAGhfePZbdczEAQAAAAAAKABe4gAAAAAAABRAB01r+cSFO3R4\n3cwWVW91kGGzFStWbFCJH8Q2rCu2Y/GxDZsD27H42IbNge1YfGzD5sB2LD62YXMoaTu26SUOAAAA\nAAAA6oN0KgAAAAAAgALgJQ4AAAAAAEAB8BIHAAAAAACgAHiJAwAAAAAAUAC8xAEAAAAAACgAXuIA\nAAAAAAAUAC9xAAAAAAAACoCXOAAAAAAAAAXASxwAAAAAAIAC+P+hAzInn5au3wAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4fc37ac90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.686466666667\n"
     ]
    }
   ],
   "source": [
    "autoencoder.load_weights(simple_model_cp_name)\n",
    "eval_autoencoder(autoencoder, x_train_autoencoder, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0WKplYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4c93a8a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.4523\n"
     ]
    }
   ],
   "source": [
    "eval_autoencoder(autoencoder, x_test_autoencoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.3499Epoch 00001: val_loss improved from inf to 0.27811, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3492 - val_loss: 0.2781\n",
      "Epoch 2/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.2865Epoch 00002: val_loss improved from 0.27811 to 0.25899, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2861 - val_loss: 0.2590\n",
      "Epoch 3/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.2707Epoch 00003: val_loss improved from 0.25899 to 0.24739, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2707 - val_loss: 0.2474\n",
      "Epoch 4/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.2657Epoch 00004: val_loss improved from 0.24739 to 0.23678, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2657 - val_loss: 0.2368\n",
      "Epoch 5/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.2628Epoch 00005: val_loss improved from 0.23678 to 0.23082, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2626 - val_loss: 0.2308\n",
      "Epoch 6/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.2603Epoch 00006: val_loss improved from 0.23082 to 0.21701, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2602 - val_loss: 0.2170\n",
      "Epoch 7/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2567Epoch 00007: val_loss improved from 0.21701 to 0.21163, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2567 - val_loss: 0.2116\n",
      "Epoch 8/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.2514Epoch 00008: val_loss improved from 0.21163 to 0.20875, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2513 - val_loss: 0.2087\n",
      "Epoch 9/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.2461Epoch 00009: val_loss improved from 0.20875 to 0.20763, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2461 - val_loss: 0.2076\n",
      "Epoch 10/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.2406Epoch 00010: val_loss improved from 0.20763 to 0.19835, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2402 - val_loss: 0.1983\n",
      "Epoch 11/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.2310Epoch 00011: val_loss improved from 0.19835 to 0.19485, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2309 - val_loss: 0.1949\n",
      "Epoch 12/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.2233Epoch 00012: val_loss improved from 0.19485 to 0.18722, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2230 - val_loss: 0.1872\n",
      "Epoch 13/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.2167Epoch 00013: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2165 - val_loss: 0.1886\n",
      "Epoch 14/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.2104Epoch 00014: val_loss improved from 0.18722 to 0.17945, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2104 - val_loss: 0.1795\n",
      "Epoch 15/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.2042Epoch 00015: val_loss improved from 0.17945 to 0.17339, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2040 - val_loss: 0.1734\n",
      "Epoch 16/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.1976Epoch 00016: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1974 - val_loss: 0.1754\n",
      "Epoch 17/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1915Epoch 00017: val_loss improved from 0.17339 to 0.17243, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1913 - val_loss: 0.1724\n",
      "Epoch 18/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.1857Epoch 00018: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1857 - val_loss: 0.1748\n",
      "Epoch 19/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1803Epoch 00019: val_loss improved from 0.17243 to 0.16523, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1801 - val_loss: 0.1652\n",
      "Epoch 20/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.1751Epoch 00020: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1749 - val_loss: 0.1701\n",
      "Epoch 21/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1701Epoch 00021: val_loss improved from 0.16523 to 0.16333, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1701 - val_loss: 0.1633\n",
      "Epoch 22/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.1656Epoch 00022: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1654 - val_loss: 0.1634\n",
      "Epoch 23/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.1614Epoch 00023: val_loss improved from 0.16333 to 0.15816, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1615 - val_loss: 0.1582\n",
      "Epoch 24/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.1574Epoch 00024: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1573 - val_loss: 0.1628\n",
      "Epoch 25/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1535Epoch 00025: val_loss improved from 0.15816 to 0.15734, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1536 - val_loss: 0.1573\n",
      "Epoch 26/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1495Epoch 00026: val_loss improved from 0.15734 to 0.15615, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1495 - val_loss: 0.1561\n",
      "Epoch 27/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1458Epoch 00027: val_loss improved from 0.15615 to 0.14902, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1457 - val_loss: 0.1490\n",
      "Epoch 28/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1418Epoch 00028: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1418 - val_loss: 0.1552\n",
      "Epoch 29/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1384Epoch 00029: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1384 - val_loss: 0.1641\n",
      "Epoch 30/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.1351Epoch 00030: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1351 - val_loss: 0.1530\n",
      "Epoch 31/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.1317Epoch 00031: val_loss improved from 0.14902 to 0.14796, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1316 - val_loss: 0.1480\n",
      "Epoch 32/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.1285Epoch 00032: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1285 - val_loss: 0.1525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500\n",
      "57600/60000 [===========================>..] - ETA: 0s - loss: 0.1257Epoch 00033: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1257 - val_loss: 0.1516\n",
      "Epoch 34/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.1231Epoch 00034: val_loss improved from 0.14796 to 0.14111, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1231 - val_loss: 0.1411\n",
      "Epoch 35/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.1206Epoch 00035: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1206 - val_loss: 0.1467\n",
      "Epoch 36/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1188Epoch 00036: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1188 - val_loss: 0.1481\n",
      "Epoch 37/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1170Epoch 00037: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1170 - val_loss: 0.1445\n",
      "Epoch 38/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1155Epoch 00038: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1154 - val_loss: 0.1423\n",
      "Epoch 39/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.1140Epoch 00039: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1140 - val_loss: 0.1422\n",
      "Epoch 40/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.1124Epoch 00040: val_loss improved from 0.14111 to 0.13956, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1124 - val_loss: 0.1396\n",
      "Epoch 41/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1115Epoch 00041: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1115 - val_loss: 0.1406\n",
      "Epoch 42/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.1102Epoch 00042: val_loss improved from 0.13956 to 0.13389, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1103 - val_loss: 0.1339\n",
      "Epoch 43/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1089Epoch 00043: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1089 - val_loss: 0.1440\n",
      "Epoch 44/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.1080Epoch 00044: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1080 - val_loss: 0.1411\n",
      "Epoch 45/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1070Epoch 00045: val_loss improved from 0.13389 to 0.13303, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1070 - val_loss: 0.1330\n",
      "Epoch 46/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1063Epoch 00046: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1063 - val_loss: 0.1349\n",
      "Epoch 47/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1054Epoch 00047: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1054 - val_loss: 0.1362\n",
      "Epoch 48/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1045Epoch 00048: val_loss improved from 0.13303 to 0.13056, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1045 - val_loss: 0.1306\n",
      "Epoch 49/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1038Epoch 00049: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1038 - val_loss: 0.1306\n",
      "Epoch 50/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1032Epoch 00050: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1032 - val_loss: 0.1321\n",
      "Epoch 51/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.1024Epoch 00051: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1024 - val_loss: 0.1309\n",
      "Epoch 52/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.1018Epoch 00052: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1018 - val_loss: 0.1320\n",
      "Epoch 53/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.1013Epoch 00053: val_loss improved from 0.13056 to 0.12805, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1013 - val_loss: 0.1280\n",
      "Epoch 54/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1005Epoch 00054: val_loss improved from 0.12805 to 0.12731, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1005 - val_loss: 0.1273\n",
      "Epoch 55/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.1002Epoch 00055: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1003 - val_loss: 0.1347\n",
      "Epoch 56/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0995Epoch 00056: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0994 - val_loss: 0.1338\n",
      "Epoch 57/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0989Epoch 00057: val_loss improved from 0.12731 to 0.12686, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0989 - val_loss: 0.1269\n",
      "Epoch 58/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0983Epoch 00058: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0983 - val_loss: 0.1286\n",
      "Epoch 59/500\n",
      "57600/60000 [===========================>..] - ETA: 0s - loss: 0.0978Epoch 00059: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0977 - val_loss: 0.1301\n",
      "Epoch 60/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0976Epoch 00060: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0976 - val_loss: 0.1284\n",
      "Epoch 61/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0972Epoch 00061: val_loss improved from 0.12686 to 0.12168, saving model to weights.best.dense_model.hdf5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0972 - val_loss: 0.1217\n",
      "Epoch 62/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0967Epoch 00062: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0967 - val_loss: 0.1302\n",
      "Epoch 63/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00063: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0963 - val_loss: 0.1250\n",
      "Epoch 64/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0960Epoch 00064: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0959 - val_loss: 0.1242\n",
      "Epoch 65/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0956Epoch 00065: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0955 - val_loss: 0.1219\n",
      "Epoch 66/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0952Epoch 00066: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0951 - val_loss: 0.1218\n",
      "Epoch 67/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0948Epoch 00067: val_loss did not improve\n",
      "\n",
      "Epoch 00067: reducing learning rate to 0.5.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0948 - val_loss: 0.1285\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0871Epoch 00068: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0871 - val_loss: 0.1271\n",
      "Epoch 69/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0870Epoch 00069: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0870 - val_loss: 0.1274\n",
      "Epoch 70/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0870Epoch 00070: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0870 - val_loss: 0.1281\n",
      "Epoch 71/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0867Epoch 00071: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0867 - val_loss: 0.1272\n",
      "Epoch 72/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0868Epoch 00072: val_loss did not improve\n",
      "\n",
      "Epoch 00072: reducing learning rate to 0.25.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0868 - val_loss: 0.1305\n",
      "Epoch 73/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0858Epoch 00073: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0858 - val_loss: 0.1294\n",
      "Epoch 74/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0857Epoch 00074: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0857 - val_loss: 0.1301\n",
      "Epoch 75/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0856Epoch 00075: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0856 - val_loss: 0.1306\n",
      "Epoch 76/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0855Epoch 00076: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0855 - val_loss: 0.1310\n",
      "Epoch 77/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0854Epoch 00077: val_loss did not improve\n",
      "\n",
      "Epoch 00077: reducing learning rate to 0.125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0855 - val_loss: 0.1311\n",
      "Epoch 78/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0853Epoch 00078: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0853 - val_loss: 0.1318\n",
      "Epoch 79/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0853Epoch 00079: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0853 - val_loss: 0.1313\n",
      "Epoch 80/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0853Epoch 00080: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0853 - val_loss: 0.1319\n",
      "Epoch 81/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0852Epoch 00081: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0852 - val_loss: 0.1317\n",
      "Epoch 82/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0852Epoch 00082: val_loss did not improve\n",
      "\n",
      "Epoch 00082: reducing learning rate to 0.0625.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0852 - val_loss: 0.1319\n",
      "Epoch 83/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0851Epoch 00083: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0851 - val_loss: 0.1321\n",
      "Epoch 84/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0851Epoch 00084: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0851 - val_loss: 0.1323\n",
      "Epoch 85/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0851Epoch 00085: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0851 - val_loss: 0.1323\n",
      "Epoch 86/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0851Epoch 00086: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0851 - val_loss: 0.1324\n",
      "Epoch 87/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00087: val_loss did not improve\n",
      "\n",
      "Epoch 00087: reducing learning rate to 0.03125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1324\n",
      "Epoch 88/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00088: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1324\n",
      "Epoch 89/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00089: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1325\n",
      "Epoch 90/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00090: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1325\n",
      "Epoch 91/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00091: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1325\n",
      "Epoch 92/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00092: val_loss did not improve\n",
      "\n",
      "Epoch 00092: reducing learning rate to 0.015625.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1327\n",
      "Epoch 93/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00093: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1327\n",
      "Epoch 94/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00094: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1327\n",
      "Epoch 95/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00095: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1327\n",
      "Epoch 96/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00096: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1327\n",
      "Epoch 97/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00097: val_loss did not improve\n",
      "\n",
      "Epoch 00097: reducing learning rate to 0.0078125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - val_loss: 0.1328\n",
      "Epoch 98/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00098: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1327\n",
      "Epoch 99/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00099: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1327\n",
      "Epoch 100/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00100: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 101/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00101: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 102/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00102: val_loss did not improve\n",
      "\n",
      "Epoch 00102: reducing learning rate to 0.00390625.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 103/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00103: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 104/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00104: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00105: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 106/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00106: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 107/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00107: val_loss did not improve\n",
      "\n",
      "Epoch 00107: reducing learning rate to 0.001953125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 108/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00108: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 109/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00109: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 110/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00110: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 111/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00111: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 112/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00112: val_loss did not improve\n",
      "\n",
      "Epoch 00112: reducing learning rate to 0.0009765625.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 113/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00113: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 114/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00114: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 115/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00115: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 116/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00116: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 117/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00117: val_loss did not improve\n",
      "\n",
      "Epoch 00117: reducing learning rate to 0.00048828125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 118/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00118: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 119/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00119: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 120/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00120: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 121/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00121: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 122/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00122: val_loss did not improve\n",
      "\n",
      "Epoch 00122: reducing learning rate to 0.000244140625.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 123/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00123: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 124/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00124: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 125/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00125: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 126/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00126: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 127/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00127: val_loss did not improve\n",
      "\n",
      "Epoch 00127: reducing learning rate to 0.0001220703125.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 128/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00128: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 129/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00129: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 130/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00130: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 131/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00131: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 132/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00132: val_loss did not improve\n",
      "\n",
      "Epoch 00132: reducing learning rate to 6.103515625e-05.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 133/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00133: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 134/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00134: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 135/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00135: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 136/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00136: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 137/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00137: val_loss did not improve\n",
      "\n",
      "Epoch 00137: reducing learning rate to 3.0517578125e-05.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 138/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00138: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 139/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00139: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 140/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00140: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00141: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 142/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00142: val_loss did not improve\n",
      "\n",
      "Epoch 00142: reducing learning rate to 1.52587890625e-05.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 143/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00143: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 144/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00144: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 145/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00145: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 146/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00146: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 147/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00147: val_loss did not improve\n",
      "\n",
      "Epoch 00147: reducing learning rate to 7.62939453125e-06.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 148/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00148: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 149/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00149: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 150/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00150: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 151/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00151: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 152/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00152: val_loss did not improve\n",
      "\n",
      "Epoch 00152: reducing learning rate to 3.81469726562e-06.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 153/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00153: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 154/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00154: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 155/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00155: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 156/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00156: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 157/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00157: val_loss did not improve\n",
      "\n",
      "Epoch 00157: reducing learning rate to 1.90734863281e-06.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 158/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00158: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 159/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00159: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 160/500\n",
      "57600/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00160: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 161/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00161: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 162/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00162: val_loss did not improve\n",
      "\n",
      "Epoch 00162: reducing learning rate to 9.53674316406e-07.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 163/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00163: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 164/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00164: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 165/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00165: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 166/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00166: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 167/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00167: val_loss did not improve\n",
      "\n",
      "Epoch 00167: reducing learning rate to 4.76837158203e-07.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 168/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00168: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 169/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00169: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 170/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00170: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 171/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00171: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 172/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00172: val_loss did not improve\n",
      "\n",
      "Epoch 00172: reducing learning rate to 2.38418579102e-07.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 173/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00173: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 174/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00174: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 175/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00175: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 176/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00176: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 177/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00177: val_loss did not improve\n",
      "\n",
      "Epoch 00177: reducing learning rate to 1.19209289551e-07.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 178/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00178: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 179/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00179: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 180/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00180: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 181/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00181: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 182/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00182: val_loss did not improve\n",
      "\n",
      "Epoch 00182: reducing learning rate to 5.96046447754e-08.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 183/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00183: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 184/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00184: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 185/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00185: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 186/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00186: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 187/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00187: val_loss did not improve\n",
      "\n",
      "Epoch 00187: reducing learning rate to 2.98023223877e-08.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 188/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00188: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 189/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00189: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 190/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00190: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 191/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00191: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 192/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00192: val_loss did not improve\n",
      "\n",
      "Epoch 00192: reducing learning rate to 1.49011611938e-08.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 193/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00193: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 194/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00194: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 195/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00195: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 196/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00196: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 197/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00197: val_loss did not improve\n",
      "\n",
      "Epoch 00197: reducing learning rate to 7.45058059692e-09.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 198/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00198: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 199/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00199: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 200/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00200: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 201/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00201: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 202/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00202: val_loss did not improve\n",
      "\n",
      "Epoch 00202: reducing learning rate to 3.72529029846e-09.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 203/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00203: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 204/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00204: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 205/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00205: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 206/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00206: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 207/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00207: val_loss did not improve\n",
      "\n",
      "Epoch 00207: reducing learning rate to 1.86264514923e-09.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 208/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00208: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 209/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00209: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 210/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00210: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 211/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00211: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 212/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00212: val_loss did not improve\n",
      "\n",
      "Epoch 00212: reducing learning rate to 9.31322574615e-10.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00213: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 214/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00214: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 215/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00215: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 216/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00216: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 217/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00217: val_loss did not improve\n",
      "\n",
      "Epoch 00217: reducing learning rate to 4.65661287308e-10.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 218/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00218: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 219/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00219: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 220/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00220: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 221/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00221: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 222/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00222: val_loss did not improve\n",
      "\n",
      "Epoch 00222: reducing learning rate to 2.32830643654e-10.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 223/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00223: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 224/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00224: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 225/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00225: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 226/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00226: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 227/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00227: val_loss did not improve\n",
      "\n",
      "Epoch 00227: reducing learning rate to 1.16415321827e-10.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 228/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00228: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 229/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00229: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 230/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00230: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 231/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00231: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 232/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00232: val_loss did not improve\n",
      "\n",
      "Epoch 00232: reducing learning rate to 5.82076609135e-11.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 233/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00233: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 234/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00234: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 235/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00235: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 236/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00236: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 237/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00237: val_loss did not improve\n",
      "\n",
      "Epoch 00237: reducing learning rate to 2.91038304567e-11.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 238/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00238: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 239/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00239: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 240/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00240: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 241/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00241: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 242/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00242: val_loss did not improve\n",
      "\n",
      "Epoch 00242: reducing learning rate to 1.45519152284e-11.\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 243/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00243: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 244/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00244: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 245/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00245: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 246/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00246: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 247/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00247: val_loss did not improve\n",
      "\n",
      "Epoch 00247: reducing learning rate to 7.27595761418e-12.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 248/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00248: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 249/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00249: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 250/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00250: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 251/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00251: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 252/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00252: val_loss did not improve\n",
      "\n",
      "Epoch 00252: reducing learning rate to 3.63797880709e-12.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 253/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00253: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 254/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00254: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 255/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00255: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 256/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00256: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 257/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00257: val_loss did not improve\n",
      "\n",
      "Epoch 00257: reducing learning rate to 1.81898940355e-12.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 258/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00258: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 259/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00259: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 260/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00260: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 261/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00261: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 262/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00262: val_loss did not improve\n",
      "\n",
      "Epoch 00262: reducing learning rate to 9.09494701773e-13.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 263/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00263: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 264/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00264: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 265/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00265: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 266/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00266: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 267/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00267: val_loss did not improve\n",
      "\n",
      "Epoch 00267: reducing learning rate to 4.54747350886e-13.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 268/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00268: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 269/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00269: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 270/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00270: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 271/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00271: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 272/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00272: val_loss did not improve\n",
      "\n",
      "Epoch 00272: reducing learning rate to 2.27373675443e-13.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 273/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00273: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 274/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00274: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 275/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00275: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 276/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00276: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 277/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00277: val_loss did not improve\n",
      "\n",
      "Epoch 00277: reducing learning rate to 1.13686837722e-13.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 278/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00278: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 279/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00279: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 280/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00280: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 281/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00281: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 282/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00282: val_loss did not improve\n",
      "\n",
      "Epoch 00282: reducing learning rate to 5.68434188608e-14.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 283/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00283: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 284/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00284: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00285: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 286/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00286: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 287/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00287: val_loss did not improve\n",
      "\n",
      "Epoch 00287: reducing learning rate to 2.84217094304e-14.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 288/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00288: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 289/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00289: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 290/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00290: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 291/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00291: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 292/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00292: val_loss did not improve\n",
      "\n",
      "Epoch 00292: reducing learning rate to 1.42108547152e-14.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 293/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00293: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 294/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00294: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 295/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00295: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 296/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00296: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 297/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00297: val_loss did not improve\n",
      "\n",
      "Epoch 00297: reducing learning rate to 7.1054273576e-15.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 298/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00298: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 299/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00299: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 300/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00300: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 301/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00301: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 302/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00302: val_loss did not improve\n",
      "\n",
      "Epoch 00302: reducing learning rate to 3.5527136788e-15.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 303/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00303: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 304/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00304: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 305/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00305: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 306/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00306: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 307/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00307: val_loss did not improve\n",
      "\n",
      "Epoch 00307: reducing learning rate to 1.7763568394e-15.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 308/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00308: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 309/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00309: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 310/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00310: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 311/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00311: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 312/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00312: val_loss did not improve\n",
      "\n",
      "Epoch 00312: reducing learning rate to 8.881784197e-16.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 313/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00313: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 314/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00314: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 315/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00315: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 316/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00316: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 317/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00317: val_loss did not improve\n",
      "\n",
      "Epoch 00317: reducing learning rate to 4.4408920985e-16.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 318/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00318: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 319/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00319: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 320/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00320: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 321/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00321: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 322/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00322: val_loss did not improve\n",
      "\n",
      "Epoch 00322: reducing learning rate to 2.22044604925e-16.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 323/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00323: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 324/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00324: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 325/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00325: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 326/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00326: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 327/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00327: val_loss did not improve\n",
      "\n",
      "Epoch 00327: reducing learning rate to 1.11022302463e-16.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 328/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00328: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 329/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00329: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 330/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00330: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 331/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00331: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 332/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00332: val_loss did not improve\n",
      "\n",
      "Epoch 00332: reducing learning rate to 5.55111512313e-17.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 333/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00333: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 334/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00334: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 335/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00335: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 336/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00336: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 337/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00337: val_loss did not improve\n",
      "\n",
      "Epoch 00337: reducing learning rate to 2.77555756156e-17.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 338/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00338: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 339/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00339: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 340/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00340: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 341/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00341: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 342/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00342: val_loss did not improve\n",
      "\n",
      "Epoch 00342: reducing learning rate to 1.38777878078e-17.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 343/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00343: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 344/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00344: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 345/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00345: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 346/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00346: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 347/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00347: val_loss did not improve\n",
      "\n",
      "Epoch 00347: reducing learning rate to 6.93889390391e-18.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 348/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00348: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 349/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00349: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 350/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00350: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 351/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00351: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 352/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00352: val_loss did not improve\n",
      "\n",
      "Epoch 00352: reducing learning rate to 3.46944695195e-18.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 353/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00353: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 354/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00354: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 355/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00355: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 356/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00356: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 357/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00357: val_loss did not improve\n",
      "\n",
      "Epoch 00357: reducing learning rate to 1.73472347598e-18.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 358/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00358: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 359/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00359: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 360/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00360: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 361/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00361: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 362/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00362: val_loss did not improve\n",
      "\n",
      "Epoch 00362: reducing learning rate to 8.67361737988e-19.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 363/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00363: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 364/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00364: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 365/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00365: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 366/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00366: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 367/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00367: val_loss did not improve\n",
      "\n",
      "Epoch 00367: reducing learning rate to 4.33680868994e-19.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 368/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00368: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 369/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00369: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 370/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00370: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 371/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00371: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 372/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00372: val_loss did not improve\n",
      "\n",
      "Epoch 00372: reducing learning rate to 2.16840434497e-19.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 373/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00373: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 374/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00374: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 375/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00375: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 376/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00376: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 377/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00377: val_loss did not improve\n",
      "\n",
      "Epoch 00377: reducing learning rate to 1.08420217249e-19.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 378/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00378: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 379/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00379: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 380/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00380: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 381/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00381: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 382/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00382: val_loss did not improve\n",
      "\n",
      "Epoch 00382: reducing learning rate to 5.42101086243e-20.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 383/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00383: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 384/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00384: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 385/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00385: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 386/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00386: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 387/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00387: val_loss did not improve\n",
      "\n",
      "Epoch 00387: reducing learning rate to 2.71050543121e-20.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 388/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00388: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 389/500\n",
      "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00389: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 390/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00390: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 391/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00391: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 392/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00392: val_loss did not improve\n",
      "\n",
      "Epoch 00392: reducing learning rate to 1.35525271561e-20.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00393: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 394/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00394: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 395/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00395: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 396/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00396: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 397/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00397: val_loss did not improve\n",
      "\n",
      "Epoch 00397: reducing learning rate to 6.77626357803e-21.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 398/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00398: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 399/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00399: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 400/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00400: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 401/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00401: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 402/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00402: val_loss did not improve\n",
      "\n",
      "Epoch 00402: reducing learning rate to 3.38813178902e-21.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 403/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00403: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 404/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00404: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 405/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00405: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 406/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00406: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 407/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00407: val_loss did not improve\n",
      "\n",
      "Epoch 00407: reducing learning rate to 1.69406589451e-21.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 408/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00408: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 409/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00409: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 410/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00410: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 411/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00411: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 412/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00412: val_loss did not improve\n",
      "\n",
      "Epoch 00412: reducing learning rate to 8.47032947254e-22.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 413/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00413: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 414/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00414: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 415/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00415: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 416/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00416: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 417/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00417: val_loss did not improve\n",
      "\n",
      "Epoch 00417: reducing learning rate to 4.23516473627e-22.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 418/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00418: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 419/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00419: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 420/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00420: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 421/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00421: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 422/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00422: val_loss did not improve\n",
      "\n",
      "Epoch 00422: reducing learning rate to 2.11758236814e-22.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 423/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00423: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 424/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00424: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 425/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00425: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 426/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00426: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 427/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00427: val_loss did not improve\n",
      "\n",
      "Epoch 00427: reducing learning rate to 1.05879118407e-22.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 428/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00428: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 429/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00429: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 430/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00430: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 431/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00431: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 432/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00432: val_loss did not improve\n",
      "\n",
      "Epoch 00432: reducing learning rate to 5.29395592034e-23.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 433/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00433: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 434/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00434: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 435/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00435: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 436/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00436: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 437/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00437: val_loss did not improve\n",
      "\n",
      "Epoch 00437: reducing learning rate to 2.64697796017e-23.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 438/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00438: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 439/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00439: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 440/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00440: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 441/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00441: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 442/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00442: val_loss did not improve\n",
      "\n",
      "Epoch 00442: reducing learning rate to 1.32348898008e-23.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 443/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00443: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 444/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00444: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 445/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00445: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 446/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00446: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 447/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00447: val_loss did not improve\n",
      "\n",
      "Epoch 00447: reducing learning rate to 6.61744490042e-24.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 448/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00448: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 449/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00449: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 450/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00450: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 451/500\n",
      "57088/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00451: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 452/500\n",
      "57600/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00452: val_loss did not improve\n",
      "\n",
      "Epoch 00452: reducing learning rate to 3.30872245021e-24.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 453/500\n",
      "58112/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00453: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 454/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00454: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 455/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00455: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 456/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00456: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 457/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00457: val_loss did not improve\n",
      "\n",
      "Epoch 00457: reducing learning rate to 1.65436122511e-24.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 458/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00458: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 459/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00459: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 460/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00460: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 461/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00461: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 462/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00462: val_loss did not improve\n",
      "\n",
      "Epoch 00462: reducing learning rate to 8.27180612553e-25.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 463/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00463: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 464/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00464: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 465/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00465: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 466/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00466: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 467/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00467: val_loss did not improve\n",
      "\n",
      "Epoch 00467: reducing learning rate to 4.13590306277e-25.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 468/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00468: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 469/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00469: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 470/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00470: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 471/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00471: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 472/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00472: val_loss did not improve\n",
      "\n",
      "Epoch 00472: reducing learning rate to 2.06795153138e-25.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 473/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00473: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 474/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00474: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 475/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00475: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 476/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00476: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 477/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00477: val_loss did not improve\n",
      "\n",
      "Epoch 00477: reducing learning rate to 1.03397576569e-25.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 478/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00478: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 479/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00479: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 480/500\n",
      "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00480: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 481/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00481: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 482/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00482: val_loss did not improve\n",
      "\n",
      "Epoch 00482: reducing learning rate to 5.16987882846e-26.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 483/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00483: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 484/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00484: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 485/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00485: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 486/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00486: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 487/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00487: val_loss did not improve\n",
      "\n",
      "Epoch 00487: reducing learning rate to 2.58493941423e-26.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 488/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00488: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 489/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00489: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 490/500\n",
      "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00490: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 491/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00491: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 492/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00492: val_loss did not improve\n",
      "\n",
      "Epoch 00492: reducing learning rate to 1.29246970711e-26.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 493/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00493: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 494/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00494: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 495/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00495: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 496/500\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00496: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 497/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00497: val_loss did not improve\n",
      "\n",
      "Epoch 00497: reducing learning rate to 6.46234853557e-27.\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 498/500\n",
      "57600/60000 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00498: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 499/500\n",
      "56576/60000 [===========================>..] - ETA: 0s - loss: 0.0850Epoch 00499: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n",
      "Epoch 500/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00500: val_loss did not improve\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - val_loss: 0.1328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb537c371d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(512, activation='relu')(input_img)\n",
    "encoded = Dense(256, activation='relu')(encoded)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(512, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "dense_model_cp_name = 'weights.best.dense_model.hdf5'\n",
    "autoencoder.fit(x_train_autoencoder, x_train_autoencoder,\n",
    "                epochs=EPOCH,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                callbacks=[ModelCheckpoint(dense_model_cp_name, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "                           ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, factor=0.5)],\n",
    "                validation_data=(x_test_autoencoder, x_test_autoencoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAAAAAAyAAPcQAAAAAAADLwf5oo1Z5VUckbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb51fe6da10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.4629\n"
     ]
    }
   ],
   "source": [
    "autoencoder.load_weights(dense_model_cp_name)\n",
    "eval_autoencoder(autoencoder, x_test_autoencoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAEAB8BIHAAAAAACgAP4frGuuB/T99MAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb537ca53d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.671616666667\n"
     ]
    }
   ],
   "source": [
    "eval_autoencoder(autoencoder, x_train_autoencoder, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn = np.reshape(x_train_autoencoder, (len(x_train), 28, 28, 1)) \n",
    "x_test_cnn = np.reshape(x_test_autoencoder, (len(x_test), 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.3668Epoch 00001: val_loss improved from inf to 0.29634, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3666 - val_loss: 0.2963\n",
      "Epoch 2/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.2824Epoch 00002: val_loss improved from 0.29634 to 0.20651, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2822 - val_loss: 0.2065\n",
      "Epoch 3/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.2376Epoch 00003: val_loss improved from 0.20651 to 0.19165, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2375 - val_loss: 0.1917\n",
      "Epoch 4/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.2116Epoch 00004: val_loss improved from 0.19165 to 0.16975, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.2115 - val_loss: 0.1697\n",
      "Epoch 5/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1963Epoch 00005: val_loss improved from 0.16975 to 0.16818, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1962 - val_loss: 0.1682\n",
      "Epoch 6/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1851Epoch 00006: val_loss improved from 0.16818 to 0.15248, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1851 - val_loss: 0.1525\n",
      "Epoch 7/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1756Epoch 00007: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1756 - val_loss: 0.1577\n",
      "Epoch 8/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1668Epoch 00008: val_loss improved from 0.15248 to 0.14162, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1668 - val_loss: 0.1416\n",
      "Epoch 9/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1595Epoch 00009: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1595 - val_loss: 0.1443\n",
      "Epoch 10/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1535Epoch 00010: val_loss improved from 0.14162 to 0.13322, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1535 - val_loss: 0.1332\n",
      "Epoch 11/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1489Epoch 00011: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1489 - val_loss: 0.1404\n",
      "Epoch 12/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1449Epoch 00012: val_loss improved from 0.13322 to 0.12911, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1448 - val_loss: 0.1291\n",
      "Epoch 13/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1417Epoch 00013: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1417 - val_loss: 0.1364\n",
      "Epoch 14/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1388Epoch 00014: val_loss improved from 0.12911 to 0.12481, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1388 - val_loss: 0.1248\n",
      "Epoch 15/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1359Epoch 00015: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1359 - val_loss: 0.1294\n",
      "Epoch 16/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1334Epoch 00016: val_loss improved from 0.12481 to 0.12064, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1334 - val_loss: 0.1206\n",
      "Epoch 17/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1316Epoch 00017: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1316 - val_loss: 0.1242\n",
      "Epoch 18/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1297Epoch 00018: val_loss improved from 0.12064 to 0.11686, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1297 - val_loss: 0.1169\n",
      "Epoch 19/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.1281Epoch 00019: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1281 - val_loss: 0.1203\n",
      "Epoch 20/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1263Epoch 00020: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1263 - val_loss: 0.1173\n",
      "Epoch 21/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1252Epoch 00021: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1252 - val_loss: 0.1188\n",
      "Epoch 22/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1235Epoch 00022: val_loss improved from 0.11686 to 0.11463, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1235 - val_loss: 0.1146\n",
      "Epoch 23/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1225Epoch 00023: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1225 - val_loss: 0.1158\n",
      "Epoch 24/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1215Epoch 00024: val_loss improved from 0.11463 to 0.11249, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1215 - val_loss: 0.1125\n",
      "Epoch 25/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1204Epoch 00025: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1204 - val_loss: 0.1142\n",
      "Epoch 26/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1193Epoch 00026: val_loss improved from 0.11249 to 0.11153, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1193 - val_loss: 0.1115\n",
      "Epoch 27/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1180Epoch 00027: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1180 - val_loss: 0.1125\n",
      "Epoch 28/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1171Epoch 00028: val_loss improved from 0.11153 to 0.10972, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1171 - val_loss: 0.1097\n",
      "Epoch 29/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1163Epoch 00029: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1163 - val_loss: 0.1132\n",
      "Epoch 30/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1155Epoch 00030: val_loss improved from 0.10972 to 0.10778, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1155 - val_loss: 0.1078\n",
      "Epoch 31/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1150Epoch 00031: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1150 - val_loss: 0.1120\n",
      "Epoch 32/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1138Epoch 00032: val_loss improved from 0.10778 to 0.10740, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1138 - val_loss: 0.1074\n",
      "Epoch 33/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1136Epoch 00033: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1136 - val_loss: 0.1089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1127Epoch 00034: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1127 - val_loss: 0.1094\n",
      "Epoch 35/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1122Epoch 00035: val_loss improved from 0.10740 to 0.10484, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1122 - val_loss: 0.1048\n",
      "Epoch 36/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1115Epoch 00036: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1115 - val_loss: 0.1082\n",
      "Epoch 37/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1109Epoch 00037: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1109 - val_loss: 0.1077\n",
      "Epoch 38/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1107Epoch 00038: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1107 - val_loss: 0.1050\n",
      "Epoch 39/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1104Epoch 00039: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1104 - val_loss: 0.1087\n",
      "Epoch 40/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1100Epoch 00040: val_loss improved from 0.10484 to 0.10379, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1100 - val_loss: 0.1038\n",
      "Epoch 41/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1098Epoch 00041: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1098 - val_loss: 0.1063\n",
      "Epoch 42/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1095Epoch 00042: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1095 - val_loss: 0.1039\n",
      "Epoch 43/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1091Epoch 00043: val_loss improved from 0.10379 to 0.10262, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1090 - val_loss: 0.1026\n",
      "Epoch 44/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1084Epoch 00044: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1084 - val_loss: 0.1062\n",
      "Epoch 45/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1084Epoch 00045: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1084 - val_loss: 0.1030\n",
      "Epoch 46/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1078Epoch 00046: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1078 - val_loss: 0.1079\n",
      "Epoch 47/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1076Epoch 00047: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1076 - val_loss: 0.1027\n",
      "Epoch 48/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1072Epoch 00048: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1072 - val_loss: 0.1029\n",
      "Epoch 49/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1071Epoch 00049: val_loss did not improve\n",
      "\n",
      "Epoch 00049: reducing learning rate to 0.5.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1070 - val_loss: 0.1047\n",
      "Epoch 50/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1041Epoch 00050: val_loss improved from 0.10262 to 0.10104, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1041 - val_loss: 0.1010\n",
      "Epoch 51/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1040Epoch 00051: val_loss improved from 0.10104 to 0.10104, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1040 - val_loss: 0.1010\n",
      "Epoch 52/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1038Epoch 00052: val_loss improved from 0.10104 to 0.09988, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1038 - val_loss: 0.0999\n",
      "Epoch 53/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1035Epoch 00053: val_loss improved from 0.09988 to 0.09919, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1035 - val_loss: 0.0992\n",
      "Epoch 54/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1034Epoch 00054: val_loss improved from 0.09919 to 0.09894, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1034 - val_loss: 0.0989\n",
      "Epoch 55/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1031Epoch 00055: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1031 - val_loss: 0.0990\n",
      "Epoch 56/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1029Epoch 00056: val_loss improved from 0.09894 to 0.09825, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1029 - val_loss: 0.0983\n",
      "Epoch 57/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1027Epoch 00057: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1027 - val_loss: 0.0989\n",
      "Epoch 58/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1026Epoch 00058: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1026 - val_loss: 0.0988\n",
      "Epoch 59/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1024Epoch 00059: val_loss improved from 0.09825 to 0.09779, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1024 - val_loss: 0.0978\n",
      "Epoch 60/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1023Epoch 00060: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1023 - val_loss: 0.0980\n",
      "Epoch 61/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1023Epoch 00061: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1023 - val_loss: 0.0988\n",
      "Epoch 62/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1021Epoch 00062: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1021 - val_loss: 0.0986\n",
      "Epoch 63/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1018Epoch 00063: val_loss improved from 0.09779 to 0.09650, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1018 - val_loss: 0.0965\n",
      "Epoch 64/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1019Epoch 00064: val_loss improved from 0.09650 to 0.09630, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1018 - val_loss: 0.0963\n",
      "Epoch 65/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1016Epoch 00065: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1016 - val_loss: 0.0971\n",
      "Epoch 66/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1014Epoch 00066: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1013 - val_loss: 0.0965\n",
      "Epoch 67/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1015Epoch 00067: val_loss improved from 0.09630 to 0.09621, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1015 - val_loss: 0.0962\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1014Epoch 00068: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1014 - val_loss: 0.0963\n",
      "Epoch 69/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1011Epoch 00069: val_loss improved from 0.09621 to 0.09579, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1011 - val_loss: 0.0958\n",
      "Epoch 70/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1009Epoch 00070: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1009 - val_loss: 0.0976\n",
      "Epoch 71/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1008Epoch 00071: val_loss improved from 0.09579 to 0.09556, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1008 - val_loss: 0.0956\n",
      "Epoch 72/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1007Epoch 00072: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1007 - val_loss: 0.0971\n",
      "Epoch 73/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1008Epoch 00073: val_loss improved from 0.09556 to 0.09550, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1008 - val_loss: 0.0955\n",
      "Epoch 74/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1004Epoch 00074: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1004 - val_loss: 0.0957\n",
      "Epoch 75/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1002Epoch 00075: val_loss improved from 0.09550 to 0.09458, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1002 - val_loss: 0.0946\n",
      "Epoch 76/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1003Epoch 00076: val_loss improved from 0.09458 to 0.09441, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1003 - val_loss: 0.0944\n",
      "Epoch 77/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1003Epoch 00077: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1003 - val_loss: 0.0969\n",
      "Epoch 78/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1000Epoch 00078: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1000 - val_loss: 0.0950\n",
      "Epoch 79/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.1001Epoch 00079: val_loss improved from 0.09441 to 0.09400, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1001 - val_loss: 0.0940\n",
      "Epoch 80/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1000Epoch 00080: val_loss improved from 0.09400 to 0.09396, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1000 - val_loss: 0.0940\n",
      "Epoch 81/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0998Epoch 00081: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0998 - val_loss: 0.0955\n",
      "Epoch 82/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.1000Epoch 00082: val_loss improved from 0.09396 to 0.09368, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1000 - val_loss: 0.0937\n",
      "Epoch 83/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0999Epoch 00083: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0999 - val_loss: 0.0957\n",
      "Epoch 84/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0999Epoch 00084: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0999 - val_loss: 0.0940\n",
      "Epoch 85/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0998Epoch 00085: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0998 - val_loss: 0.0959\n",
      "Epoch 86/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0995Epoch 00086: val_loss improved from 0.09368 to 0.09367, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0995 - val_loss: 0.0937\n",
      "Epoch 87/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0995Epoch 00087: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0994 - val_loss: 0.0959\n",
      "Epoch 88/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0993Epoch 00088: val_loss did not improve\n",
      "\n",
      "Epoch 00088: reducing learning rate to 0.25.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0993 - val_loss: 0.0940\n",
      "Epoch 89/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0982Epoch 00089: val_loss improved from 0.09367 to 0.09315, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0982 - val_loss: 0.0932\n",
      "Epoch 90/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0982Epoch 00090: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0981 - val_loss: 0.0935\n",
      "Epoch 91/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0981Epoch 00091: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0981 - val_loss: 0.0932\n",
      "Epoch 92/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0980Epoch 00092: val_loss improved from 0.09315 to 0.09299, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0980 - val_loss: 0.0930\n",
      "Epoch 93/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0979Epoch 00093: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0979 - val_loss: 0.0932\n",
      "Epoch 94/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0979Epoch 00094: val_loss improved from 0.09299 to 0.09294, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0979 - val_loss: 0.0929\n",
      "Epoch 95/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0978Epoch 00095: val_loss improved from 0.09294 to 0.09252, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0978 - val_loss: 0.0925\n",
      "Epoch 96/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0977Epoch 00096: val_loss improved from 0.09252 to 0.09251, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0977 - val_loss: 0.0925\n",
      "Epoch 97/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0976Epoch 00097: val_loss improved from 0.09251 to 0.09227, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0976 - val_loss: 0.0923\n",
      "Epoch 98/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0976Epoch 00098: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0976 - val_loss: 0.0928\n",
      "Epoch 99/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0975Epoch 00099: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0975 - val_loss: 0.0928\n",
      "Epoch 100/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0974Epoch 00100: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0975 - val_loss: 0.0927\n",
      "Epoch 101/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0974Epoch 00101: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0974 - val_loss: 0.0926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0973Epoch 00102: val_loss improved from 0.09227 to 0.09206, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0973 - val_loss: 0.0921\n",
      "Epoch 103/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0972Epoch 00103: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0973 - val_loss: 0.0922\n",
      "Epoch 104/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0972Epoch 00104: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0972 - val_loss: 0.0928\n",
      "Epoch 105/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0971Epoch 00105: val_loss improved from 0.09206 to 0.09200, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0971 - val_loss: 0.0920\n",
      "Epoch 106/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0972Epoch 00106: val_loss improved from 0.09200 to 0.09175, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0972 - val_loss: 0.0918\n",
      "Epoch 107/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0971Epoch 00107: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0971 - val_loss: 0.0918\n",
      "Epoch 108/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0970Epoch 00108: val_loss improved from 0.09175 to 0.09173, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0970 - val_loss: 0.0917\n",
      "Epoch 109/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0970Epoch 00109: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0970 - val_loss: 0.0919\n",
      "Epoch 110/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0969Epoch 00110: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0969 - val_loss: 0.0920\n",
      "Epoch 111/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0968Epoch 00111: val_loss improved from 0.09173 to 0.09167, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0969 - val_loss: 0.0917\n",
      "Epoch 112/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0968Epoch 00112: val_loss did not improve\n",
      "\n",
      "Epoch 00112: reducing learning rate to 0.125.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0968 - val_loss: 0.0922\n",
      "Epoch 113/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0966Epoch 00113: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0966 - val_loss: 0.0918\n",
      "Epoch 114/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0966Epoch 00114: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0966 - val_loss: 0.0918\n",
      "Epoch 115/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0966Epoch 00115: val_loss improved from 0.09167 to 0.09147, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0965 - val_loss: 0.0915\n",
      "Epoch 116/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0965Epoch 00116: val_loss improved from 0.09147 to 0.09144, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0965 - val_loss: 0.0914\n",
      "Epoch 117/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0965Epoch 00117: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0965 - val_loss: 0.0915\n",
      "Epoch 118/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0964Epoch 00118: val_loss improved from 0.09144 to 0.09141, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0964 - val_loss: 0.0914\n",
      "Epoch 119/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0964Epoch 00119: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0964 - val_loss: 0.0916\n",
      "Epoch 120/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0964Epoch 00120: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0964 - val_loss: 0.0915\n",
      "Epoch 121/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00121: val_loss did not improve\n",
      "\n",
      "Epoch 00121: reducing learning rate to 0.0625.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0963 - val_loss: 0.0917\n",
      "Epoch 122/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00122: val_loss improved from 0.09141 to 0.09132, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0963 - val_loss: 0.0913\n",
      "Epoch 123/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00123: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0963 - val_loss: 0.0914\n",
      "Epoch 124/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0962Epoch 00124: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0963 - val_loss: 0.0914\n",
      "Epoch 125/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00125: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0962 - val_loss: 0.0914\n",
      "Epoch 126/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0962Epoch 00126: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0962 - val_loss: 0.0915\n",
      "Epoch 127/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0962Epoch 00127: val_loss improved from 0.09132 to 0.09132, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0962 - val_loss: 0.0913\n",
      "Epoch 128/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0962Epoch 00128: val_loss improved from 0.09132 to 0.09117, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0962 - val_loss: 0.0912\n",
      "Epoch 129/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00129: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0962 - val_loss: 0.0913\n",
      "Epoch 130/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00130: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0961 - val_loss: 0.0914\n",
      "Epoch 131/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0962Epoch 00131: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0961 - val_loss: 0.0913\n",
      "Epoch 132/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00132: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0961 - val_loss: 0.0913\n",
      "Epoch 133/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00133: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0961 - val_loss: 0.0912\n",
      "Epoch 134/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00134: val_loss did not improve\n",
      "\n",
      "Epoch 00134: reducing learning rate to 0.03125.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0961 - val_loss: 0.0913\n",
      "Epoch 135/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00135: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0913\n",
      "Epoch 136/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00136: val_loss improved from 0.09117 to 0.09114, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 137/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00137: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0912\n",
      "Epoch 138/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00138: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0912\n",
      "Epoch 139/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00139: val_loss improved from 0.09114 to 0.09112, saving model to weights.best.cnn_model.hdf5\n",
      "\n",
      "Epoch 00139: reducing learning rate to 0.015625.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 140/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00140: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 141/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00141: val_loss improved from 0.09112 to 0.09111, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 142/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00142: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 143/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00143: val_loss improved from 0.09111 to 0.09110, saving model to weights.best.cnn_model.hdf5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 144/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00144: val_loss did not improve\n",
      "\n",
      "Epoch 00144: reducing learning rate to 0.0078125.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 145/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00145: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 146/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00146: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0912\n",
      "Epoch 147/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00147: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 148/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00148: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 149/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00149: val_loss did not improve\n",
      "\n",
      "Epoch 00149: reducing learning rate to 0.00390625.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 150/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00150: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 151/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00151: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 152/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00152: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 153/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00153: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 154/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00154: val_loss improved from 0.09110 to 0.09109, saving model to weights.best.cnn_model.hdf5\n",
      "\n",
      "Epoch 00154: reducing learning rate to 0.001953125.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 155/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00155: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 156/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00156: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 157/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00157: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 158/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00158: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 159/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00159: val_loss did not improve\n",
      "\n",
      "Epoch 00159: reducing learning rate to 0.0009765625.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 160/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00160: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 161/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00161: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 162/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00162: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 163/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00163: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 164/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00164: val_loss did not improve\n",
      "\n",
      "Epoch 00164: reducing learning rate to 0.00048828125.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 165/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00165: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 166/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00166: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 167/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00167: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 168/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00168: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 169/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00169: val_loss did not improve\n",
      "\n",
      "Epoch 00169: reducing learning rate to 0.000244140625.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 170/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00170: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 171/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00171: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 172/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00172: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 173/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00173: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 174/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00174: val_loss did not improve\n",
      "\n",
      "Epoch 00174: reducing learning rate to 0.0001220703125.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 175/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00175: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 176/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00176: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 177/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00177: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 178/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00178: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 179/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00179: val_loss did not improve\n",
      "\n",
      "Epoch 00179: reducing learning rate to 6.103515625e-05.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 180/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00180: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 181/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00181: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 182/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00182: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 183/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00183: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 184/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00184: val_loss did not improve\n",
      "\n",
      "Epoch 00184: reducing learning rate to 3.0517578125e-05.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 185/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00185: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 186/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00186: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 187/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00187: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 188/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00188: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 189/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00189: val_loss did not improve\n",
      "\n",
      "Epoch 00189: reducing learning rate to 1.52587890625e-05.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 190/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00190: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 191/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00191: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 192/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00192: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 193/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00193: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 194/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00194: val_loss did not improve\n",
      "\n",
      "Epoch 00194: reducing learning rate to 7.62939453125e-06.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 195/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00195: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 196/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00196: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 197/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00197: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 198/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00198: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 199/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00199: val_loss did not improve\n",
      "\n",
      "Epoch 00199: reducing learning rate to 3.81469726562e-06.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 200/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00200: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 201/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00201: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 202/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00202: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 203/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00203: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 204/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00204: val_loss did not improve\n",
      "\n",
      "Epoch 00204: reducing learning rate to 1.90734863281e-06.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 205/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00205: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 206/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00206: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 207/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00207: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 208/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00208: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 209/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00209: val_loss did not improve\n",
      "\n",
      "Epoch 00209: reducing learning rate to 9.53674316406e-07.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 210/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00210: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 211/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00211: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 212/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00212: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 213/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00213: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 214/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00214: val_loss did not improve\n",
      "\n",
      "Epoch 00214: reducing learning rate to 4.76837158203e-07.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 215/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00215: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 216/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00216: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 217/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00217: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 218/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00218: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 219/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00219: val_loss did not improve\n",
      "\n",
      "Epoch 00219: reducing learning rate to 2.38418579102e-07.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 220/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00220: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 221/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00221: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 222/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00222: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 223/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00223: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 224/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00224: val_loss did not improve\n",
      "\n",
      "Epoch 00224: reducing learning rate to 1.19209289551e-07.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 225/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00225: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 226/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00226: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 227/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00227: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 228/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00228: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 229/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00229: val_loss did not improve\n",
      "\n",
      "Epoch 00229: reducing learning rate to 5.96046447754e-08.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 230/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00230: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 231/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00231: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 232/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00232: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 233/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00233: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 234/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00234: val_loss did not improve\n",
      "\n",
      "Epoch 00234: reducing learning rate to 2.98023223877e-08.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 235/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00235: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 236/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00236: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 237/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00237: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 238/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00238: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 239/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00239: val_loss did not improve\n",
      "\n",
      "Epoch 00239: reducing learning rate to 1.49011611938e-08.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 240/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00240: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 241/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00241: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 242/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00242: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 243/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00243: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 244/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00244: val_loss did not improve\n",
      "\n",
      "Epoch 00244: reducing learning rate to 7.45058059692e-09.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 245/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00245: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 246/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00246: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 247/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00247: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 248/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00248: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 249/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00249: val_loss did not improve\n",
      "\n",
      "Epoch 00249: reducing learning rate to 3.72529029846e-09.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 250/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00250: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 251/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00251: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 252/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00252: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 253/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00253: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 254/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00254: val_loss did not improve\n",
      "\n",
      "Epoch 00254: reducing learning rate to 1.86264514923e-09.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 255/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00255: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 256/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00256: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 257/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00257: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 258/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00258: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 259/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00259: val_loss did not improve\n",
      "\n",
      "Epoch 00259: reducing learning rate to 9.31322574615e-10.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 260/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00260: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 261/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00261: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 262/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00262: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 263/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00263: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 264/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00264: val_loss did not improve\n",
      "\n",
      "Epoch 00264: reducing learning rate to 4.65661287308e-10.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 265/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00265: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 266/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00266: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 267/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00267: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 268/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00268: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 269/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00269: val_loss did not improve\n",
      "\n",
      "Epoch 00269: reducing learning rate to 2.32830643654e-10.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 270/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00270: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 271/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00271: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 272/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00272: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 273/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00273: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 274/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00274: val_loss did not improve\n",
      "\n",
      "Epoch 00274: reducing learning rate to 1.16415321827e-10.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 275/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00275: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 276/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00276: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 277/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00277: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 278/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00278: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 279/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00279: val_loss did not improve\n",
      "\n",
      "Epoch 00279: reducing learning rate to 5.82076609135e-11.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 280/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00280: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 281/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00281: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 282/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00282: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 283/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00283: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 284/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00284: val_loss did not improve\n",
      "\n",
      "Epoch 00284: reducing learning rate to 2.91038304567e-11.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 285/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00285: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 286/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00286: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 287/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00287: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 288/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00288: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 289/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00289: val_loss did not improve\n",
      "\n",
      "Epoch 00289: reducing learning rate to 1.45519152284e-11.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 290/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00290: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 291/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00291: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 292/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00292: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 293/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00293: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 294/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00294: val_loss did not improve\n",
      "\n",
      "Epoch 00294: reducing learning rate to 7.27595761418e-12.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 295/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00295: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 296/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00296: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 297/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00297: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 298/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00298: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 299/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00299: val_loss did not improve\n",
      "\n",
      "Epoch 00299: reducing learning rate to 3.63797880709e-12.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 300/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00300: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 301/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00301: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 302/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00302: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 303/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00303: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 304/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00304: val_loss did not improve\n",
      "\n",
      "Epoch 00304: reducing learning rate to 1.81898940355e-12.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 305/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00305: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 306/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00306: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 307/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00307: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 308/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00308: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 309/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00309: val_loss did not improve\n",
      "\n",
      "Epoch 00309: reducing learning rate to 9.09494701773e-13.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 310/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00310: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 311/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00311: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 312/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00312: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 313/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00313: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 314/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00314: val_loss did not improve\n",
      "\n",
      "Epoch 00314: reducing learning rate to 4.54747350886e-13.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00315: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 316/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00316: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 317/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00317: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 318/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00318: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 319/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00319: val_loss did not improve\n",
      "\n",
      "Epoch 00319: reducing learning rate to 2.27373675443e-13.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 320/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00320: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 321/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00321: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 322/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00322: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 323/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00323: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 324/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00324: val_loss did not improve\n",
      "\n",
      "Epoch 00324: reducing learning rate to 1.13686837722e-13.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 325/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00325: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 326/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00326: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 327/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00327: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 328/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00328: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 329/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00329: val_loss did not improve\n",
      "\n",
      "Epoch 00329: reducing learning rate to 5.68434188608e-14.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 330/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00330: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 331/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00331: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 332/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00332: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 333/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00333: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 334/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00334: val_loss did not improve\n",
      "\n",
      "Epoch 00334: reducing learning rate to 2.84217094304e-14.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 335/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00335: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 336/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00336: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 337/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00337: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 338/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00338: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 339/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00339: val_loss did not improve\n",
      "\n",
      "Epoch 00339: reducing learning rate to 1.42108547152e-14.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 340/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00340: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 341/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00341: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 342/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00342: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 343/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00343: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 344/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00344: val_loss did not improve\n",
      "\n",
      "Epoch 00344: reducing learning rate to 7.1054273576e-15.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 345/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00345: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 346/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00346: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 347/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00347: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 348/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00348: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 349/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00349: val_loss did not improve\n",
      "\n",
      "Epoch 00349: reducing learning rate to 3.5527136788e-15.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 350/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00350: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 351/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00351: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 352/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00352: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 353/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00353: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 354/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00354: val_loss did not improve\n",
      "\n",
      "Epoch 00354: reducing learning rate to 1.7763568394e-15.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 355/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00355: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 356/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00356: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 357/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00357: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 358/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00358: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 359/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00359: val_loss did not improve\n",
      "\n",
      "Epoch 00359: reducing learning rate to 8.881784197e-16.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 360/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00360: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 361/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00361: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 362/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00362: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 363/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00363: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 364/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00364: val_loss did not improve\n",
      "\n",
      "Epoch 00364: reducing learning rate to 4.4408920985e-16.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 365/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00365: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 366/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00366: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 367/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00367: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 368/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00368: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 369/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00369: val_loss did not improve\n",
      "\n",
      "Epoch 00369: reducing learning rate to 2.22044604925e-16.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 370/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00370: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 371/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00371: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 372/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00372: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 373/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00373: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 374/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00374: val_loss did not improve\n",
      "\n",
      "Epoch 00374: reducing learning rate to 1.11022302463e-16.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 375/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00375: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 376/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00376: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 377/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00377: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 378/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00378: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 379/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00379: val_loss did not improve\n",
      "\n",
      "Epoch 00379: reducing learning rate to 5.55111512313e-17.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 380/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00380: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 381/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00381: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 382/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00382: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 383/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00383: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 384/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00384: val_loss did not improve\n",
      "\n",
      "Epoch 00384: reducing learning rate to 2.77555756156e-17.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 385/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00385: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 386/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00386: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 387/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00387: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 388/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00388: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 389/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00389: val_loss did not improve\n",
      "\n",
      "Epoch 00389: reducing learning rate to 1.38777878078e-17.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 390/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00390: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 391/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00391: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 392/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00392: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 393/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00393: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 394/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00394: val_loss did not improve\n",
      "\n",
      "Epoch 00394: reducing learning rate to 6.93889390391e-18.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 395/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00395: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 396/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00396: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 397/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00397: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 398/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00398: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 399/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00399: val_loss did not improve\n",
      "\n",
      "Epoch 00399: reducing learning rate to 3.46944695195e-18.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 400/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00400: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 401/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00401: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 402/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00402: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 403/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00403: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 404/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00404: val_loss did not improve\n",
      "\n",
      "Epoch 00404: reducing learning rate to 1.73472347598e-18.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 405/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00405: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 406/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00406: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 407/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00407: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 408/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00408: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 409/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00409: val_loss did not improve\n",
      "\n",
      "Epoch 00409: reducing learning rate to 8.67361737988e-19.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 410/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00410: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 411/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00411: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 412/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00412: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 413/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00413: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 414/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00414: val_loss did not improve\n",
      "\n",
      "Epoch 00414: reducing learning rate to 4.33680868994e-19.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 415/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00415: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 416/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00416: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 417/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00417: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 418/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00418: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 419/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00419: val_loss did not improve\n",
      "\n",
      "Epoch 00419: reducing learning rate to 2.16840434497e-19.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 420/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00420: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 421/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00421: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 422/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00422: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 423/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00423: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 424/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00424: val_loss did not improve\n",
      "\n",
      "Epoch 00424: reducing learning rate to 1.08420217249e-19.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 425/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00425: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 426/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00426: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 427/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00427: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 428/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00428: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 429/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00429: val_loss did not improve\n",
      "\n",
      "Epoch 00429: reducing learning rate to 5.42101086243e-20.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 430/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00430: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 431/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00431: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 432/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00432: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 433/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00433: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 434/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00434: val_loss did not improve\n",
      "\n",
      "Epoch 00434: reducing learning rate to 2.71050543121e-20.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 435/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00435: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 436/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00436: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 437/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00437: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 438/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00438: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 439/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00439: val_loss did not improve\n",
      "\n",
      "Epoch 00439: reducing learning rate to 1.35525271561e-20.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 440/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00440: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 441/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00441: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 442/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00442: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 443/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00443: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 444/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00444: val_loss did not improve\n",
      "\n",
      "Epoch 00444: reducing learning rate to 6.77626357803e-21.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 445/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00445: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 446/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00446: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 447/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00447: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 448/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00448: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 449/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00449: val_loss did not improve\n",
      "\n",
      "Epoch 00449: reducing learning rate to 3.38813178902e-21.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 450/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00450: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 451/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00451: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 452/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00452: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 453/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00453: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 454/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00454: val_loss did not improve\n",
      "\n",
      "Epoch 00454: reducing learning rate to 1.69406589451e-21.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 455/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00455: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 456/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00456: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 457/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00457: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 458/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00458: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00459: val_loss did not improve\n",
      "\n",
      "Epoch 00459: reducing learning rate to 8.47032947254e-22.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 460/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00460: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 461/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00461: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 462/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00462: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 463/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00463: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 464/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00464: val_loss did not improve\n",
      "\n",
      "Epoch 00464: reducing learning rate to 4.23516473627e-22.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 465/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00465: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 466/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00466: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 467/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00467: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 468/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00468: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 469/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00469: val_loss did not improve\n",
      "\n",
      "Epoch 00469: reducing learning rate to 2.11758236814e-22.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 470/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00470: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 471/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00471: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 472/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00472: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 473/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00473: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 474/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00474: val_loss did not improve\n",
      "\n",
      "Epoch 00474: reducing learning rate to 1.05879118407e-22.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 475/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00475: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 476/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00476: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 477/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00477: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 478/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00478: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 479/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00479: val_loss did not improve\n",
      "\n",
      "Epoch 00479: reducing learning rate to 5.29395592034e-23.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 480/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00480: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 481/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00481: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 482/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00482: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 483/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00483: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 484/500\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00484: val_loss did not improve\n",
      "\n",
      "Epoch 00484: reducing learning rate to 2.64697796017e-23.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 485/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00485: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 486/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00486: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 487/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00487: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 488/500\n",
      "58368/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00488: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 489/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00489: val_loss did not improve\n",
      "\n",
      "Epoch 00489: reducing learning rate to 1.32348898008e-23.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 490/500\n",
      "58624/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00490: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 491/500\n",
      "58880/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00491: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 492/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00492: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 493/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00493: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 494/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00494: val_loss did not improve\n",
      "\n",
      "Epoch 00494: reducing learning rate to 6.61744490042e-24.\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00495: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 496/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00496: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 497/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00497: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 498/500\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00498: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 499/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00499: val_loss did not improve\n",
      "\n",
      "Epoch 00499: reducing learning rate to 3.30872245021e-24.\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 500/500\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00500: val_loss did not improve\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0960 - val_loss: 0.0911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb537732d10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "cnn_model_cp_name = 'weights.best.cnn_model.hdf5'\n",
    "autoencoder.fit(x_train_cnn, x_train_cnn,\n",
    "                epochs=EPOCH,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_cnn, x_test_cnn),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'),\n",
    "                           ModelCheckpoint(cnn_model_cp_name, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "                           ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, factor=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb534db1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.68165\n"
     ]
    }
   ],
   "source": [
    "autoencoder.load_weights(cnn_model_cp_name)\n",
    "eval_autoencoder(autoencoder, x_train_cnn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHAAAAAAAgAz8f8HwVRxEiWU3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb51fe9be10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.456\n"
     ]
    }
   ],
   "source": [
    "eval_autoencoder(autoencoder, x_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"11433pt\" viewBox=\"0.00 0.00 1289.00 11433.00\" width=\"1289pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 11429)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-11429 1285,-11429 1285,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140415579353808 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140415579353808</title>\n",
       "<polygon fill=\"none\" points=\"273.5,-11388.5 273.5,-11424.5 398.5,-11424.5 398.5,-11388.5 273.5,-11388.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11402.8\">input_5: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140416252088528 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140416252088528</title>\n",
       "<polygon fill=\"none\" points=\"274.5,-11315.5 274.5,-11351.5 397.5,-11351.5 397.5,-11315.5 274.5,-11315.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11329.8\">conv2d_8: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579353808&#45;&gt;140416252088528 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140415579353808-&gt;140416252088528</title>\n",
       "<path d=\"M336,-11388.3C336,-11380.3 336,-11370.5 336,-11361.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-11361.5 336,-11351.5 332.5,-11361.5 339.5,-11361.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579353936 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140415579353936</title>\n",
       "<polygon fill=\"none\" points=\"206,-11242.5 206,-11278.5 466,-11278.5 466,-11242.5 206,-11242.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11256.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140416252088528&#45;&gt;140415579353936 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140416252088528-&gt;140415579353936</title>\n",
       "<path d=\"M336,-11315.3C336,-11307.3 336,-11297.5 336,-11288.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-11288.5 336,-11278.5 332.5,-11288.5 339.5,-11288.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415619232976 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140415619232976</title>\n",
       "<polygon fill=\"none\" points=\"262,-11169.5 262,-11205.5 410,-11205.5 410,-11169.5 262,-11169.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11183.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 140415579353936&#45;&gt;140415619232976 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140415579353936-&gt;140415619232976</title>\n",
       "<path d=\"M336,-11242.3C336,-11234.3 336,-11224.5 336,-11215.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-11215.5 336,-11205.5 332.5,-11215.5 339.5,-11215.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579854352 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140415579854352</title>\n",
       "<polygon fill=\"none\" points=\"274.5,-11096.5 274.5,-11132.5 397.5,-11132.5 397.5,-11096.5 274.5,-11096.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11110.8\">conv2d_9: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415619232976&#45;&gt;140415579854352 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140415619232976-&gt;140415579854352</title>\n",
       "<path d=\"M336,-11169.3C336,-11161.3 336,-11151.5 336,-11142.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-11142.5 336,-11132.5 332.5,-11142.5 339.5,-11142.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579856784 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140415579856784</title>\n",
       "<polygon fill=\"none\" points=\"206,-11023.5 206,-11059.5 466,-11059.5 466,-11023.5 206,-11023.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-11037.8\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415579854352&#45;&gt;140415579856784 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140415579854352-&gt;140415579856784</title>\n",
       "<path d=\"M336,-11096.3C336,-11088.3 336,-11078.5 336,-11069.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-11069.5 336,-11059.5 332.5,-11069.5 339.5,-11069.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578694352 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140415578694352</title>\n",
       "<polygon fill=\"none\" points=\"262,-10950.5 262,-10986.5 410,-10986.5 410,-10950.5 262,-10950.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10964.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 140415579856784&#45;&gt;140415578694352 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140415579856784-&gt;140415578694352</title>\n",
       "<path d=\"M336,-11023.3C336,-11015.3 336,-11005.5 336,-10996.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10996.5 336,-10986.5 332.5,-10996.5 339.5,-10996.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578656464 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140415578656464</title>\n",
       "<polygon fill=\"none\" points=\"271,-10877.5 271,-10913.5 401,-10913.5 401,-10877.5 271,-10877.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10891.8\">conv2d_10: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415578694352&#45;&gt;140415578656464 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140415578694352-&gt;140415578656464</title>\n",
       "<path d=\"M336,-10950.3C336,-10942.3 336,-10932.5 336,-10923.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10923.5 336,-10913.5 332.5,-10923.5 339.5,-10923.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578826640 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140415578826640</title>\n",
       "<polygon fill=\"none\" points=\"206,-10804.5 206,-10840.5 466,-10840.5 466,-10804.5 206,-10804.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10818.8\">batch_normalization_3: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415578656464&#45;&gt;140415578826640 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140415578656464-&gt;140415578826640</title>\n",
       "<path d=\"M336,-10877.3C336,-10869.3 336,-10859.5 336,-10850.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10850.5 336,-10840.5 332.5,-10850.5 339.5,-10850.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578349328 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140415578349328</title>\n",
       "<polygon fill=\"none\" points=\"262,-10731.5 262,-10767.5 410,-10767.5 410,-10731.5 262,-10731.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10745.8\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 140415578826640&#45;&gt;140415578349328 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140415578826640-&gt;140415578349328</title>\n",
       "<path d=\"M336,-10804.3C336,-10796.3 336,-10786.5 336,-10777.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10777.5 336,-10767.5 332.5,-10777.5 339.5,-10777.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578059088 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140415578059088</title>\n",
       "<polygon fill=\"none\" points=\"232.5,-10658.5 232.5,-10694.5 439.5,-10694.5 439.5,-10658.5 232.5,-10658.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10672.8\">max_pooling2d_4: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140415578349328&#45;&gt;140415578059088 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140415578349328-&gt;140415578059088</title>\n",
       "<path d=\"M336,-10731.3C336,-10723.3 336,-10713.5 336,-10704.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10704.5 336,-10694.5 332.5,-10704.5 339.5,-10704.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578062224 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140415578062224</title>\n",
       "<polygon fill=\"none\" points=\"271,-10585.5 271,-10621.5 401,-10621.5 401,-10585.5 271,-10585.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10599.8\">conv2d_11: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415578059088&#45;&gt;140415578062224 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140415578059088-&gt;140415578062224</title>\n",
       "<path d=\"M336,-10658.3C336,-10650.3 336,-10640.5 336,-10631.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10631.5 336,-10621.5 332.5,-10631.5 339.5,-10631.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578113488 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140415578113488</title>\n",
       "<polygon fill=\"none\" points=\"206,-10512.5 206,-10548.5 466,-10548.5 466,-10512.5 206,-10512.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10526.8\">batch_normalization_4: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415578062224&#45;&gt;140415578113488 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140415578062224-&gt;140415578113488</title>\n",
       "<path d=\"M336,-10585.3C336,-10577.3 336,-10567.5 336,-10558.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10558.5 336,-10548.5 332.5,-10558.5 339.5,-10558.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577953296 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140415577953296</title>\n",
       "<polygon fill=\"none\" points=\"262,-10439.5 262,-10475.5 410,-10475.5 410,-10439.5 262,-10439.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10453.8\">activation_4: Activation</text>\n",
       "</g>\n",
       "<!-- 140415578113488&#45;&gt;140415577953296 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140415578113488-&gt;140415577953296</title>\n",
       "<path d=\"M336,-10512.3C336,-10504.3 336,-10494.5 336,-10485.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10485.5 336,-10475.5 332.5,-10485.5 339.5,-10485.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415578264720 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140415578264720</title>\n",
       "<polygon fill=\"none\" points=\"271,-10366.5 271,-10402.5 401,-10402.5 401,-10366.5 271,-10366.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10380.8\">conv2d_12: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415577953296&#45;&gt;140415578264720 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140415577953296-&gt;140415578264720</title>\n",
       "<path d=\"M336,-10439.3C336,-10431.3 336,-10421.5 336,-10412.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10412.5 336,-10402.5 332.5,-10412.5 339.5,-10412.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577910608 -->\n",
       "<g class=\"node\" id=\"node16\"><title>140415577910608</title>\n",
       "<polygon fill=\"none\" points=\"206,-10293.5 206,-10329.5 466,-10329.5 466,-10293.5 206,-10293.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10307.8\">batch_normalization_5: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415578264720&#45;&gt;140415577910608 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140415578264720-&gt;140415577910608</title>\n",
       "<path d=\"M336,-10366.3C336,-10358.3 336,-10348.5 336,-10339.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10339.5 336,-10329.5 332.5,-10339.5 339.5,-10339.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577653136 -->\n",
       "<g class=\"node\" id=\"node17\"><title>140415577653136</title>\n",
       "<polygon fill=\"none\" points=\"262,-10220.5 262,-10256.5 410,-10256.5 410,-10220.5 262,-10220.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10234.8\">activation_5: Activation</text>\n",
       "</g>\n",
       "<!-- 140415577910608&#45;&gt;140415577653136 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140415577910608-&gt;140415577653136</title>\n",
       "<path d=\"M336,-10293.3C336,-10285.3 336,-10275.5 336,-10266.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10266.5 336,-10256.5 332.5,-10266.5 339.5,-10266.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577362896 -->\n",
       "<g class=\"node\" id=\"node18\"><title>140415577362896</title>\n",
       "<polygon fill=\"none\" points=\"232.5,-10147.5 232.5,-10183.5 439.5,-10183.5 439.5,-10147.5 232.5,-10147.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10161.8\">max_pooling2d_5: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140415577653136&#45;&gt;140415577362896 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140415577653136-&gt;140415577362896</title>\n",
       "<path d=\"M336,-10220.3C336,-10212.3 336,-10202.5 336,-10193.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10193.5 336,-10183.5 332.5,-10193.5 339.5,-10193.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580048592 -->\n",
       "<g class=\"node\" id=\"node19\"><title>140415580048592</title>\n",
       "<polygon fill=\"none\" points=\"86,-10074.5 86,-10110.5 216,-10110.5 216,-10074.5 86,-10074.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-10088.8\">conv2d_16: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415577362896&#45;&gt;140415580048592 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140415577362896-&gt;140415580048592</title>\n",
       "<path d=\"M291.688,-10147.5C265.679,-10137.5 232.618,-10124.8 205.081,-10114.3\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"206.182,-10110.9 195.592,-10110.6 203.674,-10117.5 206.182,-10110.9\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415901197008 -->\n",
       "<g class=\"node\" id=\"node22\"><title>140415901197008</title>\n",
       "<polygon fill=\"none\" points=\"271,-10074.5 271,-10110.5 401,-10110.5 401,-10074.5 271,-10074.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-10088.8\">conv2d_14: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415577362896&#45;&gt;140415901197008 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>140415577362896-&gt;140415901197008</title>\n",
       "<path d=\"M336,-10147.3C336,-10139.3 336,-10129.5 336,-10120.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-10120.5 336,-10110.5 332.5,-10120.5 339.5,-10120.5\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630619216 -->\n",
       "<g class=\"node\" id=\"node28\"><title>140415630619216</title>\n",
       "<polygon fill=\"none\" points=\"381.5,-10001.5 381.5,-10037.5 626.5,-10037.5 626.5,-10001.5 381.5,-10001.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504\" y=\"-10015.8\">average_pooling2d_1: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415577362896&#45;&gt;140415630619216 -->\n",
       "<g class=\"edge\" id=\"edge27\"><title>140415577362896-&gt;140415630619216</title>\n",
       "<path d=\"M439.746,-10150.6C507.185,-10139.2 586.292,-10120.5 607,-10093.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M607,-10091.5C622.963,-10070.7 600.906,-10053.7 573.166,-10041.6\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"574.168,-10038.2 563.587,-10037.6 571.513,-10044.7 574.168,-10038.2\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577366032 -->\n",
       "<g class=\"node\" id=\"node29\"><title>140415577366032</title>\n",
       "<polygon fill=\"none\" points=\"645,-10001.5 645,-10037.5 775,-10037.5 775,-10001.5 645,-10001.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"710\" y=\"-10015.8\">conv2d_13: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415577362896&#45;&gt;140415577366032 -->\n",
       "<g class=\"edge\" id=\"edge28\"><title>140415577362896-&gt;140415577366032</title>\n",
       "<path d=\"M607,-10091.5C622.42,-10071.4 645.192,-10054.8 665.405,-10042.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"667.254,-10045.8 674.179,-10037.8 663.765,-10039.7 667.254,-10045.8\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580789136 -->\n",
       "<g class=\"node\" id=\"node20\"><title>140415580789136</title>\n",
       "<polygon fill=\"none\" points=\"2,-10001.5 2,-10037.5 262,-10037.5 262,-10001.5 2,-10001.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132\" y=\"-10015.8\">batch_normalization_9: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415580048592&#45;&gt;140415580789136 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>140415580048592-&gt;140415580789136</title>\n",
       "<path d=\"M146.401,-10074.3C144.23,-10066.2 141.59,-10056.3 139.166,-10047.3\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"142.523,-10046.3 136.557,-10037.5 135.761,-10048.1 142.523,-10046.3\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579184784 -->\n",
       "<g class=\"node\" id=\"node21\"><title>140415579184784</title>\n",
       "<polygon fill=\"none\" points=\"40,-9928.5 40,-9964.5 188,-9964.5 188,-9928.5 40,-9928.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"114\" y=\"-9942.8\">activation_9: Activation</text>\n",
       "</g>\n",
       "<!-- 140415580789136&#45;&gt;140415579184784 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>140415580789136-&gt;140415579184784</title>\n",
       "<path d=\"M127.643,-10001.3C125.586,-9993.2 123.085,-9983.34 120.788,-9974.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"124.167,-9973.36 118.317,-9964.53 117.382,-9975.08 124.167,-9973.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373195472 -->\n",
       "<g class=\"node\" id=\"node23\"><title>140416373195472</title>\n",
       "<polygon fill=\"none\" points=\"51,-9855.5 51,-9891.5 181,-9891.5 181,-9855.5 51,-9855.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116\" y=\"-9869.8\">conv2d_17: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579184784&#45;&gt;140416373195472 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>140415579184784-&gt;140416373195472</title>\n",
       "<path d=\"M114.484,-9928.31C114.71,-9920.29 114.985,-9910.55 115.237,-9901.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"118.737,-9901.62 115.52,-9891.53 111.74,-9901.43 118.737,-9901.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416296102992 -->\n",
       "<g class=\"node\" id=\"node24\"><title>140416296102992</title>\n",
       "<polygon fill=\"none\" points=\"206,-9928.5 206,-9964.5 466,-9964.5 466,-9928.5 206,-9928.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336\" y=\"-9942.8\">batch_normalization_7: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415901197008&#45;&gt;140416296102992 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>140415901197008-&gt;140416296102992</title>\n",
       "<path d=\"M336,-10074.4C336,-10049.8 336,-10004.3 336,-9974.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"339.5,-9974.56 336,-9964.56 332.5,-9974.56 339.5,-9974.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373197136 -->\n",
       "<g class=\"node\" id=\"node25\"><title>140416373197136</title>\n",
       "<polygon fill=\"none\" points=\"0,-9782.5 0,-9818.5 266,-9818.5 266,-9782.5 0,-9782.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-9796.8\">batch_normalization_10: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140416373195472&#45;&gt;140416373197136 -->\n",
       "<g class=\"edge\" id=\"edge24\"><title>140416373195472-&gt;140416373197136</title>\n",
       "<path d=\"M120.115,-9855.31C122.057,-9847.2 124.42,-9837.34 126.589,-9828.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"129.998,-9829.07 128.923,-9818.53 123.19,-9827.44 129.998,-9829.07\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580114768 -->\n",
       "<g class=\"node\" id=\"node26\"><title>140415580114768</title>\n",
       "<polygon fill=\"none\" points=\"250,-9855.5 250,-9891.5 398,-9891.5 398,-9855.5 250,-9855.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-9869.8\">activation_7: Activation</text>\n",
       "</g>\n",
       "<!-- 140416296102992&#45;&gt;140415580114768 -->\n",
       "<g class=\"edge\" id=\"edge25\"><title>140416296102992-&gt;140415580114768</title>\n",
       "<path d=\"M333.095,-9928.31C331.739,-9920.29 330.093,-9910.55 328.575,-9901.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"331.996,-9900.81 326.878,-9891.53 325.094,-9901.97 331.996,-9900.81\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579549456 -->\n",
       "<g class=\"node\" id=\"node27\"><title>140415579549456</title>\n",
       "<polygon fill=\"none\" points=\"59.5,-9709.5 59.5,-9745.5 214.5,-9745.5 214.5,-9709.5 59.5,-9709.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"137\" y=\"-9723.8\">activation_10: Activation</text>\n",
       "</g>\n",
       "<!-- 140416373197136&#45;&gt;140415579549456 -->\n",
       "<g class=\"edge\" id=\"edge26\"><title>140416373197136-&gt;140415579549456</title>\n",
       "<path d=\"M133.968,-9782.31C134.42,-9774.29 134.969,-9764.55 135.475,-9755.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"138.973,-9755.71 136.041,-9745.53 131.984,-9755.32 138.973,-9755.71\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579853264 -->\n",
       "<g class=\"node\" id=\"node30\"><title>140415579853264</title>\n",
       "<polygon fill=\"none\" points=\"295,-9782.5 295,-9818.5 425,-9818.5 425,-9782.5 295,-9782.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-9796.8\">conv2d_15: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415580114768&#45;&gt;140415579853264 -->\n",
       "<g class=\"edge\" id=\"edge29\"><title>140415580114768-&gt;140415579853264</title>\n",
       "<path d=\"M332.715,-9855.31C336.915,-9847.03 342.045,-9836.91 346.719,-9827.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"349.965,-9829.03 351.366,-9818.53 343.722,-9825.87 349.965,-9829.03\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577153168 -->\n",
       "<g class=\"node\" id=\"node31\"><title>140415577153168</title>\n",
       "<polygon fill=\"none\" points=\"74,-9636.5 74,-9672.5 204,-9672.5 204,-9636.5 74,-9636.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139\" y=\"-9650.8\">conv2d_18: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579549456&#45;&gt;140415577153168 -->\n",
       "<g class=\"edge\" id=\"edge30\"><title>140415579549456-&gt;140415577153168</title>\n",
       "<path d=\"M137.484,-9709.31C137.71,-9701.29 137.985,-9691.55 138.237,-9682.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"141.737,-9682.62 138.52,-9672.53 134.74,-9682.43 141.737,-9682.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630621456 -->\n",
       "<g class=\"node\" id=\"node32\"><title>140415630621456</title>\n",
       "<polygon fill=\"none\" points=\"484,-9928.5 484,-9964.5 614,-9964.5 614,-9928.5 484,-9928.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549\" y=\"-9942.8\">conv2d_19: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415630619216&#45;&gt;140415630621456 -->\n",
       "<g class=\"edge\" id=\"edge31\"><title>140415630619216-&gt;140415630621456</title>\n",
       "<path d=\"M514.893,-10001.3C520.255,-9992.85 526.827,-9982.48 532.767,-9973.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"535.81,-9974.85 538.207,-9964.53 529.897,-9971.1 535.81,-9974.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415577421008 -->\n",
       "<g class=\"node\" id=\"node33\"><title>140415577421008</title>\n",
       "<polygon fill=\"none\" points=\"574,-9782.5 574,-9818.5 834,-9818.5 834,-9782.5 574,-9782.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704\" y=\"-9796.8\">batch_normalization_6: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415577366032&#45;&gt;140415577421008 -->\n",
       "<g class=\"edge\" id=\"edge32\"><title>140415577366032-&gt;140415577421008</title>\n",
       "<path d=\"M709.529,-10001.5C708.493,-9964.01 706.009,-9874.18 704.754,-9828.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"708.252,-9828.65 704.477,-9818.75 701.255,-9828.84 708.252,-9828.65\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579438544 -->\n",
       "<g class=\"node\" id=\"node34\"><title>140415579438544</title>\n",
       "<polygon fill=\"none\" points=\"235,-9709.5 235,-9745.5 495,-9745.5 495,-9709.5 235,-9709.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"365\" y=\"-9723.8\">batch_normalization_8: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415579853264&#45;&gt;140415579438544 -->\n",
       "<g class=\"edge\" id=\"edge33\"><title>140415579853264-&gt;140415579438544</title>\n",
       "<path d=\"M361.21,-9782.31C361.775,-9774.29 362.461,-9764.55 363.094,-9755.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"366.59,-9755.75 363.801,-9745.53 359.607,-9755.26 366.59,-9755.75\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415619188752 -->\n",
       "<g class=\"node\" id=\"node35\"><title>140415619188752</title>\n",
       "<polygon fill=\"none\" points=\"8,-9563.5 8,-9599.5 274,-9599.5 274,-9563.5 8,-9563.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-9577.8\">batch_normalization_11: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415577153168&#45;&gt;140415619188752 -->\n",
       "<g class=\"edge\" id=\"edge34\"><title>140415577153168-&gt;140415619188752</title>\n",
       "<path d=\"M139.484,-9636.31C139.71,-9628.29 139.985,-9618.55 140.237,-9609.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"143.737,-9609.62 140.52,-9599.53 136.74,-9609.43 143.737,-9609.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630621584 -->\n",
       "<g class=\"node\" id=\"node36\"><title>140415630621584</title>\n",
       "<polygon fill=\"none\" points=\"416,-9855.5 416,-9891.5 682,-9891.5 682,-9855.5 416,-9855.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549\" y=\"-9869.8\">batch_normalization_12: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415630621456&#45;&gt;140415630621584 -->\n",
       "<g class=\"edge\" id=\"edge35\"><title>140415630621456-&gt;140415630621584</title>\n",
       "<path d=\"M549,-9928.31C549,-9920.29 549,-9910.55 549,-9901.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"552.5,-9901.53 549,-9891.53 545.5,-9901.53 552.5,-9901.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373675920 -->\n",
       "<g class=\"node\" id=\"node37\"><title>140416373675920</title>\n",
       "<polygon fill=\"none\" points=\"583,-9636.5 583,-9672.5 731,-9672.5 731,-9636.5 583,-9636.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"657\" y=\"-9650.8\">activation_6: Activation</text>\n",
       "</g>\n",
       "<!-- 140415577421008&#45;&gt;140416373675920 -->\n",
       "<g class=\"edge\" id=\"edge36\"><title>140415577421008-&gt;140416373675920</title>\n",
       "<path d=\"M698.424,-9782.42C690.332,-9757.62 675.261,-9711.45 665.7,-9682.16\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"668.999,-9680.98 662.569,-9672.56 662.344,-9683.15 668.999,-9680.98\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581022416 -->\n",
       "<g class=\"node\" id=\"node38\"><title>140415581022416</title>\n",
       "<polygon fill=\"none\" points=\"293,-9563.5 293,-9599.5 441,-9599.5 441,-9563.5 293,-9563.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367\" y=\"-9577.8\">activation_8: Activation</text>\n",
       "</g>\n",
       "<!-- 140415579438544&#45;&gt;140415581022416 -->\n",
       "<g class=\"edge\" id=\"edge37\"><title>140415579438544-&gt;140415581022416</title>\n",
       "<path d=\"M365.237,-9709.42C365.579,-9684.84 366.212,-9639.25 366.619,-9609.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"370.124,-9609.61 366.763,-9599.56 363.124,-9609.51 370.124,-9609.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630573136 -->\n",
       "<g class=\"node\" id=\"node39\"><title>140415630573136</title>\n",
       "<polygon fill=\"none\" points=\"124.5,-9490.5 124.5,-9526.5 279.5,-9526.5 279.5,-9490.5 124.5,-9490.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-9504.8\">activation_11: Activation</text>\n",
       "</g>\n",
       "<!-- 140415619188752&#45;&gt;140415630573136 -->\n",
       "<g class=\"edge\" id=\"edge38\"><title>140415619188752-&gt;140415630573136</title>\n",
       "<path d=\"M155.766,-9563.31C163.259,-9554.59 172.496,-9543.84 180.739,-9534.25\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"183.508,-9536.39 187.37,-9526.53 178.198,-9531.83 183.508,-9536.39\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415629976592 -->\n",
       "<g class=\"node\" id=\"node40\"><title>140415629976592</title>\n",
       "<polygon fill=\"none\" points=\"401.5,-9490.5 401.5,-9526.5 556.5,-9526.5 556.5,-9490.5 401.5,-9490.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479\" y=\"-9504.8\">activation_12: Activation</text>\n",
       "</g>\n",
       "<!-- 140415630621584&#45;&gt;140415629976592 -->\n",
       "<g class=\"edge\" id=\"edge39\"><title>140415630621584-&gt;140415629976592</title>\n",
       "<path d=\"M547.222,-9855.23C544.599,-9828.23 540,-9774.34 540,-9728.5 540,-9728.5 540,-9728.5 540,-9653.5 540,-9609.16 515.171,-9562.78 497.195,-9535.02\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"500.098,-9533.06 491.639,-9526.68 494.274,-9536.95 500.098,-9533.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579959696 -->\n",
       "<g class=\"node\" id=\"node41\"><title>140415579959696</title>\n",
       "<polygon fill=\"none\" points=\"357,-9417.5 357,-9453.5 489,-9453.5 489,-9417.5 357,-9417.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-9431.8\">mixed0: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140416373675920&#45;&gt;140415579959696 -->\n",
       "<g class=\"edge\" id=\"edge40\"><title>140416373675920-&gt;140415579959696</title>\n",
       "<path d=\"M651.778,-9636.36C640.913,-9603.45 612.824,-9531.05 565,-9490 546.196,-9473.86 522.04,-9462.36 499.003,-9454.27\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"499.811,-9450.85 489.217,-9451 497.596,-9457.49 499.811,-9450.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581022416&#45;&gt;140415579959696 -->\n",
       "<g class=\"edge\" id=\"edge41\"><title>140415581022416-&gt;140415579959696</title>\n",
       "<path d=\"M370.358,-9563.39C374.305,-9544.86 381.655,-9514.65 392,-9490 395.905,-9480.7 401.122,-9471.01 406.161,-9462.48\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"409.267,-9464.11 411.488,-9453.75 403.292,-9460.46 409.267,-9464.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630573136&#45;&gt;140415579959696 -->\n",
       "<g class=\"edge\" id=\"edge42\"><title>140415630573136-&gt;140415579959696</title>\n",
       "<path d=\"M254.935,-9490.49C286.548,-9480.34 326.882,-9467.38 360.113,-9456.7\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"361.281,-9460 369.731,-9453.61 359.14,-9453.34 361.281,-9460\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415629976592&#45;&gt;140415579959696 -->\n",
       "<g class=\"edge\" id=\"edge43\"><title>140415629976592-&gt;140415579959696</title>\n",
       "<path d=\"M465.444,-9490.31C458.634,-9481.68 450.255,-9471.06 442.745,-9461.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"445.372,-9459.21 436.431,-9453.53 439.876,-9463.55 445.372,-9459.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618769616 -->\n",
       "<g class=\"node\" id=\"node42\"><title>140415618769616</title>\n",
       "<polygon fill=\"none\" points=\"170,-9344.5 170,-9380.5 300,-9380.5 300,-9344.5 170,-9344.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-9358.8\">conv2d_23: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579959696&#45;&gt;140415618769616 -->\n",
       "<g class=\"edge\" id=\"edge44\"><title>140415579959696-&gt;140415618769616</title>\n",
       "<path d=\"M377.969,-9417.49C351.538,-9407.51 317.941,-9394.82 289.958,-9384.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"290.907,-9380.87 280.315,-9380.61 288.433,-9387.42 290.907,-9380.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416507359120 -->\n",
       "<g class=\"node\" id=\"node45\"><title>140416507359120</title>\n",
       "<polygon fill=\"none\" points=\"358,-9344.5 358,-9380.5 488,-9380.5 488,-9344.5 358,-9344.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-9358.8\">conv2d_21: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579959696&#45;&gt;140416507359120 -->\n",
       "<g class=\"edge\" id=\"edge47\"><title>140415579959696-&gt;140416507359120</title>\n",
       "<path d=\"M423,-9417.31C423,-9409.29 423,-9399.55 423,-9390.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"426.5,-9390.53 423,-9380.53 419.5,-9390.53 426.5,-9390.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580395088 -->\n",
       "<g class=\"node\" id=\"node51\"><title>140415580395088</title>\n",
       "<polygon fill=\"none\" points=\"471.5,-9271.5 471.5,-9307.5 716.5,-9307.5 716.5,-9271.5 471.5,-9271.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"594\" y=\"-9285.8\">average_pooling2d_2: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415579959696&#45;&gt;140415580395088 -->\n",
       "<g class=\"edge\" id=\"edge53\"><title>140415579959696-&gt;140415580395088</title>\n",
       "<path d=\"M489.281,-9426.66C562.034,-9416.52 671.886,-9396.18 697,-9363.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M697,-9361.5C713.121,-9340.52 690.578,-9323.43 662.516,-9311.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"663.835,-9308.04 653.253,-9307.51 661.196,-9314.52 663.835,-9308.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579963280 -->\n",
       "<g class=\"node\" id=\"node52\"><title>140415579963280</title>\n",
       "<polygon fill=\"none\" points=\"735,-9271.5 735,-9307.5 865,-9307.5 865,-9271.5 735,-9271.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"800\" y=\"-9285.8\">conv2d_20: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579959696&#45;&gt;140415579963280 -->\n",
       "<g class=\"edge\" id=\"edge54\"><title>140415579959696-&gt;140415579963280</title>\n",
       "<path d=\"M697,-9361.5C712.563,-9341.25 735.562,-9324.62 755.882,-9312.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"757.759,-9315.52 764.694,-9307.51 754.278,-9309.45 757.759,-9315.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618771856 -->\n",
       "<g class=\"node\" id=\"node43\"><title>140415618771856</title>\n",
       "<polygon fill=\"none\" points=\"81,-9271.5 81,-9307.5 347,-9307.5 347,-9271.5 81,-9271.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214\" y=\"-9285.8\">batch_normalization_16: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415618769616&#45;&gt;140415618771856 -->\n",
       "<g class=\"edge\" id=\"edge45\"><title>140415618769616-&gt;140415618771856</title>\n",
       "<path d=\"M229.916,-9344.31C227.518,-9336.2 224.599,-9326.34 221.92,-9317.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"225.229,-9316.13 219.037,-9307.53 218.517,-9318.11 225.229,-9316.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140417514057232 -->\n",
       "<g class=\"node\" id=\"node44\"><title>140417514057232</title>\n",
       "<polygon fill=\"none\" points=\"116.5,-9198.5 116.5,-9234.5 271.5,-9234.5 271.5,-9198.5 116.5,-9198.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-9212.8\">activation_16: Activation</text>\n",
       "</g>\n",
       "<!-- 140415618771856&#45;&gt;140417514057232 -->\n",
       "<g class=\"edge\" id=\"edge46\"><title>140415618771856-&gt;140417514057232</title>\n",
       "<path d=\"M209.159,-9271.31C206.874,-9263.2 204.095,-9253.34 201.543,-9244.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.877,-9243.21 198.797,-9234.53 198.139,-9245.1 204.877,-9243.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416507112848 -->\n",
       "<g class=\"node\" id=\"node46\"><title>140416507112848</title>\n",
       "<polygon fill=\"none\" points=\"150,-9125.5 150,-9161.5 280,-9161.5 280,-9125.5 150,-9125.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-9139.8\">conv2d_24: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140417514057232&#45;&gt;140416507112848 -->\n",
       "<g class=\"edge\" id=\"edge48\"><title>140417514057232-&gt;140416507112848</title>\n",
       "<path d=\"M199.084,-9198.31C201.482,-9190.2 204.401,-9180.34 207.08,-9171.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"210.483,-9172.11 209.963,-9161.53 203.771,-9170.13 210.483,-9172.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416507498832 -->\n",
       "<g class=\"node\" id=\"node47\"><title>140416507498832</title>\n",
       "<polygon fill=\"none\" points=\"290,-9198.5 290,-9234.5 556,-9234.5 556,-9198.5 290,-9198.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-9212.8\">batch_normalization_14: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140416507359120&#45;&gt;140416507498832 -->\n",
       "<g class=\"edge\" id=\"edge49\"><title>140416507359120-&gt;140416507498832</title>\n",
       "<path d=\"M423,-9344.42C423,-9319.84 423,-9274.25 423,-9244.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"426.5,-9244.56 423,-9234.56 419.5,-9244.56 426.5,-9244.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416507109904 -->\n",
       "<g class=\"node\" id=\"node48\"><title>140416507109904</title>\n",
       "<polygon fill=\"none\" points=\"92,-9052.5 92,-9088.5 358,-9088.5 358,-9052.5 92,-9052.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225\" y=\"-9066.8\">batch_normalization_17: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140416507112848&#45;&gt;140416507109904 -->\n",
       "<g class=\"edge\" id=\"edge50\"><title>140416507112848-&gt;140416507109904</title>\n",
       "<path d=\"M217.421,-9125.31C218.551,-9117.29 219.923,-9107.55 221.187,-9098.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.673,-9098.92 222.602,-9088.53 217.741,-9097.94 224.673,-9098.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579791248 -->\n",
       "<g class=\"node\" id=\"node49\"><title>140415579791248</title>\n",
       "<polygon fill=\"none\" points=\"332.5,-9125.5 332.5,-9161.5 487.5,-9161.5 487.5,-9125.5 332.5,-9125.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410\" y=\"-9139.8\">activation_14: Activation</text>\n",
       "</g>\n",
       "<!-- 140416507498832&#45;&gt;140415579791248 -->\n",
       "<g class=\"edge\" id=\"edge51\"><title>140416507498832-&gt;140415579791248</title>\n",
       "<path d=\"M419.853,-9198.31C418.384,-9190.29 416.6,-9180.55 414.956,-9171.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"418.362,-9170.73 413.118,-9161.53 411.476,-9172 418.362,-9170.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618978064 -->\n",
       "<g class=\"node\" id=\"node50\"><title>140415618978064</title>\n",
       "<polygon fill=\"none\" points=\"150.5,-8979.5 150.5,-9015.5 305.5,-9015.5 305.5,-8979.5 150.5,-8979.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228\" y=\"-8993.8\">activation_17: Activation</text>\n",
       "</g>\n",
       "<!-- 140416507109904&#45;&gt;140415618978064 -->\n",
       "<g class=\"edge\" id=\"edge52\"><title>140416507109904-&gt;140415618978064</title>\n",
       "<path d=\"M225.726,-9052.31C226.065,-9044.29 226.477,-9034.55 226.856,-9025.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"230.355,-9025.67 227.28,-9015.53 223.361,-9025.37 230.355,-9025.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579695376 -->\n",
       "<g class=\"node\" id=\"node53\"><title>140415579695376</title>\n",
       "<polygon fill=\"none\" points=\"388,-9052.5 388,-9088.5 518,-9088.5 518,-9052.5 388,-9052.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453\" y=\"-9066.8\">conv2d_22: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415579791248&#45;&gt;140415579695376 -->\n",
       "<g class=\"edge\" id=\"edge55\"><title>140415579791248-&gt;140415579695376</title>\n",
       "<path d=\"M420.409,-9125.31C425.532,-9116.85 431.813,-9106.48 437.488,-9097.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"440.5,-9098.9 442.687,-9088.53 434.513,-9095.27 440.5,-9098.9\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415619120464 -->\n",
       "<g class=\"node\" id=\"node54\"><title>140415619120464</title>\n",
       "<polygon fill=\"none\" points=\"164,-8906.5 164,-8942.5 294,-8942.5 294,-8906.5 164,-8906.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"229\" y=\"-8920.8\">conv2d_25: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415618978064&#45;&gt;140415619120464 -->\n",
       "<g class=\"edge\" id=\"edge56\"><title>140415618978064-&gt;140415619120464</title>\n",
       "<path d=\"M228.242,-8979.31C228.355,-8971.29 228.492,-8961.55 228.619,-8952.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"232.119,-8952.58 228.76,-8942.53 225.12,-8952.48 232.119,-8952.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580393552 -->\n",
       "<g class=\"node\" id=\"node55\"><title>140415580393552</title>\n",
       "<polygon fill=\"none\" points=\"574,-9198.5 574,-9234.5 704,-9234.5 704,-9198.5 574,-9198.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"639\" y=\"-9212.8\">conv2d_26: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415580395088&#45;&gt;140415580393552 -->\n",
       "<g class=\"edge\" id=\"edge57\"><title>140415580395088-&gt;140415580393552</title>\n",
       "<path d=\"M604.893,-9271.31C610.255,-9262.85 616.827,-9252.48 622.767,-9243.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"625.81,-9244.85 628.207,-9234.53 619.897,-9241.1 625.81,-9244.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579961744 -->\n",
       "<g class=\"node\" id=\"node56\"><title>140415579961744</title>\n",
       "<polygon fill=\"none\" points=\"666,-9052.5 666,-9088.5 932,-9088.5 932,-9052.5 666,-9052.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"799\" y=\"-9066.8\">batch_normalization_13: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415579963280&#45;&gt;140415579961744 -->\n",
       "<g class=\"edge\" id=\"edge58\"><title>140415579963280-&gt;140415579961744</title>\n",
       "<path d=\"M799.922,-9271.47C799.749,-9234.01 799.335,-9144.18 799.126,-9098.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"802.626,-9098.73 799.079,-9088.75 795.626,-9098.76 802.626,-9098.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415579695248 -->\n",
       "<g class=\"node\" id=\"node57\"><title>140415579695248</title>\n",
       "<polygon fill=\"none\" points=\"326,-8979.5 326,-9015.5 592,-9015.5 592,-8979.5 326,-8979.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-8993.8\">batch_normalization_15: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415579695376&#45;&gt;140415579695248 -->\n",
       "<g class=\"edge\" id=\"edge59\"><title>140415579695376-&gt;140415579695248</title>\n",
       "<path d=\"M454.452,-9052.31C455.13,-9044.29 455.954,-9034.55 456.712,-9025.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"460.206,-9025.79 457.561,-9015.53 453.231,-9025.2 460.206,-9025.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580437520 -->\n",
       "<g class=\"node\" id=\"node58\"><title>140415580437520</title>\n",
       "<polygon fill=\"none\" points=\"97,-8833.5 97,-8869.5 363,-8869.5 363,-8833.5 97,-8833.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230\" y=\"-8847.8\">batch_normalization_18: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415619120464&#45;&gt;140415580437520 -->\n",
       "<g class=\"edge\" id=\"edge60\"><title>140415619120464-&gt;140415580437520</title>\n",
       "<path d=\"M229.242,-8906.31C229.355,-8898.29 229.492,-8888.55 229.619,-8879.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"233.119,-8879.58 229.76,-8869.53 226.12,-8879.48 233.119,-8879.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618429712 -->\n",
       "<g class=\"node\" id=\"node59\"><title>140415618429712</title>\n",
       "<polygon fill=\"none\" points=\"506,-9125.5 506,-9161.5 772,-9161.5 772,-9125.5 506,-9125.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"639\" y=\"-9139.8\">batch_normalization_19: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415580393552&#45;&gt;140415618429712 -->\n",
       "<g class=\"edge\" id=\"edge61\"><title>140415580393552-&gt;140415618429712</title>\n",
       "<path d=\"M639,-9198.31C639,-9190.29 639,-9180.55 639,-9171.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"642.5,-9171.53 639,-9161.53 635.5,-9171.53 642.5,-9171.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415629981392 -->\n",
       "<g class=\"node\" id=\"node60\"><title>140415629981392</title>\n",
       "<polygon fill=\"none\" points=\"676.5,-8906.5 676.5,-8942.5 831.5,-8942.5 831.5,-8906.5 676.5,-8906.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"754\" y=\"-8920.8\">activation_13: Activation</text>\n",
       "</g>\n",
       "<!-- 140415579961744&#45;&gt;140415629981392 -->\n",
       "<g class=\"edge\" id=\"edge62\"><title>140415579961744-&gt;140415629981392</title>\n",
       "<path d=\"M793.661,-9052.42C785.914,-9027.62 771.484,-8981.45 762.33,-8952.16\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"765.655,-8951.06 759.332,-8942.56 758.974,-8953.15 765.655,-8951.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618883280 -->\n",
       "<g class=\"node\" id=\"node61\"><title>140415618883280</title>\n",
       "<polygon fill=\"none\" points=\"381.5,-8833.5 381.5,-8869.5 536.5,-8869.5 536.5,-8833.5 381.5,-8833.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-8847.8\">activation_15: Activation</text>\n",
       "</g>\n",
       "<!-- 140415579695248&#45;&gt;140415618883280 -->\n",
       "<g class=\"edge\" id=\"edge63\"><title>140415579695248-&gt;140415618883280</title>\n",
       "<path d=\"M459,-8979.42C459,-8954.84 459,-8909.25 459,-8879.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"462.5,-8879.56 459,-8869.56 455.5,-8879.56 462.5,-8879.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618434384 -->\n",
       "<g class=\"node\" id=\"node62\"><title>140415618434384</title>\n",
       "<polygon fill=\"none\" points=\"215.5,-8760.5 215.5,-8796.5 370.5,-8796.5 370.5,-8760.5 215.5,-8760.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-8774.8\">activation_18: Activation</text>\n",
       "</g>\n",
       "<!-- 140415580437520&#45;&gt;140415618434384 -->\n",
       "<g class=\"edge\" id=\"edge64\"><title>140415580437520-&gt;140415618434384</title>\n",
       "<path d=\"M245.251,-8833.31C252.989,-8824.59 262.529,-8813.84 271.042,-8804.25\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"273.871,-8806.33 277.89,-8796.53 268.635,-8801.69 273.871,-8806.33\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415628182992 -->\n",
       "<g class=\"node\" id=\"node63\"><title>140415628182992</title>\n",
       "<polygon fill=\"none\" points=\"491.5,-8760.5 491.5,-8796.5 646.5,-8796.5 646.5,-8760.5 491.5,-8760.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569\" y=\"-8774.8\">activation_19: Activation</text>\n",
       "</g>\n",
       "<!-- 140415618429712&#45;&gt;140415628182992 -->\n",
       "<g class=\"edge\" id=\"edge65\"><title>140415618429712-&gt;140415628182992</title>\n",
       "<path d=\"M637.815,-9125.22C636.066,-9098.2 633,-9044.29 633,-8998.5 633,-8998.5 633,-8998.5 633,-8923.5 633,-8878.82 606.95,-8832.54 588.09,-8804.88\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"590.873,-8802.76 582.261,-8796.59 585.145,-8806.78 590.873,-8802.76\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415628327696 -->\n",
       "<g class=\"node\" id=\"node64\"><title>140415628327696</title>\n",
       "<polygon fill=\"none\" points=\"448,-8687.5 448,-8723.5 580,-8723.5 580,-8687.5 448,-8687.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"514\" y=\"-8701.8\">mixed1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415629981392&#45;&gt;140415628327696 -->\n",
       "<g class=\"edge\" id=\"edge66\"><title>140415629981392-&gt;140415628327696</title>\n",
       "<path d=\"M748.001,-8906.37C735.652,-8873.48 704.337,-8801.09 655,-8760 636.253,-8744.39 612.471,-8733.05 589.803,-8724.95\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"590.77,-8721.58 580.175,-8721.67 588.514,-8728.2 590.77,-8721.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618883280&#45;&gt;140415628327696 -->\n",
       "<g class=\"edge\" id=\"edge67\"><title>140415618883280-&gt;140415628327696</title>\n",
       "<path d=\"M461.822,-8833.35C465.226,-8814.79 471.795,-8784.55 482,-8760 485.905,-8750.61 491.257,-8740.9 496.466,-8732.37\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"499.593,-8733.98 501.988,-8723.66 493.68,-8730.23 499.593,-8733.98\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415618434384&#45;&gt;140415628327696 -->\n",
       "<g class=\"edge\" id=\"edge68\"><title>140415618434384-&gt;140415628327696</title>\n",
       "<path d=\"M345.935,-8760.49C377.548,-8750.34 417.882,-8737.38 451.113,-8726.7\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"452.281,-8730 460.731,-8723.61 450.14,-8723.34 452.281,-8730\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415628182992&#45;&gt;140415628327696 -->\n",
       "<g class=\"edge\" id=\"edge69\"><title>140415628182992-&gt;140415628327696</title>\n",
       "<path d=\"M555.686,-8760.31C548.998,-8751.68 540.768,-8741.06 533.393,-8731.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"536.082,-8729.29 527.191,-8723.53 530.548,-8733.58 536.082,-8729.29\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373225104 -->\n",
       "<g class=\"node\" id=\"node65\"><title>140416373225104</title>\n",
       "<polygon fill=\"none\" points=\"295,-8614.5 295,-8650.5 425,-8650.5 425,-8614.5 295,-8614.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-8628.8\">conv2d_30: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415628327696&#45;&gt;140416373225104 -->\n",
       "<g class=\"edge\" id=\"edge70\"><title>140415628327696-&gt;140416373225104</title>\n",
       "<path d=\"M477.113,-8687.49C455.841,-8677.69 428.901,-8665.27 406.224,-8654.81\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"407.666,-8651.62 397.12,-8650.61 404.736,-8657.98 407.666,-8651.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415627898704 -->\n",
       "<g class=\"node\" id=\"node68\"><title>140415627898704</title>\n",
       "<polygon fill=\"none\" points=\"449,-8614.5 449,-8650.5 579,-8650.5 579,-8614.5 449,-8614.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"514\" y=\"-8628.8\">conv2d_28: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415628327696&#45;&gt;140415627898704 -->\n",
       "<g class=\"edge\" id=\"edge73\"><title>140415628327696-&gt;140415627898704</title>\n",
       "<path d=\"M514,-8687.31C514,-8679.29 514,-8669.55 514,-8660.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"517.5,-8660.53 514,-8650.53 510.5,-8660.53 517.5,-8660.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630441872 -->\n",
       "<g class=\"node\" id=\"node74\"><title>140415630441872</title>\n",
       "<polygon fill=\"none\" points=\"562.5,-8541.5 562.5,-8577.5 807.5,-8577.5 807.5,-8541.5 562.5,-8541.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"685\" y=\"-8555.8\">average_pooling2d_3: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415628327696&#45;&gt;140415630441872 -->\n",
       "<g class=\"edge\" id=\"edge79\"><title>140415628327696-&gt;140415630441872</title>\n",
       "<path d=\"M580.281,-8696.66C653.034,-8686.52 762.886,-8666.18 788,-8633.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M788,-8631.5C804.121,-8610.52 781.578,-8593.43 753.516,-8581.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"754.835,-8578.04 744.253,-8577.51 752.196,-8584.52 754.835,-8578.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415628288336 -->\n",
       "<g class=\"node\" id=\"node75\"><title>140415628288336</title>\n",
       "<polygon fill=\"none\" points=\"826,-8541.5 826,-8577.5 956,-8577.5 956,-8541.5 826,-8541.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891\" y=\"-8555.8\">conv2d_27: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415628327696&#45;&gt;140415628288336 -->\n",
       "<g class=\"edge\" id=\"edge80\"><title>140415628327696-&gt;140415628288336</title>\n",
       "<path d=\"M788,-8631.5C803.563,-8611.25 826.562,-8594.62 846.882,-8582.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"848.759,-8585.52 855.694,-8577.51 845.278,-8579.45 848.759,-8585.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373227152 -->\n",
       "<g class=\"node\" id=\"node66\"><title>140416373227152</title>\n",
       "<polygon fill=\"none\" points=\"186,-8541.5 186,-8577.5 452,-8577.5 452,-8541.5 186,-8541.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-8555.8\">batch_normalization_23: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140416373225104&#45;&gt;140416373227152 -->\n",
       "<g class=\"edge\" id=\"edge71\"><title>140416373225104-&gt;140416373227152</title>\n",
       "<path d=\"M350.075,-8614.31C345.24,-8605.94 339.325,-8595.7 333.958,-8586.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336.865,-8584.44 328.834,-8577.53 330.803,-8587.94 336.865,-8584.44\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583190800 -->\n",
       "<g class=\"node\" id=\"node67\"><title>140415583190800</title>\n",
       "<polygon fill=\"none\" points=\"207.5,-8468.5 207.5,-8504.5 362.5,-8504.5 362.5,-8468.5 207.5,-8468.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"285\" y=\"-8482.8\">activation_23: Activation</text>\n",
       "</g>\n",
       "<!-- 140416373227152&#45;&gt;140415583190800 -->\n",
       "<g class=\"edge\" id=\"edge72\"><title>140416373227152-&gt;140415583190800</title>\n",
       "<path d=\"M310.77,-8541.31C306.802,-8533.03 301.957,-8522.91 297.543,-8513.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"300.63,-8512.04 293.155,-8504.53 294.317,-8515.06 300.63,-8512.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583051408 -->\n",
       "<g class=\"node\" id=\"node69\"><title>140415583051408</title>\n",
       "<polygon fill=\"none\" points=\"241,-8395.5 241,-8431.5 371,-8431.5 371,-8395.5 241,-8395.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-8409.8\">conv2d_31: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415583190800&#45;&gt;140415583051408 -->\n",
       "<g class=\"edge\" id=\"edge74\"><title>140415583190800-&gt;140415583051408</title>\n",
       "<path d=\"M290.084,-8468.31C292.482,-8460.2 295.401,-8450.34 298.08,-8441.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"301.483,-8442.11 300.963,-8431.53 294.771,-8440.13 301.483,-8442.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415627898320 -->\n",
       "<g class=\"node\" id=\"node70\"><title>140415627898320</title>\n",
       "<polygon fill=\"none\" points=\"381,-8468.5 381,-8504.5 647,-8504.5 647,-8468.5 381,-8468.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"514\" y=\"-8482.8\">batch_normalization_21: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415627898704&#45;&gt;140415627898320 -->\n",
       "<g class=\"edge\" id=\"edge75\"><title>140415627898704-&gt;140415627898320</title>\n",
       "<path d=\"M514,-8614.42C514,-8589.84 514,-8544.25 514,-8514.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"517.5,-8514.56 514,-8504.56 510.5,-8514.56 517.5,-8514.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583048464 -->\n",
       "<g class=\"node\" id=\"node71\"><title>140415583048464</title>\n",
       "<polygon fill=\"none\" points=\"183,-8322.5 183,-8358.5 449,-8358.5 449,-8322.5 183,-8322.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"316\" y=\"-8336.8\">batch_normalization_24: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415583051408&#45;&gt;140415583048464 -->\n",
       "<g class=\"edge\" id=\"edge76\"><title>140415583051408-&gt;140415583048464</title>\n",
       "<path d=\"M308.421,-8395.31C309.551,-8387.29 310.923,-8377.55 312.187,-8368.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"315.673,-8368.92 313.602,-8358.53 308.741,-8367.94 315.673,-8368.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415627906384 -->\n",
       "<g class=\"node\" id=\"node72\"><title>140415627906384</title>\n",
       "<polygon fill=\"none\" points=\"423.5,-8395.5 423.5,-8431.5 578.5,-8431.5 578.5,-8395.5 423.5,-8395.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"501\" y=\"-8409.8\">activation_21: Activation</text>\n",
       "</g>\n",
       "<!-- 140415627898320&#45;&gt;140415627906384 -->\n",
       "<g class=\"edge\" id=\"edge77\"><title>140415627898320-&gt;140415627906384</title>\n",
       "<path d=\"M510.853,-8468.31C509.384,-8460.29 507.6,-8450.55 505.956,-8441.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"509.362,-8440.73 504.118,-8431.53 502.476,-8442 509.362,-8440.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580794512 -->\n",
       "<g class=\"node\" id=\"node73\"><title>140415580794512</title>\n",
       "<polygon fill=\"none\" points=\"241.5,-8249.5 241.5,-8285.5 396.5,-8285.5 396.5,-8249.5 241.5,-8249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-8263.8\">activation_24: Activation</text>\n",
       "</g>\n",
       "<!-- 140415583048464&#45;&gt;140415580794512 -->\n",
       "<g class=\"edge\" id=\"edge78\"><title>140415583048464-&gt;140415580794512</title>\n",
       "<path d=\"M316.726,-8322.31C317.065,-8314.29 317.477,-8304.55 317.856,-8295.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"321.355,-8295.67 318.28,-8285.53 314.361,-8295.37 321.355,-8295.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415627992272 -->\n",
       "<g class=\"node\" id=\"node76\"><title>140415627992272</title>\n",
       "<polygon fill=\"none\" points=\"479,-8322.5 479,-8358.5 609,-8358.5 609,-8322.5 479,-8322.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-8336.8\">conv2d_29: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415627906384&#45;&gt;140415627992272 -->\n",
       "<g class=\"edge\" id=\"edge81\"><title>140415627906384-&gt;140415627992272</title>\n",
       "<path d=\"M511.409,-8395.31C516.532,-8386.85 522.813,-8376.48 528.488,-8367.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"531.5,-8368.9 533.687,-8358.53 525.513,-8365.27 531.5,-8368.9\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581008080 -->\n",
       "<g class=\"node\" id=\"node77\"><title>140415581008080</title>\n",
       "<polygon fill=\"none\" points=\"255,-8176.5 255,-8212.5 385,-8212.5 385,-8176.5 255,-8176.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"320\" y=\"-8190.8\">conv2d_32: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415580794512&#45;&gt;140415581008080 -->\n",
       "<g class=\"edge\" id=\"edge82\"><title>140415580794512-&gt;140415581008080</title>\n",
       "<path d=\"M319.242,-8249.31C319.355,-8241.29 319.492,-8231.55 319.619,-8222.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"323.119,-8222.58 319.76,-8212.53 316.12,-8222.48 323.119,-8222.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630438480 -->\n",
       "<g class=\"node\" id=\"node78\"><title>140415630438480</title>\n",
       "<polygon fill=\"none\" points=\"665,-8468.5 665,-8504.5 795,-8504.5 795,-8468.5 665,-8468.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-8482.8\">conv2d_33: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415630441872&#45;&gt;140415630438480 -->\n",
       "<g class=\"edge\" id=\"edge83\"><title>140415630441872-&gt;140415630438480</title>\n",
       "<path d=\"M695.893,-8541.31C701.255,-8532.85 707.827,-8522.48 713.767,-8513.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"716.81,-8514.85 719.207,-8504.53 710.897,-8511.1 716.81,-8514.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415628291728 -->\n",
       "<g class=\"node\" id=\"node79\"><title>140415628291728</title>\n",
       "<polygon fill=\"none\" points=\"753,-8322.5 753,-8358.5 1019,-8358.5 1019,-8322.5 753,-8322.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"886\" y=\"-8336.8\">batch_normalization_20: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415628288336&#45;&gt;140415628291728 -->\n",
       "<g class=\"edge\" id=\"edge84\"><title>140415628288336-&gt;140415628291728</title>\n",
       "<path d=\"M890.608,-8541.47C889.744,-8504.01 887.675,-8414.18 886.628,-8368.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"890.127,-8368.67 886.397,-8358.75 883.129,-8368.83 890.127,-8368.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415627989328 -->\n",
       "<g class=\"node\" id=\"node80\"><title>140415627989328</title>\n",
       "<polygon fill=\"none\" points=\"417,-8249.5 417,-8285.5 683,-8285.5 683,-8249.5 417,-8249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550\" y=\"-8263.8\">batch_normalization_22: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415627992272&#45;&gt;140415627989328 -->\n",
       "<g class=\"edge\" id=\"edge85\"><title>140415627992272-&gt;140415627989328</title>\n",
       "<path d=\"M545.452,-8322.31C546.13,-8314.29 546.954,-8304.55 547.712,-8295.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"551.206,-8295.79 548.561,-8285.53 544.231,-8295.2 551.206,-8295.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581011920 -->\n",
       "<g class=\"node\" id=\"node81\"><title>140415581011920</title>\n",
       "<polygon fill=\"none\" points=\"188,-8103.5 188,-8139.5 454,-8139.5 454,-8103.5 188,-8103.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321\" y=\"-8117.8\">batch_normalization_25: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415581008080&#45;&gt;140415581011920 -->\n",
       "<g class=\"edge\" id=\"edge86\"><title>140415581008080-&gt;140415581011920</title>\n",
       "<path d=\"M320.242,-8176.31C320.355,-8168.29 320.492,-8158.55 320.619,-8149.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"324.119,-8149.58 320.76,-8139.53 317.12,-8149.48 324.119,-8149.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630378128 -->\n",
       "<g class=\"node\" id=\"node82\"><title>140415630378128</title>\n",
       "<polygon fill=\"none\" points=\"597,-8395.5 597,-8431.5 863,-8431.5 863,-8395.5 597,-8395.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-8409.8\">batch_normalization_26: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415630438480&#45;&gt;140415630378128 -->\n",
       "<g class=\"edge\" id=\"edge87\"><title>140415630438480-&gt;140415630378128</title>\n",
       "<path d=\"M730,-8468.31C730,-8460.29 730,-8450.55 730,-8441.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"733.5,-8441.53 730,-8431.53 726.5,-8441.53 733.5,-8441.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415580371728 -->\n",
       "<g class=\"node\" id=\"node83\"><title>140415580371728</title>\n",
       "<polygon fill=\"none\" points=\"766.5,-8176.5 766.5,-8212.5 921.5,-8212.5 921.5,-8176.5 766.5,-8176.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844\" y=\"-8190.8\">activation_20: Activation</text>\n",
       "</g>\n",
       "<!-- 140415628291728&#45;&gt;140415580371728 -->\n",
       "<g class=\"edge\" id=\"edge88\"><title>140415628291728-&gt;140415580371728</title>\n",
       "<path d=\"M881.017,-8322.42C873.818,-8297.73 860.436,-8251.85 851.887,-8222.54\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"855.136,-8221.18 848.976,-8212.56 848.416,-8223.14 855.136,-8221.18\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373215120 -->\n",
       "<g class=\"node\" id=\"node84\"><title>140416373215120</title>\n",
       "<polygon fill=\"none\" points=\"472.5,-8103.5 472.5,-8139.5 627.5,-8139.5 627.5,-8103.5 472.5,-8103.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550\" y=\"-8117.8\">activation_22: Activation</text>\n",
       "</g>\n",
       "<!-- 140415627989328&#45;&gt;140416373215120 -->\n",
       "<g class=\"edge\" id=\"edge89\"><title>140415627989328-&gt;140416373215120</title>\n",
       "<path d=\"M550,-8249.42C550,-8224.84 550,-8179.25 550,-8149.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"553.5,-8149.56 550,-8139.56 546.5,-8149.56 553.5,-8149.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630396496 -->\n",
       "<g class=\"node\" id=\"node85\"><title>140415630396496</title>\n",
       "<polygon fill=\"none\" points=\"306.5,-8030.5 306.5,-8066.5 461.5,-8066.5 461.5,-8030.5 306.5,-8030.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384\" y=\"-8044.8\">activation_25: Activation</text>\n",
       "</g>\n",
       "<!-- 140415581011920&#45;&gt;140415630396496 -->\n",
       "<g class=\"edge\" id=\"edge90\"><title>140415581011920-&gt;140415630396496</title>\n",
       "<path d=\"M336.251,-8103.31C343.989,-8094.59 353.529,-8083.84 362.042,-8074.25\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"364.871,-8076.33 368.89,-8066.53 359.635,-8071.69 364.871,-8076.33\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583569552 -->\n",
       "<g class=\"node\" id=\"node86\"><title>140415583569552</title>\n",
       "<polygon fill=\"none\" points=\"582.5,-8030.5 582.5,-8066.5 737.5,-8066.5 737.5,-8030.5 582.5,-8030.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"660\" y=\"-8044.8\">activation_26: Activation</text>\n",
       "</g>\n",
       "<!-- 140415630378128&#45;&gt;140415583569552 -->\n",
       "<g class=\"edge\" id=\"edge91\"><title>140415630378128-&gt;140415583569552</title>\n",
       "<path d=\"M728.815,-8395.22C727.066,-8368.2 724,-8314.29 724,-8268.5 724,-8268.5 724,-8268.5 724,-8193.5 724,-8148.82 697.95,-8102.54 679.09,-8074.88\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"681.873,-8072.76 673.261,-8066.59 676.145,-8076.78 681.873,-8072.76\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583648720 -->\n",
       "<g class=\"node\" id=\"node87\"><title>140415583648720</title>\n",
       "<polygon fill=\"none\" points=\"539,-7957.5 539,-7993.5 671,-7993.5 671,-7957.5 539,-7957.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"605\" y=\"-7971.8\">mixed2: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415580371728&#45;&gt;140415583648720 -->\n",
       "<g class=\"edge\" id=\"edge92\"><title>140415580371728-&gt;140415583648720</title>\n",
       "<path d=\"M838.11,-8176.37C825.97,-8143.48 795.112,-8071.09 746,-8030 727.288,-8014.34 703.515,-8003 680.846,-7994.9\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"681.811,-7991.53 671.216,-7991.62 679.556,-7998.15 681.811,-7991.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416373215120&#45;&gt;140415583648720 -->\n",
       "<g class=\"edge\" id=\"edge93\"><title>140416373215120-&gt;140415583648720</title>\n",
       "<path d=\"M552.822,-8103.35C556.226,-8084.79 562.795,-8054.55 573,-8030 576.905,-8020.61 582.257,-8010.9 587.466,-8002.37\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"590.593,-8003.98 592.988,-7993.66 584.68,-8000.23 590.593,-8003.98\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415630396496&#45;&gt;140415583648720 -->\n",
       "<g class=\"edge\" id=\"edge94\"><title>140415630396496-&gt;140415583648720</title>\n",
       "<path d=\"M436.935,-8030.49C468.548,-8020.34 508.882,-8007.38 542.113,-7996.7\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"543.281,-8000 551.731,-7993.61 541.14,-7993.34 543.281,-8000\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583569552&#45;&gt;140415583648720 -->\n",
       "<g class=\"edge\" id=\"edge95\"><title>140415583569552-&gt;140415583648720</title>\n",
       "<path d=\"M646.686,-8030.31C639.998,-8021.68 631.768,-8011.06 624.393,-8001.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"627.082,-7999.29 618.191,-7993.53 621.548,-8003.58 627.082,-7999.29\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584407440 -->\n",
       "<g class=\"node\" id=\"node88\"><title>140415584407440</title>\n",
       "<polygon fill=\"none\" points=\"345,-7884.5 345,-7920.5 475,-7920.5 475,-7884.5 345,-7884.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410\" y=\"-7898.8\">conv2d_35: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415583648720&#45;&gt;140415584407440 -->\n",
       "<g class=\"edge\" id=\"edge96\"><title>140415583648720-&gt;140415584407440</title>\n",
       "<path d=\"M558.293,-7957.49C530.758,-7947.47 495.725,-7934.71 466.624,-7924.12\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.596,-7920.75 457.002,-7920.61 465.201,-7927.32 467.596,-7920.75\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583674896 -->\n",
       "<g class=\"node\" id=\"node94\"><title>140415583674896</title>\n",
       "<polygon fill=\"none\" points=\"488,-7811.5 488,-7847.5 618,-7847.5 618,-7811.5 488,-7811.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"553\" y=\"-7825.8\">conv2d_34: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415583648720&#45;&gt;140415583674896 -->\n",
       "<g class=\"edge\" id=\"edge102\"><title>140415583648720-&gt;140415583674896</title>\n",
       "<path d=\"M640.052,-7957.31C659.109,-7945.28 679.018,-7927.15 680,-7903.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M680,-7901.5C681.196,-7872.7 656.096,-7855.47 627.904,-7845.22\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"628.852,-7841.84 618.258,-7842 626.637,-7848.48 628.852,-7841.84\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582797200 -->\n",
       "<g class=\"node\" id=\"node100\"><title>140415582797200</title>\n",
       "<polygon fill=\"none\" points=\"547.5,-7592.5 547.5,-7628.5 754.5,-7628.5 754.5,-7592.5 547.5,-7592.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651\" y=\"-7606.8\">max_pooling2d_6: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140415583648720&#45;&gt;140415582797200 -->\n",
       "<g class=\"edge\" id=\"edge108\"><title>140415583648720-&gt;140415582797200</title>\n",
       "<path d=\"M680,-7901.5C684.027,-7804.51 665.731,-7690.02 656.309,-7638.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"659.725,-7637.96 654.444,-7628.77 652.845,-7639.25 659.725,-7637.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584407248 -->\n",
       "<g class=\"node\" id=\"node89\"><title>140415584407248</title>\n",
       "<polygon fill=\"none\" points=\"180,-7811.5 180,-7847.5 446,-7847.5 446,-7811.5 180,-7811.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313\" y=\"-7825.8\">batch_normalization_28: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415584407440&#45;&gt;140415584407248 -->\n",
       "<g class=\"edge\" id=\"edge97\"><title>140415584407440-&gt;140415584407248</title>\n",
       "<path d=\"M386.519,-7884.31C373.889,-7875.07 358.143,-7863.54 344.466,-7853.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"346.401,-7850.61 336.265,-7847.53 342.267,-7856.26 346.401,-7850.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584513616 -->\n",
       "<g class=\"node\" id=\"node90\"><title>140415584513616</title>\n",
       "<polygon fill=\"none\" points=\"212.5,-7738.5 212.5,-7774.5 367.5,-7774.5 367.5,-7738.5 212.5,-7738.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-7752.8\">activation_28: Activation</text>\n",
       "</g>\n",
       "<!-- 140415584407248&#45;&gt;140415584513616 -->\n",
       "<g class=\"edge\" id=\"edge98\"><title>140415584407248-&gt;140415584513616</title>\n",
       "<path d=\"M307.432,-7811.31C304.805,-7803.2 301.609,-7793.34 298.674,-7784.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"301.928,-7782.96 295.516,-7774.53 295.269,-7785.12 301.928,-7782.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584574928 -->\n",
       "<g class=\"node\" id=\"node91\"><title>140415584574928</title>\n",
       "<polygon fill=\"none\" points=\"242,-7665.5 242,-7701.5 372,-7701.5 372,-7665.5 242,-7665.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-7679.8\">conv2d_36: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415584513616&#45;&gt;140415584574928 -->\n",
       "<g class=\"edge\" id=\"edge99\"><title>140415584513616-&gt;140415584574928</title>\n",
       "<path d=\"M294.115,-7738.31C296.057,-7730.2 298.42,-7720.34 300.589,-7711.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.998,-7712.07 302.923,-7701.53 297.19,-7710.44 303.998,-7712.07\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584571984 -->\n",
       "<g class=\"node\" id=\"node92\"><title>140415584571984</title>\n",
       "<polygon fill=\"none\" points=\"191,-7592.5 191,-7628.5 457,-7628.5 457,-7592.5 191,-7592.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324\" y=\"-7606.8\">batch_normalization_29: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415584574928&#45;&gt;140415584571984 -->\n",
       "<g class=\"edge\" id=\"edge100\"><title>140415584574928-&gt;140415584571984</title>\n",
       "<path d=\"M311.115,-7665.31C313.057,-7657.2 315.42,-7647.34 317.589,-7638.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"320.998,-7639.07 319.923,-7628.53 314.19,-7637.44 320.998,-7639.07\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583932048 -->\n",
       "<g class=\"node\" id=\"node93\"><title>140415583932048</title>\n",
       "<polygon fill=\"none\" points=\"267.5,-7519.5 267.5,-7555.5 422.5,-7555.5 422.5,-7519.5 267.5,-7519.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345\" y=\"-7533.8\">activation_29: Activation</text>\n",
       "</g>\n",
       "<!-- 140415584571984&#45;&gt;140415583932048 -->\n",
       "<g class=\"edge\" id=\"edge101\"><title>140415584571984-&gt;140415583932048</title>\n",
       "<path d=\"M329.084,-7592.31C331.482,-7584.2 334.401,-7574.34 337.08,-7565.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"340.483,-7566.11 339.963,-7555.53 333.771,-7564.13 340.483,-7566.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583371472 -->\n",
       "<g class=\"node\" id=\"node95\"><title>140415583371472</title>\n",
       "<polygon fill=\"none\" points=\"363,-7446.5 363,-7482.5 493,-7482.5 493,-7446.5 363,-7446.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"428\" y=\"-7460.8\">conv2d_37: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415583932048&#45;&gt;140415583371472 -->\n",
       "<g class=\"edge\" id=\"edge103\"><title>140415583932048-&gt;140415583371472</title>\n",
       "<path d=\"M365.092,-7519.31C375.695,-7510.24 388.865,-7498.98 400.411,-7489.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"402.769,-7491.69 408.093,-7482.53 398.219,-7486.37 402.769,-7491.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583678288 -->\n",
       "<g class=\"node\" id=\"node96\"><title>140415583678288</title>\n",
       "<polygon fill=\"none\" points=\"386,-7738.5 386,-7774.5 652,-7774.5 652,-7738.5 386,-7738.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"519\" y=\"-7752.8\">batch_normalization_27: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415583674896&#45;&gt;140415583678288 -->\n",
       "<g class=\"edge\" id=\"edge104\"><title>140415583674896-&gt;140415583678288</title>\n",
       "<path d=\"M544.77,-7811.31C540.802,-7803.03 535.957,-7792.91 531.543,-7783.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"534.63,-7782.04 527.155,-7774.53 528.317,-7785.06 534.63,-7782.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415583375312 -->\n",
       "<g class=\"node\" id=\"node97\"><title>140415583375312</title>\n",
       "<polygon fill=\"none\" points=\"336,-7373.5 336,-7409.5 602,-7409.5 602,-7373.5 336,-7373.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-7387.8\">batch_normalization_30: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415583371472&#45;&gt;140415583375312 -->\n",
       "<g class=\"edge\" id=\"edge105\"><title>140415583371472-&gt;140415583375312</title>\n",
       "<path d=\"M437.925,-7446.31C442.76,-7437.94 448.675,-7427.7 454.042,-7418.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"457.197,-7419.94 459.166,-7409.53 451.135,-7416.44 457.197,-7419.94\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415584730128 -->\n",
       "<g class=\"node\" id=\"node98\"><title>140415584730128</title>\n",
       "<polygon fill=\"none\" points=\"441.5,-7665.5 441.5,-7701.5 596.5,-7701.5 596.5,-7665.5 441.5,-7665.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"519\" y=\"-7679.8\">activation_27: Activation</text>\n",
       "</g>\n",
       "<!-- 140415583678288&#45;&gt;140415584730128 -->\n",
       "<g class=\"edge\" id=\"edge106\"><title>140415583678288-&gt;140415584730128</title>\n",
       "<path d=\"M519,-7738.31C519,-7730.29 519,-7720.55 519,-7711.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"522.5,-7711.53 519,-7701.53 515.5,-7711.53 522.5,-7711.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582510160 -->\n",
       "<g class=\"node\" id=\"node99\"><title>140415582510160</title>\n",
       "<polygon fill=\"none\" points=\"429.5,-7300.5 429.5,-7336.5 584.5,-7336.5 584.5,-7300.5 429.5,-7300.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"507\" y=\"-7314.8\">activation_30: Activation</text>\n",
       "</g>\n",
       "<!-- 140415583375312&#45;&gt;140415582510160 -->\n",
       "<g class=\"edge\" id=\"edge107\"><title>140415583375312-&gt;140415582510160</title>\n",
       "<path d=\"M478.199,-7373.31C482.68,-7364.94 488.162,-7354.7 493.137,-7345.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"496.253,-7347 497.886,-7336.53 490.081,-7343.69 496.253,-7347\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582793808 -->\n",
       "<g class=\"node\" id=\"node101\"><title>140415582793808</title>\n",
       "<polygon fill=\"none\" points=\"513,-7227.5 513,-7263.5 645,-7263.5 645,-7227.5 513,-7227.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-7241.8\">mixed3: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415584730128&#45;&gt;140415582793808 -->\n",
       "<g class=\"edge\" id=\"edge109\"><title>140415584730128-&gt;140415582793808</title>\n",
       "<path d=\"M518.271,-7665.28C518.335,-7645.53 521.298,-7613.18 538,-7592 572.409,-7548.37 631.034,-7590.36 651,-7538.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M651,-7536.5C685.009,-7441.56 625.848,-7323.58 595.408,-7272.3\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"598.263,-7270.26 590.087,-7263.52 592.277,-7273.89 598.263,-7270.26\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582510160&#45;&gt;140415582793808 -->\n",
       "<g class=\"edge\" id=\"edge110\"><title>140415582510160-&gt;140415582793808</title>\n",
       "<path d=\"M524.429,-7300.31C533.45,-7291.42 544.613,-7280.41 554.488,-7270.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"557.068,-7273.04 561.731,-7263.53 552.153,-7268.06 557.068,-7273.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582797200&#45;&gt;140415582793808 -->\n",
       "<g class=\"edge\" id=\"edge111\"><title>140415582797200-&gt;140415582793808</title>\n",
       "<path d=\"M647.625,-7592.23C645.514,-7577.45 644.312,-7555.87 651,-7538.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576896336 -->\n",
       "<g class=\"node\" id=\"node102\"><title>140415576896336</title>\n",
       "<polygon fill=\"none\" points=\"326,-7154.5 326,-7190.5 456,-7190.5 456,-7154.5 326,-7154.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"391\" y=\"-7168.8\">conv2d_42: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415582793808&#45;&gt;140415576896336 -->\n",
       "<g class=\"edge\" id=\"edge112\"><title>140415582793808-&gt;140415576896336</title>\n",
       "<path d=\"M533.969,-7227.49C507.538,-7217.51 473.941,-7204.82 445.958,-7194.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"446.907,-7190.87 436.315,-7190.61 444.433,-7197.42 446.907,-7190.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582306768 -->\n",
       "<g class=\"node\" id=\"node108\"><title>140415582306768</title>\n",
       "<polygon fill=\"none\" points=\"514,-7154.5 514,-7190.5 644,-7190.5 644,-7154.5 514,-7154.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-7168.8\">conv2d_39: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415582793808&#45;&gt;140415582306768 -->\n",
       "<g class=\"edge\" id=\"edge118\"><title>140415582793808-&gt;140415582306768</title>\n",
       "<path d=\"M579,-7227.31C579,-7219.29 579,-7209.55 579,-7200.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"582.5,-7200.53 579,-7190.53 575.5,-7200.53 582.5,-7200.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574720912 -->\n",
       "<g class=\"node\" id=\"node120\"><title>140415574720912</title>\n",
       "<polygon fill=\"none\" points=\"627.5,-7081.5 627.5,-7117.5 872.5,-7117.5 872.5,-7081.5 627.5,-7081.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"750\" y=\"-7095.8\">average_pooling2d_4: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415582793808&#45;&gt;140415574720912 -->\n",
       "<g class=\"edge\" id=\"edge130\"><title>140415582793808-&gt;140415574720912</title>\n",
       "<path d=\"M645.281,-7236.66C718.034,-7226.52 827.886,-7206.18 853,-7173.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M853,-7171.5C869.121,-7150.52 846.578,-7133.43 818.516,-7121.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"819.835,-7118.04 809.253,-7117.51 817.196,-7124.52 819.835,-7118.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582569616 -->\n",
       "<g class=\"node\" id=\"node121\"><title>140415582569616</title>\n",
       "<polygon fill=\"none\" points=\"891,-7081.5 891,-7117.5 1021,-7117.5 1021,-7081.5 891,-7081.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"956\" y=\"-7095.8\">conv2d_38: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415582793808&#45;&gt;140415582569616 -->\n",
       "<g class=\"edge\" id=\"edge131\"><title>140415582793808-&gt;140415582569616</title>\n",
       "<path d=\"M853,-7171.5C868.563,-7151.25 891.562,-7134.62 911.882,-7122.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"913.759,-7125.52 920.694,-7117.51 910.278,-7119.45 913.759,-7125.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576826128 -->\n",
       "<g class=\"node\" id=\"node103\"><title>140415576826128</title>\n",
       "<polygon fill=\"none\" points=\"237,-7081.5 237,-7117.5 503,-7117.5 503,-7081.5 237,-7081.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-7095.8\">batch_normalization_35: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415576896336&#45;&gt;140415576826128 -->\n",
       "<g class=\"edge\" id=\"edge113\"><title>140415576896336-&gt;140415576826128</title>\n",
       "<path d=\"M385.916,-7154.31C383.518,-7146.2 380.599,-7136.34 377.92,-7127.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"381.229,-7126.13 375.037,-7117.53 374.517,-7128.11 381.229,-7126.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576294800 -->\n",
       "<g class=\"node\" id=\"node104\"><title>140415576294800</title>\n",
       "<polygon fill=\"none\" points=\"272.5,-7008.5 272.5,-7044.5 427.5,-7044.5 427.5,-7008.5 272.5,-7008.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-7022.8\">activation_35: Activation</text>\n",
       "</g>\n",
       "<!-- 140415576826128&#45;&gt;140415576294800 -->\n",
       "<g class=\"edge\" id=\"edge114\"><title>140415576826128-&gt;140415576294800</title>\n",
       "<path d=\"M365.159,-7081.31C362.874,-7073.2 360.095,-7063.34 357.543,-7054.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"360.877,-7053.21 354.797,-7044.53 354.139,-7055.1 360.877,-7053.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576639888 -->\n",
       "<g class=\"node\" id=\"node105\"><title>140415576639888</title>\n",
       "<polygon fill=\"none\" points=\"306,-6935.5 306,-6971.5 436,-6971.5 436,-6935.5 306,-6935.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371\" y=\"-6949.8\">conv2d_43: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415576294800&#45;&gt;140415576639888 -->\n",
       "<g class=\"edge\" id=\"edge115\"><title>140415576294800-&gt;140415576639888</title>\n",
       "<path d=\"M355.084,-7008.31C357.482,-7000.2 360.401,-6990.34 363.08,-6981.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"366.483,-6982.11 365.963,-6971.53 359.771,-6980.13 366.483,-6982.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576224976 -->\n",
       "<g class=\"node\" id=\"node106\"><title>140415576224976</title>\n",
       "<polygon fill=\"none\" points=\"248,-6862.5 248,-6898.5 514,-6898.5 514,-6862.5 248,-6862.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"381\" y=\"-6876.8\">batch_normalization_36: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415576639888&#45;&gt;140415576224976 -->\n",
       "<g class=\"edge\" id=\"edge116\"><title>140415576639888-&gt;140415576224976</title>\n",
       "<path d=\"M373.421,-6935.31C374.551,-6927.29 375.923,-6917.55 377.187,-6908.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"380.673,-6908.92 378.602,-6898.53 373.741,-6907.94 380.673,-6908.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576000848 -->\n",
       "<g class=\"node\" id=\"node107\"><title>140415576000848</title>\n",
       "<polygon fill=\"none\" points=\"304.5,-6789.5 304.5,-6825.5 459.5,-6825.5 459.5,-6789.5 304.5,-6789.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-6803.8\">activation_36: Activation</text>\n",
       "</g>\n",
       "<!-- 140415576224976&#45;&gt;140415576000848 -->\n",
       "<g class=\"edge\" id=\"edge117\"><title>140415576224976-&gt;140415576000848</title>\n",
       "<path d=\"M381.242,-6862.31C381.355,-6854.29 381.492,-6844.55 381.619,-6835.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"385.119,-6835.58 381.76,-6825.53 378.12,-6835.48 385.119,-6835.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576175888 -->\n",
       "<g class=\"node\" id=\"node109\"><title>140415576175888</title>\n",
       "<polygon fill=\"none\" points=\"318,-6716.5 318,-6752.5 448,-6752.5 448,-6716.5 318,-6716.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383\" y=\"-6730.8\">conv2d_44: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415576000848&#45;&gt;140415576175888 -->\n",
       "<g class=\"edge\" id=\"edge119\"><title>140415576000848-&gt;140415576175888</title>\n",
       "<path d=\"M382.242,-6789.31C382.355,-6781.29 382.492,-6771.55 382.619,-6762.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"386.119,-6762.58 382.76,-6752.53 379.12,-6762.48 386.119,-6762.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582441168 -->\n",
       "<g class=\"node\" id=\"node110\"><title>140415582441168</title>\n",
       "<polygon fill=\"none\" points=\"446,-7008.5 446,-7044.5 712,-7044.5 712,-7008.5 446,-7008.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-7022.8\">batch_normalization_32: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415582306768&#45;&gt;140415582441168 -->\n",
       "<g class=\"edge\" id=\"edge120\"><title>140415582306768-&gt;140415582441168</title>\n",
       "<path d=\"M579,-7154.42C579,-7129.84 579,-7084.25 579,-7054.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"582.5,-7054.56 579,-7044.56 575.5,-7054.56 582.5,-7054.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576101456 -->\n",
       "<g class=\"node\" id=\"node111\"><title>140415576101456</title>\n",
       "<polygon fill=\"none\" points=\"254,-6643.5 254,-6679.5 520,-6679.5 520,-6643.5 254,-6643.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-6657.8\">batch_normalization_37: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415576175888&#45;&gt;140415576101456 -->\n",
       "<g class=\"edge\" id=\"edge121\"><title>140415576175888-&gt;140415576101456</title>\n",
       "<path d=\"M383.968,-6716.31C384.42,-6708.29 384.969,-6698.55 385.475,-6689.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"388.973,-6689.71 386.041,-6679.53 381.984,-6689.32 388.973,-6689.71\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581811856 -->\n",
       "<g class=\"node\" id=\"node112\"><title>140415581811856</title>\n",
       "<polygon fill=\"none\" points=\"488.5,-6935.5 488.5,-6971.5 643.5,-6971.5 643.5,-6935.5 488.5,-6935.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"566\" y=\"-6949.8\">activation_32: Activation</text>\n",
       "</g>\n",
       "<!-- 140415582441168&#45;&gt;140415581811856 -->\n",
       "<g class=\"edge\" id=\"edge122\"><title>140415582441168-&gt;140415581811856</title>\n",
       "<path d=\"M575.853,-7008.31C574.384,-7000.29 572.6,-6990.55 570.956,-6981.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"574.362,-6980.73 569.118,-6971.53 567.476,-6982 574.362,-6980.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415575918736 -->\n",
       "<g class=\"node\" id=\"node113\"><title>140415575918736</title>\n",
       "<polygon fill=\"none\" points=\"309.5,-6570.5 309.5,-6606.5 464.5,-6606.5 464.5,-6570.5 309.5,-6570.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-6584.8\">activation_37: Activation</text>\n",
       "</g>\n",
       "<!-- 140415576101456&#45;&gt;140415575918736 -->\n",
       "<g class=\"edge\" id=\"edge123\"><title>140415576101456-&gt;140415575918736</title>\n",
       "<path d=\"M387,-6643.31C387,-6635.29 387,-6625.55 387,-6616.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"390.5,-6616.53 387,-6606.53 383.5,-6616.53 390.5,-6616.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582156944 -->\n",
       "<g class=\"node\" id=\"node114\"><title>140415582156944</title>\n",
       "<polygon fill=\"none\" points=\"538,-6862.5 538,-6898.5 668,-6898.5 668,-6862.5 538,-6862.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"603\" y=\"-6876.8\">conv2d_40: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415581811856&#45;&gt;140415582156944 -->\n",
       "<g class=\"edge\" id=\"edge124\"><title>140415581811856-&gt;140415582156944</title>\n",
       "<path d=\"M574.957,-6935.31C579.32,-6926.94 584.658,-6916.7 589.501,-6907.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"592.608,-6909.01 594.126,-6898.53 586.401,-6905.78 592.608,-6909.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415575581776 -->\n",
       "<g class=\"node\" id=\"node115\"><title>140415575581776</title>\n",
       "<polygon fill=\"none\" points=\"322,-6497.5 322,-6533.5 452,-6533.5 452,-6497.5 322,-6497.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-6511.8\">conv2d_45: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415575918736&#45;&gt;140415575581776 -->\n",
       "<g class=\"edge\" id=\"edge125\"><title>140415575918736-&gt;140415575581776</title>\n",
       "<path d=\"M387,-6570.31C387,-6562.29 387,-6552.55 387,-6543.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"390.5,-6543.53 387,-6533.53 383.5,-6543.53 390.5,-6543.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581742032 -->\n",
       "<g class=\"node\" id=\"node116\"><title>140415581742032</title>\n",
       "<polygon fill=\"none\" points=\"482,-6789.5 482,-6825.5 748,-6825.5 748,-6789.5 482,-6789.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"615\" y=\"-6803.8\">batch_normalization_33: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415582156944&#45;&gt;140415581742032 -->\n",
       "<g class=\"edge\" id=\"edge126\"><title>140415582156944-&gt;140415581742032</title>\n",
       "<path d=\"M605.905,-6862.31C607.261,-6854.29 608.907,-6844.55 610.425,-6835.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"613.906,-6835.97 612.122,-6825.53 607.004,-6834.81 613.906,-6835.97\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415575495056 -->\n",
       "<g class=\"node\" id=\"node117\"><title>140415575495056</title>\n",
       "<polygon fill=\"none\" points=\"254,-6424.5 254,-6460.5 520,-6460.5 520,-6424.5 254,-6424.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387\" y=\"-6438.8\">batch_normalization_38: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415575581776&#45;&gt;140415575495056 -->\n",
       "<g class=\"edge\" id=\"edge127\"><title>140415575581776-&gt;140415575495056</title>\n",
       "<path d=\"M387,-6497.31C387,-6489.29 387,-6479.55 387,-6470.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"390.5,-6470.53 387,-6460.53 383.5,-6470.53 390.5,-6470.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581517904 -->\n",
       "<g class=\"node\" id=\"node118\"><title>140415581517904</title>\n",
       "<polygon fill=\"none\" points=\"537.5,-6716.5 537.5,-6752.5 692.5,-6752.5 692.5,-6716.5 537.5,-6716.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"615\" y=\"-6730.8\">activation_33: Activation</text>\n",
       "</g>\n",
       "<!-- 140415581742032&#45;&gt;140415581517904 -->\n",
       "<g class=\"edge\" id=\"edge128\"><title>140415581742032-&gt;140415581517904</title>\n",
       "<path d=\"M615,-6789.31C615,-6781.29 615,-6771.55 615,-6762.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"618.5,-6762.53 615,-6752.53 611.5,-6762.53 618.5,-6762.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415575324624 -->\n",
       "<g class=\"node\" id=\"node119\"><title>140415575324624</title>\n",
       "<polygon fill=\"none\" points=\"374.5,-6351.5 374.5,-6387.5 529.5,-6387.5 529.5,-6351.5 374.5,-6351.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-6365.8\">activation_38: Activation</text>\n",
       "</g>\n",
       "<!-- 140415575495056&#45;&gt;140415575324624 -->\n",
       "<g class=\"edge\" id=\"edge129\"><title>140415575495056-&gt;140415575324624</title>\n",
       "<path d=\"M402.735,-6424.31C410.799,-6415.5 420.759,-6404.63 429.608,-6394.96\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"432.239,-6397.27 436.41,-6387.53 427.076,-6392.54 432.239,-6397.27\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581180944 -->\n",
       "<g class=\"node\" id=\"node122\"><title>140415581180944</title>\n",
       "<polygon fill=\"none\" points=\"551,-6643.5 551,-6679.5 681,-6679.5 681,-6643.5 551,-6643.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-6657.8\">conv2d_41: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415581517904&#45;&gt;140415581180944 -->\n",
       "<g class=\"edge\" id=\"edge132\"><title>140415581517904-&gt;140415581180944</title>\n",
       "<path d=\"M615.242,-6716.31C615.355,-6708.29 615.492,-6698.55 615.619,-6689.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"619.119,-6689.58 615.76,-6679.53 612.12,-6689.48 619.119,-6689.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574979472 -->\n",
       "<g class=\"node\" id=\"node123\"><title>140415574979472</title>\n",
       "<polygon fill=\"none\" points=\"415,-6278.5 415,-6314.5 545,-6314.5 545,-6278.5 415,-6278.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"480\" y=\"-6292.8\">conv2d_46: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415575324624&#45;&gt;140415574979472 -->\n",
       "<g class=\"edge\" id=\"edge133\"><title>140415575324624-&gt;140415574979472</title>\n",
       "<path d=\"M458.778,-6351.31C462.011,-6343.12 465.951,-6333.12 469.555,-6323.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"472.872,-6325.12 473.284,-6314.53 466.36,-6322.55 472.872,-6325.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574722320 -->\n",
       "<g class=\"node\" id=\"node124\"><title>140415574722320</title>\n",
       "<polygon fill=\"none\" points=\"730,-7008.5 730,-7044.5 860,-7044.5 860,-7008.5 730,-7008.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"795\" y=\"-7022.8\">conv2d_47: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415574720912&#45;&gt;140415574722320 -->\n",
       "<g class=\"edge\" id=\"edge134\"><title>140415574720912-&gt;140415574722320</title>\n",
       "<path d=\"M760.893,-7081.31C766.255,-7072.85 772.827,-7062.48 778.767,-7053.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"781.81,-7054.85 784.207,-7044.53 775.897,-7051.1 781.81,-7054.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582569424 -->\n",
       "<g class=\"node\" id=\"node125\"><title>140415582569424</title>\n",
       "<polygon fill=\"none\" points=\"818,-6862.5 818,-6898.5 1084,-6898.5 1084,-6862.5 818,-6862.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"951\" y=\"-6876.8\">batch_normalization_31: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415582569616&#45;&gt;140415582569424 -->\n",
       "<g class=\"edge\" id=\"edge135\"><title>140415582569616-&gt;140415582569424</title>\n",
       "<path d=\"M955.608,-7081.47C954.744,-7044.01 952.675,-6954.18 951.628,-6908.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"955.127,-6908.67 951.397,-6898.75 948.129,-6908.83 955.127,-6908.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415581622608 -->\n",
       "<g class=\"node\" id=\"node126\"><title>140415581622608</title>\n",
       "<polygon fill=\"none\" points=\"483,-6570.5 483,-6606.5 749,-6606.5 749,-6570.5 483,-6570.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-6584.8\">batch_normalization_34: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415581180944&#45;&gt;140415581622608 -->\n",
       "<g class=\"edge\" id=\"edge136\"><title>140415581180944-&gt;140415581622608</title>\n",
       "<path d=\"M616,-6643.31C616,-6635.29 616,-6625.55 616,-6616.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"619.5,-6616.53 616,-6606.53 612.5,-6616.53 619.5,-6616.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574901072 -->\n",
       "<g class=\"node\" id=\"node127\"><title>140415574901072</title>\n",
       "<polygon fill=\"none\" points=\"403,-6205.5 403,-6241.5 669,-6241.5 669,-6205.5 403,-6205.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"536\" y=\"-6219.8\">batch_normalization_39: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415574979472&#45;&gt;140415574901072 -->\n",
       "<g class=\"edge\" id=\"edge137\"><title>140415574979472-&gt;140415574901072</title>\n",
       "<path d=\"M493.556,-6278.31C500.366,-6269.68 508.745,-6259.06 516.255,-6249.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"519.124,-6251.55 522.569,-6241.53 513.628,-6247.21 519.124,-6251.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574484624 -->\n",
       "<g class=\"node\" id=\"node128\"><title>140415574484624</title>\n",
       "<polygon fill=\"none\" points=\"662,-6935.5 662,-6971.5 928,-6971.5 928,-6935.5 662,-6935.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"795\" y=\"-6949.8\">batch_normalization_40: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415574722320&#45;&gt;140415574484624 -->\n",
       "<g class=\"edge\" id=\"edge138\"><title>140415574722320-&gt;140415574484624</title>\n",
       "<path d=\"M795,-7008.31C795,-7000.29 795,-6990.55 795,-6981.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"798.5,-6981.53 795,-6971.53 791.5,-6981.53 798.5,-6981.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415582462864 -->\n",
       "<g class=\"node\" id=\"node129\"><title>140415582462864</title>\n",
       "<polygon fill=\"none\" points=\"829.5,-6497.5 829.5,-6533.5 984.5,-6533.5 984.5,-6497.5 829.5,-6497.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"907\" y=\"-6511.8\">activation_31: Activation</text>\n",
       "</g>\n",
       "<!-- 140415582569424&#45;&gt;140415582462864 -->\n",
       "<g class=\"edge\" id=\"edge139\"><title>140415582569424-&gt;140415582462864</title>\n",
       "<path d=\"M948.036,-6862.27C943.665,-6835.32 936,-6781.5 936,-6735.5 936,-6735.5 936,-6735.5 936,-6660.5 936,-6618.94 924.245,-6571.93 915.703,-6543.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"919.01,-6542.1 912.729,-6533.57 912.318,-6544.15 919.01,-6542.1\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576725392 -->\n",
       "<g class=\"node\" id=\"node130\"><title>140415576725392</title>\n",
       "<polygon fill=\"none\" points=\"538.5,-6424.5 538.5,-6460.5 693.5,-6460.5 693.5,-6424.5 538.5,-6424.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-6438.8\">activation_34: Activation</text>\n",
       "</g>\n",
       "<!-- 140415581622608&#45;&gt;140415576725392 -->\n",
       "<g class=\"edge\" id=\"edge140\"><title>140415581622608-&gt;140415576725392</title>\n",
       "<path d=\"M616,-6570.42C616,-6545.84 616,-6500.25 616,-6470.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"619.5,-6470.56 616,-6460.56 612.5,-6470.56 619.5,-6470.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574422416 -->\n",
       "<g class=\"node\" id=\"node131\"><title>140415574422416</title>\n",
       "<polygon fill=\"none\" points=\"500.5,-6132.5 500.5,-6168.5 655.5,-6168.5 655.5,-6132.5 500.5,-6132.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"578\" y=\"-6146.8\">activation_39: Activation</text>\n",
       "</g>\n",
       "<!-- 140415574901072&#45;&gt;140415574422416 -->\n",
       "<g class=\"edge\" id=\"edge141\"><title>140415574901072-&gt;140415574422416</title>\n",
       "<path d=\"M546.167,-6205.31C551.12,-6196.94 557.179,-6186.7 562.677,-6177.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"565.848,-6178.92 567.927,-6168.53 559.823,-6175.35 565.848,-6178.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574114192 -->\n",
       "<g class=\"node\" id=\"node132\"><title>140415574114192</title>\n",
       "<polygon fill=\"none\" points=\"647.5,-6351.5 647.5,-6387.5 802.5,-6387.5 802.5,-6351.5 647.5,-6351.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"725\" y=\"-6365.8\">activation_40: Activation</text>\n",
       "</g>\n",
       "<!-- 140415574484624&#45;&gt;140415574114192 -->\n",
       "<g class=\"edge\" id=\"edge142\"><title>140415574484624-&gt;140415574114192</title>\n",
       "<path d=\"M793.419,-6935.23C791.088,-6908.22 787,-6854.32 787,-6808.5 787,-6808.5 787,-6808.5 787,-6514.5 787,-6470.05 761.764,-6423.7 743.493,-6395.97\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"746.357,-6393.96 737.846,-6387.65 740.565,-6397.89 746.357,-6393.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574296848 -->\n",
       "<g class=\"node\" id=\"node133\"><title>140415574296848</title>\n",
       "<polygon fill=\"none\" points=\"585,-6059.5 585,-6095.5 717,-6095.5 717,-6059.5 585,-6059.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651\" y=\"-6073.8\">mixed4: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415582462864&#45;&gt;140415574296848 -->\n",
       "<g class=\"edge\" id=\"edge143\"><title>140415582462864-&gt;140415574296848</title>\n",
       "<path d=\"M900.271,-6497.44C887.057,-6465.6 855.114,-6396.26 811,-6351 779.58,-6318.77 729.488,-6342.29 725,-6297.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415576725392&#45;&gt;140415574296848 -->\n",
       "<g class=\"edge\" id=\"edge144\"><title>140415576725392-&gt;140415574296848</title>\n",
       "<path d=\"M616.368,-6424.21C617.592,-6404.69 622.114,-6372.84 638,-6351 664.705,-6314.29 725,-6342.89 725,-6297.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M725,-6295.5C723.187,-6223.27 686.735,-6144.36 665.682,-6104.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"668.644,-6102.69 660.816,-6095.55 662.483,-6106.01 668.644,-6102.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574422416&#45;&gt;140415574296848 -->\n",
       "<g class=\"edge\" id=\"edge145\"><title>140415574422416-&gt;140415574296848</title>\n",
       "<path d=\"M595.671,-6132.31C604.817,-6123.42 616.135,-6112.41 626.147,-6102.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"628.763,-6105.01 633.491,-6095.53 623.883,-6099.99 628.763,-6105.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574114192&#45;&gt;140415574296848 -->\n",
       "<g class=\"edge\" id=\"edge146\"><title>140415574114192-&gt;140415574296848</title>\n",
       "<path d=\"M725,-6351.28C725,-6336.91 725,-6315.87 725,-6297.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572431632 -->\n",
       "<g class=\"node\" id=\"node134\"><title>140415572431632</title>\n",
       "<polygon fill=\"none\" points=\"398,-5986.5 398,-6022.5 528,-6022.5 528,-5986.5 398,-5986.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"463\" y=\"-6000.8\">conv2d_52: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415574296848&#45;&gt;140415572431632 -->\n",
       "<g class=\"edge\" id=\"edge147\"><title>140415574296848-&gt;140415572431632</title>\n",
       "<path d=\"M605.969,-6059.49C579.538,-6049.51 545.941,-6036.82 517.958,-6026.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"518.907,-6022.87 508.315,-6022.61 516.433,-6029.42 518.907,-6022.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573769552 -->\n",
       "<g class=\"node\" id=\"node140\"><title>140415573769552</title>\n",
       "<polygon fill=\"none\" points=\"586,-5986.5 586,-6022.5 716,-6022.5 716,-5986.5 586,-5986.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651\" y=\"-6000.8\">conv2d_49: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415574296848&#45;&gt;140415573769552 -->\n",
       "<g class=\"edge\" id=\"edge153\"><title>140415574296848-&gt;140415573769552</title>\n",
       "<path d=\"M651,-6059.31C651,-6051.29 651,-6041.55 651,-6032.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"654.5,-6032.53 651,-6022.53 647.5,-6032.53 654.5,-6032.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569911760 -->\n",
       "<g class=\"node\" id=\"node152\"><title>140415569911760</title>\n",
       "<polygon fill=\"none\" points=\"699.5,-5913.5 699.5,-5949.5 944.5,-5949.5 944.5,-5913.5 699.5,-5913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"822\" y=\"-5927.8\">average_pooling2d_5: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415574296848&#45;&gt;140415569911760 -->\n",
       "<g class=\"edge\" id=\"edge165\"><title>140415574296848-&gt;140415569911760</title>\n",
       "<path d=\"M717.281,-6068.66C790.034,-6058.52 899.886,-6038.18 925,-6005.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M925,-6003.5C941.121,-5982.52 918.578,-5965.43 890.516,-5953.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"891.835,-5950.04 881.253,-5949.51 889.196,-5956.52 891.835,-5950.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574298576 -->\n",
       "<g class=\"node\" id=\"node153\"><title>140415574298576</title>\n",
       "<polygon fill=\"none\" points=\"963,-5913.5 963,-5949.5 1093,-5949.5 1093,-5913.5 963,-5913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1028\" y=\"-5927.8\">conv2d_48: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415574296848&#45;&gt;140415574298576 -->\n",
       "<g class=\"edge\" id=\"edge166\"><title>140415574296848-&gt;140415574298576</title>\n",
       "<path d=\"M925,-6003.5C940.563,-5983.25 963.562,-5966.62 983.882,-5954.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"985.759,-5957.52 992.694,-5949.51 982.278,-5951.45 985.759,-5957.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572365520 -->\n",
       "<g class=\"node\" id=\"node135\"><title>140415572365520</title>\n",
       "<polygon fill=\"none\" points=\"309,-5913.5 309,-5949.5 575,-5949.5 575,-5913.5 309,-5913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442\" y=\"-5927.8\">batch_normalization_45: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415572431632&#45;&gt;140415572365520 -->\n",
       "<g class=\"edge\" id=\"edge148\"><title>140415572431632-&gt;140415572365520</title>\n",
       "<path d=\"M457.916,-5986.31C455.518,-5978.2 452.599,-5968.34 449.92,-5959.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"453.229,-5958.13 447.037,-5949.53 446.517,-5960.11 453.229,-5958.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571821904 -->\n",
       "<g class=\"node\" id=\"node136\"><title>140415571821904</title>\n",
       "<polygon fill=\"none\" points=\"344.5,-5840.5 344.5,-5876.5 499.5,-5876.5 499.5,-5840.5 344.5,-5840.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422\" y=\"-5854.8\">activation_45: Activation</text>\n",
       "</g>\n",
       "<!-- 140415572365520&#45;&gt;140415571821904 -->\n",
       "<g class=\"edge\" id=\"edge149\"><title>140415572365520-&gt;140415571821904</title>\n",
       "<path d=\"M437.159,-5913.31C434.874,-5905.2 432.095,-5895.34 429.543,-5886.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"432.877,-5885.21 426.797,-5876.53 426.139,-5887.1 432.877,-5885.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572166992 -->\n",
       "<g class=\"node\" id=\"node137\"><title>140415572166992</title>\n",
       "<polygon fill=\"none\" points=\"378,-5767.5 378,-5803.5 508,-5803.5 508,-5767.5 378,-5767.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"443\" y=\"-5781.8\">conv2d_53: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415571821904&#45;&gt;140415572166992 -->\n",
       "<g class=\"edge\" id=\"edge150\"><title>140415571821904-&gt;140415572166992</title>\n",
       "<path d=\"M427.084,-5840.31C429.482,-5832.2 432.401,-5822.34 435.08,-5813.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"438.483,-5814.11 437.963,-5803.53 431.771,-5812.13 438.483,-5814.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571747984 -->\n",
       "<g class=\"node\" id=\"node138\"><title>140415571747984</title>\n",
       "<polygon fill=\"none\" points=\"314,-5694.5 314,-5730.5 580,-5730.5 580,-5694.5 314,-5694.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"447\" y=\"-5708.8\">batch_normalization_46: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415572166992&#45;&gt;140415571747984 -->\n",
       "<g class=\"edge\" id=\"edge151\"><title>140415572166992-&gt;140415571747984</title>\n",
       "<path d=\"M443.968,-5767.31C444.42,-5759.29 444.969,-5749.55 445.475,-5740.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"448.973,-5740.71 446.041,-5730.53 441.984,-5740.32 448.973,-5740.71\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571531984 -->\n",
       "<g class=\"node\" id=\"node139\"><title>140415571531984</title>\n",
       "<polygon fill=\"none\" points=\"372.5,-5621.5 372.5,-5657.5 527.5,-5657.5 527.5,-5621.5 372.5,-5621.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450\" y=\"-5635.8\">activation_46: Activation</text>\n",
       "</g>\n",
       "<!-- 140415571747984&#45;&gt;140415571531984 -->\n",
       "<g class=\"edge\" id=\"edge152\"><title>140415571747984-&gt;140415571531984</title>\n",
       "<path d=\"M447.726,-5694.31C448.065,-5686.29 448.477,-5676.55 448.856,-5667.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"452.355,-5667.67 449.28,-5657.53 445.361,-5667.37 452.355,-5667.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571719312 -->\n",
       "<g class=\"node\" id=\"node141\"><title>140415571719312</title>\n",
       "<polygon fill=\"none\" points=\"386,-5548.5 386,-5584.5 516,-5584.5 516,-5548.5 386,-5548.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"451\" y=\"-5562.8\">conv2d_54: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415571531984&#45;&gt;140415571719312 -->\n",
       "<g class=\"edge\" id=\"edge154\"><title>140415571531984-&gt;140415571719312</title>\n",
       "<path d=\"M450.242,-5621.31C450.355,-5613.29 450.492,-5603.55 450.619,-5594.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"454.119,-5594.58 450.76,-5584.53 447.12,-5594.48 454.119,-5594.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573799504 -->\n",
       "<g class=\"node\" id=\"node142\"><title>140415573799504</title>\n",
       "<polygon fill=\"none\" points=\"518,-5840.5 518,-5876.5 784,-5876.5 784,-5840.5 518,-5840.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651\" y=\"-5854.8\">batch_normalization_42: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415573769552&#45;&gt;140415573799504 -->\n",
       "<g class=\"edge\" id=\"edge155\"><title>140415573769552-&gt;140415573799504</title>\n",
       "<path d=\"M651,-5986.42C651,-5961.84 651,-5916.25 651,-5886.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"654.5,-5886.56 651,-5876.56 647.5,-5886.56 654.5,-5886.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571640784 -->\n",
       "<g class=\"node\" id=\"node143\"><title>140415571640784</title>\n",
       "<polygon fill=\"none\" points=\"319,-5475.5 319,-5511.5 585,-5511.5 585,-5475.5 319,-5475.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-5489.8\">batch_normalization_47: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415571719312&#45;&gt;140415571640784 -->\n",
       "<g class=\"edge\" id=\"edge156\"><title>140415571719312-&gt;140415571640784</title>\n",
       "<path d=\"M451.242,-5548.31C451.355,-5540.29 451.492,-5530.55 451.619,-5521.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"455.119,-5521.58 451.76,-5511.53 448.12,-5521.48 455.119,-5521.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573144528 -->\n",
       "<g class=\"node\" id=\"node144\"><title>140415573144528</title>\n",
       "<polygon fill=\"none\" points=\"560.5,-5767.5 560.5,-5803.5 715.5,-5803.5 715.5,-5767.5 560.5,-5767.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638\" y=\"-5781.8\">activation_42: Activation</text>\n",
       "</g>\n",
       "<!-- 140415573799504&#45;&gt;140415573144528 -->\n",
       "<g class=\"edge\" id=\"edge157\"><title>140415573799504-&gt;140415573144528</title>\n",
       "<path d=\"M647.853,-5840.31C646.384,-5832.29 644.6,-5822.55 642.956,-5813.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"646.362,-5812.73 641.118,-5803.53 639.476,-5814 646.362,-5812.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571462160 -->\n",
       "<g class=\"node\" id=\"node145\"><title>140415571462160</title>\n",
       "<polygon fill=\"none\" points=\"374.5,-5402.5 374.5,-5438.5 529.5,-5438.5 529.5,-5402.5 374.5,-5402.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-5416.8\">activation_47: Activation</text>\n",
       "</g>\n",
       "<!-- 140415571640784&#45;&gt;140415571462160 -->\n",
       "<g class=\"edge\" id=\"edge158\"><title>140415571640784-&gt;140415571462160</title>\n",
       "<path d=\"M452,-5475.31C452,-5467.29 452,-5457.55 452,-5448.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"455.5,-5448.53 452,-5438.53 448.5,-5448.53 455.5,-5448.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573481424 -->\n",
       "<g class=\"node\" id=\"node146\"><title>140415573481424</title>\n",
       "<polygon fill=\"none\" points=\"608,-5694.5 608,-5730.5 738,-5730.5 738,-5694.5 608,-5694.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-5708.8\">conv2d_50: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415573144528&#45;&gt;140415573481424 -->\n",
       "<g class=\"edge\" id=\"edge159\"><title>140415573144528-&gt;140415573481424</title>\n",
       "<path d=\"M646.473,-5767.31C650.557,-5759.03 655.544,-5748.91 660.088,-5739.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"663.323,-5741.05 664.605,-5730.53 657.045,-5737.95 663.323,-5741.05\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571108816 -->\n",
       "<g class=\"node\" id=\"node147\"><title>140415571108816</title>\n",
       "<polygon fill=\"none\" points=\"387,-5329.5 387,-5365.5 517,-5365.5 517,-5329.5 387,-5329.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-5343.8\">conv2d_55: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415571462160&#45;&gt;140415571108816 -->\n",
       "<g class=\"edge\" id=\"edge160\"><title>140415571462160-&gt;140415571108816</title>\n",
       "<path d=\"M452,-5402.31C452,-5394.29 452,-5384.55 452,-5375.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"455.5,-5375.53 452,-5365.53 448.5,-5375.53 455.5,-5375.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573482960 -->\n",
       "<g class=\"node\" id=\"node148\"><title>140415573482960</title>\n",
       "<polygon fill=\"none\" points=\"546,-5621.5 546,-5657.5 812,-5657.5 812,-5621.5 546,-5621.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-5635.8\">batch_normalization_43: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415573481424&#45;&gt;140415573482960 -->\n",
       "<g class=\"edge\" id=\"edge161\"><title>140415573481424-&gt;140415573482960</title>\n",
       "<path d=\"M674.452,-5694.31C675.13,-5686.29 675.954,-5676.55 676.712,-5667.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"680.206,-5667.79 677.561,-5657.53 673.231,-5667.2 680.206,-5667.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415571038480 -->\n",
       "<g class=\"node\" id=\"node149\"><title>140415571038480</title>\n",
       "<polygon fill=\"none\" points=\"319,-5256.5 319,-5292.5 585,-5292.5 585,-5256.5 319,-5256.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-5270.8\">batch_normalization_48: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415571108816&#45;&gt;140415571038480 -->\n",
       "<g class=\"edge\" id=\"edge162\"><title>140415571108816-&gt;140415571038480</title>\n",
       "<path d=\"M452,-5329.31C452,-5321.29 452,-5311.55 452,-5302.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"455.5,-5302.53 452,-5292.53 448.5,-5302.53 455.5,-5302.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572838416 -->\n",
       "<g class=\"node\" id=\"node150\"><title>140415572838416</title>\n",
       "<polygon fill=\"none\" points=\"601.5,-5548.5 601.5,-5584.5 756.5,-5584.5 756.5,-5548.5 601.5,-5548.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-5562.8\">activation_43: Activation</text>\n",
       "</g>\n",
       "<!-- 140415573482960&#45;&gt;140415572838416 -->\n",
       "<g class=\"edge\" id=\"edge163\"><title>140415573482960-&gt;140415572838416</title>\n",
       "<path d=\"M679,-5621.31C679,-5613.29 679,-5603.55 679,-5594.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"682.5,-5594.53 679,-5584.53 675.5,-5594.53 682.5,-5594.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415570843536 -->\n",
       "<g class=\"node\" id=\"node151\"><title>140415570843536</title>\n",
       "<polygon fill=\"none\" points=\"439.5,-5183.5 439.5,-5219.5 594.5,-5219.5 594.5,-5183.5 439.5,-5183.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-5197.8\">activation_48: Activation</text>\n",
       "</g>\n",
       "<!-- 140415571038480&#45;&gt;140415570843536 -->\n",
       "<g class=\"edge\" id=\"edge164\"><title>140415571038480-&gt;140415570843536</title>\n",
       "<path d=\"M467.735,-5256.31C475.799,-5247.5 485.759,-5236.63 494.608,-5226.96\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"497.239,-5229.27 501.41,-5219.53 492.076,-5224.54 497.239,-5229.27\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573033936 -->\n",
       "<g class=\"node\" id=\"node154\"><title>140415573033936</title>\n",
       "<polygon fill=\"none\" points=\"615,-5475.5 615,-5511.5 745,-5511.5 745,-5475.5 615,-5475.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"680\" y=\"-5489.8\">conv2d_51: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415572838416&#45;&gt;140415573033936 -->\n",
       "<g class=\"edge\" id=\"edge167\"><title>140415572838416-&gt;140415573033936</title>\n",
       "<path d=\"M679.242,-5548.31C679.355,-5540.29 679.492,-5530.55 679.619,-5521.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"683.119,-5521.58 679.76,-5511.53 676.12,-5521.48 683.119,-5521.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415570514768 -->\n",
       "<g class=\"node\" id=\"node155\"><title>140415570514768</title>\n",
       "<polygon fill=\"none\" points=\"480,-5110.5 480,-5146.5 610,-5146.5 610,-5110.5 480,-5110.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545\" y=\"-5124.8\">conv2d_56: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415570843536&#45;&gt;140415570514768 -->\n",
       "<g class=\"edge\" id=\"edge168\"><title>140415570843536-&gt;140415570514768</title>\n",
       "<path d=\"M523.778,-5183.31C527.011,-5175.12 530.951,-5165.12 534.555,-5155.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"537.872,-5157.12 538.284,-5146.53 531.36,-5154.55 537.872,-5157.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415570252112 -->\n",
       "<g class=\"node\" id=\"node156\"><title>140415570252112</title>\n",
       "<polygon fill=\"none\" points=\"802,-5840.5 802,-5876.5 932,-5876.5 932,-5840.5 802,-5840.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"867\" y=\"-5854.8\">conv2d_57: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415569911760&#45;&gt;140415570252112 -->\n",
       "<g class=\"edge\" id=\"edge169\"><title>140415569911760-&gt;140415570252112</title>\n",
       "<path d=\"M832.893,-5913.31C838.255,-5904.85 844.827,-5894.48 850.767,-5885.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"853.81,-5886.85 856.207,-5876.53 847.897,-5883.1 853.81,-5886.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415573863696 -->\n",
       "<g class=\"node\" id=\"node157\"><title>140415573863696</title>\n",
       "<polygon fill=\"none\" points=\"885,-5694.5 885,-5730.5 1151,-5730.5 1151,-5694.5 885,-5694.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1018\" y=\"-5708.8\">batch_normalization_41: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415574298576&#45;&gt;140415573863696 -->\n",
       "<g class=\"edge\" id=\"edge170\"><title>140415574298576-&gt;140415573863696</title>\n",
       "<path d=\"M1027.22,-5913.47C1025.49,-5876.01 1021.35,-5786.18 1019.26,-5740.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1022.75,-5740.58 1018.79,-5730.75 1015.76,-5740.9 1022.75,-5740.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572955408 -->\n",
       "<g class=\"node\" id=\"node158\"><title>140415572955408</title>\n",
       "<polygon fill=\"none\" points=\"548,-5402.5 548,-5438.5 814,-5438.5 814,-5402.5 548,-5402.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"681\" y=\"-5416.8\">batch_normalization_44: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415573033936&#45;&gt;140415572955408 -->\n",
       "<g class=\"edge\" id=\"edge171\"><title>140415573033936-&gt;140415572955408</title>\n",
       "<path d=\"M680.242,-5475.31C680.355,-5467.29 680.492,-5457.55 680.619,-5448.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.119,-5448.58 680.76,-5438.53 677.12,-5448.48 684.119,-5448.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415570432272 -->\n",
       "<g class=\"node\" id=\"node159\"><title>140415570432272</title>\n",
       "<polygon fill=\"none\" points=\"468,-5037.5 468,-5073.5 734,-5073.5 734,-5037.5 468,-5037.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"601\" y=\"-5051.8\">batch_normalization_49: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415570514768&#45;&gt;140415570432272 -->\n",
       "<g class=\"edge\" id=\"edge172\"><title>140415570514768-&gt;140415570432272</title>\n",
       "<path d=\"M558.556,-5110.31C565.366,-5101.68 573.745,-5091.06 581.255,-5081.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"584.124,-5083.55 587.569,-5073.53 578.628,-5079.21 584.124,-5083.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415570019920 -->\n",
       "<g class=\"node\" id=\"node160\"><title>140415570019920</title>\n",
       "<polygon fill=\"none\" points=\"734,-5767.5 734,-5803.5 1000,-5803.5 1000,-5767.5 734,-5767.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"867\" y=\"-5781.8\">batch_normalization_50: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415570252112&#45;&gt;140415570019920 -->\n",
       "<g class=\"edge\" id=\"edge173\"><title>140415570252112-&gt;140415570019920</title>\n",
       "<path d=\"M867,-5840.31C867,-5832.29 867,-5822.55 867,-5813.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"870.5,-5813.53 867,-5803.53 863.5,-5813.53 870.5,-5813.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574003344 -->\n",
       "<g class=\"node\" id=\"node161\"><title>140415574003344</title>\n",
       "<polygon fill=\"none\" points=\"895.5,-5329.5 895.5,-5365.5 1050.5,-5365.5 1050.5,-5329.5 895.5,-5329.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"973\" y=\"-5343.8\">activation_41: Activation</text>\n",
       "</g>\n",
       "<!-- 140415573863696&#45;&gt;140415574003344 -->\n",
       "<g class=\"edge\" id=\"edge174\"><title>140415573863696-&gt;140415574003344</title>\n",
       "<path d=\"M1015.04,-5694.27C1010.67,-5667.32 1003,-5613.5 1003,-5567.5 1003,-5567.5 1003,-5567.5 1003,-5492.5 1003,-5450.88 990.84,-5403.89 982.004,-5375.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"985.291,-5374.02 978.927,-5365.55 978.619,-5376.14 985.291,-5374.02\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572780880 -->\n",
       "<g class=\"node\" id=\"node162\"><title>140415572780880</title>\n",
       "<polygon fill=\"none\" points=\"603.5,-5256.5 603.5,-5292.5 758.5,-5292.5 758.5,-5256.5 603.5,-5256.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"681\" y=\"-5270.8\">activation_44: Activation</text>\n",
       "</g>\n",
       "<!-- 140415572955408&#45;&gt;140415572780880 -->\n",
       "<g class=\"edge\" id=\"edge175\"><title>140415572955408-&gt;140415572780880</title>\n",
       "<path d=\"M681,-5402.42C681,-5377.84 681,-5332.25 681,-5302.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"684.5,-5302.56 681,-5292.56 677.5,-5302.56 684.5,-5302.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569953616 -->\n",
       "<g class=\"node\" id=\"node163\"><title>140415569953616</title>\n",
       "<polygon fill=\"none\" points=\"564.5,-4964.5 564.5,-5000.5 719.5,-5000.5 719.5,-4964.5 564.5,-4964.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"642\" y=\"-4978.8\">activation_49: Activation</text>\n",
       "</g>\n",
       "<!-- 140415570432272&#45;&gt;140415569953616 -->\n",
       "<g class=\"edge\" id=\"edge176\"><title>140415570432272-&gt;140415569953616</title>\n",
       "<path d=\"M610.925,-5037.31C615.76,-5028.94 621.675,-5018.7 627.042,-5009.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"630.197,-5010.94 632.166,-5000.53 624.135,-5007.44 630.197,-5010.94\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569649488 -->\n",
       "<g class=\"node\" id=\"node164\"><title>140415569649488</title>\n",
       "<polygon fill=\"none\" points=\"712.5,-5183.5 712.5,-5219.5 867.5,-5219.5 867.5,-5183.5 712.5,-5183.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790\" y=\"-5197.8\">activation_50: Activation</text>\n",
       "</g>\n",
       "<!-- 140415570019920&#45;&gt;140415569649488 -->\n",
       "<g class=\"edge\" id=\"edge177\"><title>140415570019920-&gt;140415569649488</title>\n",
       "<path d=\"M864.036,-5767.27C859.665,-5740.32 852,-5686.5 852,-5640.5 852,-5640.5 852,-5640.5 852,-5346.5 852,-5302.05 826.764,-5255.7 808.493,-5227.97\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"811.357,-5225.96 802.846,-5219.65 805.565,-5229.89 811.357,-5225.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569832144 -->\n",
       "<g class=\"node\" id=\"node165\"><title>140415569832144</title>\n",
       "<polygon fill=\"none\" points=\"650,-4891.5 650,-4927.5 782,-4927.5 782,-4891.5 650,-4891.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"716\" y=\"-4905.8\">mixed5: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415574003344&#45;&gt;140415569832144 -->\n",
       "<g class=\"edge\" id=\"edge178\"><title>140415574003344-&gt;140415569832144</title>\n",
       "<path d=\"M966.157,-5329.44C952.73,-5297.61 920.335,-5228.28 876,-5183 844.507,-5150.84 794.488,-5174.29 790,-5129.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415572780880&#45;&gt;140415569832144 -->\n",
       "<g class=\"edge\" id=\"edge179\"><title>140415572780880-&gt;140415569832144</title>\n",
       "<path d=\"M681.368,-5256.21C682.592,-5236.69 687.114,-5204.84 703,-5183 729.705,-5146.29 790,-5174.89 790,-5129.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M790,-5127.5C788.187,-5055.27 751.735,-4976.36 730.682,-4936.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"733.644,-4934.69 725.816,-4927.55 727.483,-4938.01 733.644,-4934.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569953616&#45;&gt;140415569832144 -->\n",
       "<g class=\"edge\" id=\"edge180\"><title>140415569953616-&gt;140415569832144</title>\n",
       "<path d=\"M659.913,-4964.31C669.185,-4955.42 680.658,-4944.41 690.807,-4934.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"693.459,-4936.98 698.252,-4927.53 688.613,-4931.93 693.459,-4936.98\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569649488&#45;&gt;140415569832144 -->\n",
       "<g class=\"edge\" id=\"edge181\"><title>140415569649488-&gt;140415569832144</title>\n",
       "<path d=\"M790,-5183.28C790,-5168.91 790,-5147.87 790,-5129.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568023952 -->\n",
       "<g class=\"node\" id=\"node166\"><title>140415568023952</title>\n",
       "<polygon fill=\"none\" points=\"463,-4818.5 463,-4854.5 593,-4854.5 593,-4818.5 463,-4818.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"528\" y=\"-4832.8\">conv2d_62: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415569832144&#45;&gt;140415568023952 -->\n",
       "<g class=\"edge\" id=\"edge182\"><title>140415569832144-&gt;140415568023952</title>\n",
       "<path d=\"M670.969,-4891.49C644.538,-4881.51 610.941,-4868.82 582.958,-4858.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"583.907,-4854.87 573.315,-4854.61 581.433,-4861.42 583.907,-4854.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569312912 -->\n",
       "<g class=\"node\" id=\"node172\"><title>140415569312912</title>\n",
       "<polygon fill=\"none\" points=\"651,-4818.5 651,-4854.5 781,-4854.5 781,-4818.5 651,-4818.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"716\" y=\"-4832.8\">conv2d_59: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415569832144&#45;&gt;140415569312912 -->\n",
       "<g class=\"edge\" id=\"edge188\"><title>140415569832144-&gt;140415569312912</title>\n",
       "<path d=\"M716,-4891.31C716,-4883.29 716,-4873.55 716,-4864.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"719.5,-4864.53 716,-4854.53 712.5,-4864.53 719.5,-4864.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332536528 -->\n",
       "<g class=\"node\" id=\"node184\"><title>140415332536528</title>\n",
       "<polygon fill=\"none\" points=\"764.5,-4745.5 764.5,-4781.5 1009.5,-4781.5 1009.5,-4745.5 764.5,-4745.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"887\" y=\"-4759.8\">average_pooling2d_6: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415569832144&#45;&gt;140415332536528 -->\n",
       "<g class=\"edge\" id=\"edge200\"><title>140415569832144-&gt;140415332536528</title>\n",
       "<path d=\"M782.281,-4900.66C855.034,-4890.52 964.886,-4870.18 990,-4837.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M990,-4835.5C1006.12,-4814.52 983.578,-4797.43 955.516,-4785.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"956.835,-4782.04 946.253,-4781.51 954.196,-4788.52 956.835,-4782.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569832592 -->\n",
       "<g class=\"node\" id=\"node185\"><title>140415569832592</title>\n",
       "<polygon fill=\"none\" points=\"1028,-4745.5 1028,-4781.5 1158,-4781.5 1158,-4745.5 1028,-4745.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093\" y=\"-4759.8\">conv2d_58: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415569832144&#45;&gt;140415569832592 -->\n",
       "<g class=\"edge\" id=\"edge201\"><title>140415569832144-&gt;140415569832592</title>\n",
       "<path d=\"M990,-4835.5C1005.56,-4815.25 1028.56,-4798.62 1048.88,-4786.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1050.76,-4789.52 1057.69,-4781.51 1047.28,-4783.45 1050.76,-4789.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568024208 -->\n",
       "<g class=\"node\" id=\"node167\"><title>140415568024208</title>\n",
       "<polygon fill=\"none\" points=\"374,-4745.5 374,-4781.5 640,-4781.5 640,-4745.5 374,-4745.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"507\" y=\"-4759.8\">batch_normalization_55: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415568023952&#45;&gt;140415568024208 -->\n",
       "<g class=\"edge\" id=\"edge183\"><title>140415568023952-&gt;140415568024208</title>\n",
       "<path d=\"M522.916,-4818.31C520.518,-4810.2 517.599,-4800.34 514.92,-4791.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"518.229,-4790.13 512.037,-4781.53 511.517,-4792.11 518.229,-4790.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415567373328 -->\n",
       "<g class=\"node\" id=\"node168\"><title>140415567373328</title>\n",
       "<polygon fill=\"none\" points=\"409.5,-4672.5 409.5,-4708.5 564.5,-4708.5 564.5,-4672.5 409.5,-4672.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"487\" y=\"-4686.8\">activation_55: Activation</text>\n",
       "</g>\n",
       "<!-- 140415568024208&#45;&gt;140415567373328 -->\n",
       "<g class=\"edge\" id=\"edge184\"><title>140415568024208-&gt;140415567373328</title>\n",
       "<path d=\"M502.159,-4745.31C499.874,-4737.2 497.095,-4727.34 494.543,-4718.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"497.877,-4717.21 491.797,-4708.53 491.139,-4719.1 497.877,-4717.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415567719952 -->\n",
       "<g class=\"node\" id=\"node169\"><title>140415567719952</title>\n",
       "<polygon fill=\"none\" points=\"426,-4599.5 426,-4635.5 556,-4635.5 556,-4599.5 426,-4599.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"491\" y=\"-4613.8\">conv2d_63: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415567373328&#45;&gt;140415567719952 -->\n",
       "<g class=\"edge\" id=\"edge185\"><title>140415567373328-&gt;140415567719952</title>\n",
       "<path d=\"M487.968,-4672.31C488.42,-4664.29 488.969,-4654.55 489.475,-4645.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"492.973,-4645.71 490.041,-4635.53 485.984,-4645.32 492.973,-4645.71\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415567719760 -->\n",
       "<g class=\"node\" id=\"node170\"><title>140415567719760</title>\n",
       "<polygon fill=\"none\" points=\"379,-4526.5 379,-4562.5 645,-4562.5 645,-4526.5 379,-4526.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"512\" y=\"-4540.8\">batch_normalization_56: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415567719952&#45;&gt;140415567719760 -->\n",
       "<g class=\"edge\" id=\"edge186\"><title>140415567719952-&gt;140415567719760</title>\n",
       "<path d=\"M496.084,-4599.31C498.482,-4591.2 501.401,-4581.34 504.08,-4572.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"507.483,-4573.11 506.963,-4562.53 500.771,-4571.13 507.483,-4573.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415334107024 -->\n",
       "<g class=\"node\" id=\"node171\"><title>140415334107024</title>\n",
       "<polygon fill=\"none\" points=\"437.5,-4453.5 437.5,-4489.5 592.5,-4489.5 592.5,-4453.5 437.5,-4453.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"515\" y=\"-4467.8\">activation_56: Activation</text>\n",
       "</g>\n",
       "<!-- 140415567719760&#45;&gt;140415334107024 -->\n",
       "<g class=\"edge\" id=\"edge187\"><title>140415567719760-&gt;140415334107024</title>\n",
       "<path d=\"M512.726,-4526.31C513.065,-4518.29 513.477,-4508.55 513.856,-4499.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"517.355,-4499.67 514.28,-4489.53 510.361,-4499.37 517.355,-4499.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415334294352 -->\n",
       "<g class=\"node\" id=\"node173\"><title>140415334294352</title>\n",
       "<polygon fill=\"none\" points=\"451,-4380.5 451,-4416.5 581,-4416.5 581,-4380.5 451,-4380.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"516\" y=\"-4394.8\">conv2d_64: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415334107024&#45;&gt;140415334294352 -->\n",
       "<g class=\"edge\" id=\"edge189\"><title>140415334107024-&gt;140415334294352</title>\n",
       "<path d=\"M515.242,-4453.31C515.355,-4445.29 515.492,-4435.55 515.619,-4426.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"519.119,-4426.58 515.76,-4416.53 512.12,-4426.48 519.119,-4426.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569354640 -->\n",
       "<g class=\"node\" id=\"node174\"><title>140415569354640</title>\n",
       "<polygon fill=\"none\" points=\"583,-4672.5 583,-4708.5 849,-4708.5 849,-4672.5 583,-4672.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"716\" y=\"-4686.8\">batch_normalization_52: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415569312912&#45;&gt;140415569354640 -->\n",
       "<g class=\"edge\" id=\"edge190\"><title>140415569312912-&gt;140415569354640</title>\n",
       "<path d=\"M716,-4818.42C716,-4793.84 716,-4748.25 716,-4718.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"719.5,-4718.56 716,-4708.56 712.5,-4718.56 719.5,-4718.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415334215952 -->\n",
       "<g class=\"node\" id=\"node175\"><title>140415334215952</title>\n",
       "<polygon fill=\"none\" points=\"384,-4307.5 384,-4343.5 650,-4343.5 650,-4307.5 384,-4307.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-4321.8\">batch_normalization_57: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415334294352&#45;&gt;140415334215952 -->\n",
       "<g class=\"edge\" id=\"edge191\"><title>140415334294352-&gt;140415334215952</title>\n",
       "<path d=\"M516.242,-4380.31C516.355,-4372.29 516.492,-4362.55 516.619,-4353.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520.119,-4353.58 516.76,-4343.53 513.12,-4353.48 520.119,-4353.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568679696 -->\n",
       "<g class=\"node\" id=\"node176\"><title>140415568679696</title>\n",
       "<polygon fill=\"none\" points=\"625.5,-4599.5 625.5,-4635.5 780.5,-4635.5 780.5,-4599.5 625.5,-4599.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-4613.8\">activation_52: Activation</text>\n",
       "</g>\n",
       "<!-- 140415569354640&#45;&gt;140415568679696 -->\n",
       "<g class=\"edge\" id=\"edge192\"><title>140415569354640-&gt;140415568679696</title>\n",
       "<path d=\"M712.853,-4672.31C711.384,-4664.29 709.6,-4654.55 707.956,-4645.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"711.362,-4644.73 706.118,-4635.53 704.476,-4646 711.362,-4644.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415333741392 -->\n",
       "<g class=\"node\" id=\"node177\"><title>140415333741392</title>\n",
       "<polygon fill=\"none\" points=\"439.5,-4234.5 439.5,-4270.5 594.5,-4270.5 594.5,-4234.5 439.5,-4234.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-4248.8\">activation_57: Activation</text>\n",
       "</g>\n",
       "<!-- 140415334215952&#45;&gt;140415333741392 -->\n",
       "<g class=\"edge\" id=\"edge193\"><title>140415334215952-&gt;140415333741392</title>\n",
       "<path d=\"M517,-4307.31C517,-4299.29 517,-4289.55 517,-4280.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520.5,-4280.53 517,-4270.53 513.5,-4280.53 520.5,-4280.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569020688 -->\n",
       "<g class=\"node\" id=\"node178\"><title>140415569020688</title>\n",
       "<polygon fill=\"none\" points=\"674,-4526.5 674,-4562.5 804,-4562.5 804,-4526.5 674,-4526.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"739\" y=\"-4540.8\">conv2d_60: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415568679696&#45;&gt;140415569020688 -->\n",
       "<g class=\"edge\" id=\"edge194\"><title>140415568679696-&gt;140415569020688</title>\n",
       "<path d=\"M711.715,-4599.31C715.915,-4591.03 721.045,-4580.91 725.719,-4571.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"728.965,-4573.03 730.366,-4562.53 722.722,-4569.87 728.965,-4573.03\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415333691344 -->\n",
       "<g class=\"node\" id=\"node179\"><title>140415333691344</title>\n",
       "<polygon fill=\"none\" points=\"452,-4161.5 452,-4197.5 582,-4197.5 582,-4161.5 452,-4161.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-4175.8\">conv2d_65: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415333741392&#45;&gt;140415333691344 -->\n",
       "<g class=\"edge\" id=\"edge195\"><title>140415333741392-&gt;140415333691344</title>\n",
       "<path d=\"M517,-4234.31C517,-4226.29 517,-4216.55 517,-4207.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520.5,-4207.53 517,-4197.53 513.5,-4207.53 520.5,-4207.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569022224 -->\n",
       "<g class=\"node\" id=\"node180\"><title>140415569022224</title>\n",
       "<polygon fill=\"none\" points=\"612,-4453.5 612,-4489.5 878,-4489.5 878,-4453.5 612,-4453.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"745\" y=\"-4467.8\">batch_normalization_53: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415569020688&#45;&gt;140415569022224 -->\n",
       "<g class=\"edge\" id=\"edge196\"><title>140415569020688-&gt;140415569022224</title>\n",
       "<path d=\"M740.452,-4526.31C741.13,-4518.29 741.954,-4508.55 742.712,-4499.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"746.206,-4499.79 743.561,-4489.53 739.231,-4499.2 746.206,-4499.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415334042000 -->\n",
       "<g class=\"node\" id=\"node181\"><title>140415334042000</title>\n",
       "<polygon fill=\"none\" points=\"384,-4088.5 384,-4124.5 650,-4124.5 650,-4088.5 384,-4088.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-4102.8\">batch_normalization_58: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415333691344&#45;&gt;140415334042000 -->\n",
       "<g class=\"edge\" id=\"edge197\"><title>140415333691344-&gt;140415334042000</title>\n",
       "<path d=\"M517,-4161.31C517,-4153.29 517,-4143.55 517,-4134.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520.5,-4134.53 517,-4124.53 513.5,-4134.53 520.5,-4134.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568110416 -->\n",
       "<g class=\"node\" id=\"node182\"><title>140415568110416</title>\n",
       "<polygon fill=\"none\" points=\"667.5,-4380.5 667.5,-4416.5 822.5,-4416.5 822.5,-4380.5 667.5,-4380.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"745\" y=\"-4394.8\">activation_53: Activation</text>\n",
       "</g>\n",
       "<!-- 140415569022224&#45;&gt;140415568110416 -->\n",
       "<g class=\"edge\" id=\"edge198\"><title>140415569022224-&gt;140415568110416</title>\n",
       "<path d=\"M745,-4453.31C745,-4445.29 745,-4435.55 745,-4426.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"748.5,-4426.53 745,-4416.53 741.5,-4426.53 748.5,-4426.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415333437264 -->\n",
       "<g class=\"node\" id=\"node183\"><title>140415333437264</title>\n",
       "<polygon fill=\"none\" points=\"504.5,-4015.5 504.5,-4051.5 659.5,-4051.5 659.5,-4015.5 504.5,-4015.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-4029.8\">activation_58: Activation</text>\n",
       "</g>\n",
       "<!-- 140415334042000&#45;&gt;140415333437264 -->\n",
       "<g class=\"edge\" id=\"edge199\"><title>140415334042000-&gt;140415333437264</title>\n",
       "<path d=\"M532.735,-4088.31C540.799,-4079.5 550.759,-4068.63 559.608,-4058.96\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"562.239,-4061.27 566.41,-4051.53 557.076,-4056.54 562.239,-4061.27\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568584656 -->\n",
       "<g class=\"node\" id=\"node186\"><title>140415568584656</title>\n",
       "<polygon fill=\"none\" points=\"680,-4307.5 680,-4343.5 810,-4343.5 810,-4307.5 680,-4307.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"745\" y=\"-4321.8\">conv2d_61: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415568110416&#45;&gt;140415568584656 -->\n",
       "<g class=\"edge\" id=\"edge202\"><title>140415568110416-&gt;140415568584656</title>\n",
       "<path d=\"M745,-4380.31C745,-4372.29 745,-4362.55 745,-4353.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"748.5,-4353.53 745,-4343.53 741.5,-4353.53 748.5,-4353.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415333138832 -->\n",
       "<g class=\"node\" id=\"node187\"><title>140415333138832</title>\n",
       "<polygon fill=\"none\" points=\"545,-3942.5 545,-3978.5 675,-3978.5 675,-3942.5 545,-3942.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610\" y=\"-3956.8\">conv2d_66: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415333437264&#45;&gt;140415333138832 -->\n",
       "<g class=\"edge\" id=\"edge203\"><title>140415333437264-&gt;140415333138832</title>\n",
       "<path d=\"M588.778,-4015.31C592.011,-4007.12 595.951,-3997.12 599.555,-3987.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"602.872,-3989.12 603.284,-3978.53 596.36,-3986.55 602.872,-3989.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332539664 -->\n",
       "<g class=\"node\" id=\"node188\"><title>140415332539664</title>\n",
       "<polygon fill=\"none\" points=\"867,-4672.5 867,-4708.5 997,-4708.5 997,-4672.5 867,-4672.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"932\" y=\"-4686.8\">conv2d_67: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415332536528&#45;&gt;140415332539664 -->\n",
       "<g class=\"edge\" id=\"edge204\"><title>140415332536528-&gt;140415332539664</title>\n",
       "<path d=\"M897.893,-4745.31C903.255,-4736.85 909.827,-4726.48 915.767,-4717.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"918.81,-4718.85 921.207,-4708.53 912.897,-4715.1 918.81,-4718.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569833488 -->\n",
       "<g class=\"node\" id=\"node189\"><title>140415569833488</title>\n",
       "<polygon fill=\"none\" points=\"950,-4526.5 950,-4562.5 1216,-4562.5 1216,-4526.5 950,-4526.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1083\" y=\"-4540.8\">batch_normalization_51: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415569832592&#45;&gt;140415569833488 -->\n",
       "<g class=\"edge\" id=\"edge205\"><title>140415569832592-&gt;140415569833488</title>\n",
       "<path d=\"M1092.22,-4745.47C1090.49,-4708.01 1086.35,-4618.18 1084.26,-4572.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1087.75,-4572.58 1083.79,-4562.75 1080.76,-4572.9 1087.75,-4572.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568398736 -->\n",
       "<g class=\"node\" id=\"node190\"><title>140415568398736</title>\n",
       "<polygon fill=\"none\" points=\"613,-4234.5 613,-4270.5 879,-4270.5 879,-4234.5 613,-4234.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"746\" y=\"-4248.8\">batch_normalization_54: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415568584656&#45;&gt;140415568398736 -->\n",
       "<g class=\"edge\" id=\"edge206\"><title>140415568584656-&gt;140415568398736</title>\n",
       "<path d=\"M745.242,-4307.31C745.355,-4299.29 745.492,-4289.55 745.619,-4280.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"749.119,-4280.58 745.76,-4270.53 742.12,-4280.48 749.119,-4280.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415333139088 -->\n",
       "<g class=\"node\" id=\"node191\"><title>140415333139088</title>\n",
       "<polygon fill=\"none\" points=\"533,-3869.5 533,-3905.5 799,-3905.5 799,-3869.5 533,-3869.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"666\" y=\"-3883.8\">batch_normalization_59: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415333138832&#45;&gt;140415333139088 -->\n",
       "<g class=\"edge\" id=\"edge207\"><title>140415333138832-&gt;140415333139088</title>\n",
       "<path d=\"M623.556,-3942.31C630.366,-3933.68 638.745,-3923.06 646.255,-3913.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"649.124,-3915.55 652.569,-3905.53 643.628,-3911.21 649.124,-3915.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332598992 -->\n",
       "<g class=\"node\" id=\"node192\"><title>140415332598992</title>\n",
       "<polygon fill=\"none\" points=\"799,-4599.5 799,-4635.5 1065,-4635.5 1065,-4599.5 799,-4599.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"932\" y=\"-4613.8\">batch_normalization_60: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415332539664&#45;&gt;140415332598992 -->\n",
       "<g class=\"edge\" id=\"edge208\"><title>140415332539664-&gt;140415332598992</title>\n",
       "<path d=\"M932,-4672.31C932,-4664.29 932,-4654.55 932,-4645.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"935.5,-4645.53 932,-4635.53 928.5,-4645.53 935.5,-4645.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415569542672 -->\n",
       "<g class=\"node\" id=\"node193\"><title>140415569542672</title>\n",
       "<polygon fill=\"none\" points=\"960.5,-4161.5 960.5,-4197.5 1115.5,-4197.5 1115.5,-4161.5 960.5,-4161.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-4175.8\">activation_51: Activation</text>\n",
       "</g>\n",
       "<!-- 140415569833488&#45;&gt;140415569542672 -->\n",
       "<g class=\"edge\" id=\"edge209\"><title>140415569833488-&gt;140415569542672</title>\n",
       "<path d=\"M1080.04,-4526.27C1075.67,-4499.32 1068,-4445.5 1068,-4399.5 1068,-4399.5 1068,-4399.5 1068,-4324.5 1068,-4282.88 1055.84,-4235.89 1047,-4207.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1050.29,-4206.02 1043.93,-4197.55 1043.62,-4208.14 1050.29,-4206.02\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568322384 -->\n",
       "<g class=\"node\" id=\"node194\"><title>140415568322384</title>\n",
       "<polygon fill=\"none\" points=\"668.5,-4088.5 668.5,-4124.5 823.5,-4124.5 823.5,-4088.5 668.5,-4088.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"746\" y=\"-4102.8\">activation_54: Activation</text>\n",
       "</g>\n",
       "<!-- 140415568398736&#45;&gt;140415568322384 -->\n",
       "<g class=\"edge\" id=\"edge210\"><title>140415568398736-&gt;140415568322384</title>\n",
       "<path d=\"M746,-4234.42C746,-4209.84 746,-4164.25 746,-4134.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"749.5,-4134.56 746,-4124.56 742.5,-4134.56 749.5,-4134.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332822672 -->\n",
       "<g class=\"node\" id=\"node195\"><title>140415332822672</title>\n",
       "<polygon fill=\"none\" points=\"630.5,-3796.5 630.5,-3832.5 785.5,-3832.5 785.5,-3796.5 630.5,-3796.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"708\" y=\"-3810.8\">activation_59: Activation</text>\n",
       "</g>\n",
       "<!-- 140415333139088&#45;&gt;140415332822672 -->\n",
       "<g class=\"edge\" id=\"edge211\"><title>140415333139088-&gt;140415332822672</title>\n",
       "<path d=\"M676.167,-3869.31C681.12,-3860.94 687.179,-3850.7 692.677,-3841.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"695.848,-3842.92 697.927,-3832.53 689.823,-3839.35 695.848,-3842.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332232656 -->\n",
       "<g class=\"node\" id=\"node196\"><title>140415332232656</title>\n",
       "<polygon fill=\"none\" points=\"777.5,-4015.5 777.5,-4051.5 932.5,-4051.5 932.5,-4015.5 777.5,-4015.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855\" y=\"-4029.8\">activation_60: Activation</text>\n",
       "</g>\n",
       "<!-- 140415332598992&#45;&gt;140415332232656 -->\n",
       "<g class=\"edge\" id=\"edge212\"><title>140415332598992-&gt;140415332232656</title>\n",
       "<path d=\"M929.036,-4599.27C924.665,-4572.32 917,-4518.5 917,-4472.5 917,-4472.5 917,-4472.5 917,-4178.5 917,-4134.05 891.764,-4087.7 873.493,-4059.97\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"876.357,-4057.96 867.846,-4051.65 870.565,-4061.89 876.357,-4057.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332407120 -->\n",
       "<g class=\"node\" id=\"node197\"><title>140415332407120</title>\n",
       "<polygon fill=\"none\" points=\"715,-3723.5 715,-3759.5 847,-3759.5 847,-3723.5 715,-3723.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"781\" y=\"-3737.8\">mixed6: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415569542672&#45;&gt;140415332407120 -->\n",
       "<g class=\"edge\" id=\"edge213\"><title>140415569542672-&gt;140415332407120</title>\n",
       "<path d=\"M1031.29,-4161.42C1018.12,-4129.54 986.231,-4060.14 942,-4015 910.232,-3982.58 859.526,-4006.67 855,-3961.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415568322384&#45;&gt;140415332407120 -->\n",
       "<g class=\"edge\" id=\"edge214\"><title>140415568322384-&gt;140415332407120</title>\n",
       "<path d=\"M746.368,-4088.21C747.592,-4068.69 752.114,-4036.84 768,-4015 794.705,-3978.29 855,-4006.89 855,-3961.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M855,-3959.5C853.187,-3887.27 816.735,-3808.36 795.682,-3768.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"798.644,-3766.69 790.816,-3759.55 792.483,-3770.01 798.644,-3766.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332822672&#45;&gt;140415332407120 -->\n",
       "<g class=\"edge\" id=\"edge215\"><title>140415332822672-&gt;140415332407120</title>\n",
       "<path d=\"M725.671,-3796.31C734.817,-3787.42 746.135,-3776.41 756.147,-3766.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"758.763,-3769.01 763.491,-3759.53 753.883,-3763.99 758.763,-3769.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332232656&#45;&gt;140415332407120 -->\n",
       "<g class=\"edge\" id=\"edge216\"><title>140415332232656-&gt;140415332407120</title>\n",
       "<path d=\"M855,-4015.28C855,-4000.91 855,-3979.87 855,-3961.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330889232 -->\n",
       "<g class=\"node\" id=\"node198\"><title>140415330889232</title>\n",
       "<polygon fill=\"none\" points=\"528,-3650.5 528,-3686.5 658,-3686.5 658,-3650.5 528,-3650.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"593\" y=\"-3664.8\">conv2d_72: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415332407120&#45;&gt;140415330889232 -->\n",
       "<g class=\"edge\" id=\"edge217\"><title>140415332407120-&gt;140415330889232</title>\n",
       "<path d=\"M735.969,-3723.49C709.538,-3713.51 675.941,-3700.82 647.958,-3690.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"648.907,-3686.87 638.315,-3686.61 646.433,-3693.42 648.907,-3686.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331897296 -->\n",
       "<g class=\"node\" id=\"node204\"><title>140415331897296</title>\n",
       "<polygon fill=\"none\" points=\"716,-3650.5 716,-3686.5 846,-3686.5 846,-3650.5 716,-3650.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"781\" y=\"-3664.8\">conv2d_69: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415332407120&#45;&gt;140415331897296 -->\n",
       "<g class=\"edge\" id=\"edge223\"><title>140415332407120-&gt;140415331897296</title>\n",
       "<path d=\"M781,-3723.31C781,-3715.29 781,-3705.55 781,-3696.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"784.5,-3696.53 781,-3686.53 777.5,-3696.53 784.5,-3696.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328357776 -->\n",
       "<g class=\"node\" id=\"node216\"><title>140415328357776</title>\n",
       "<polygon fill=\"none\" points=\"829.5,-3577.5 829.5,-3613.5 1074.5,-3613.5 1074.5,-3577.5 829.5,-3577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"952\" y=\"-3591.8\">average_pooling2d_7: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415332407120&#45;&gt;140415328357776 -->\n",
       "<g class=\"edge\" id=\"edge235\"><title>140415332407120-&gt;140415328357776</title>\n",
       "<path d=\"M847.281,-3732.66C920.034,-3722.52 1029.89,-3702.18 1055,-3669.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M1055,-3667.5C1071.12,-3646.52 1048.58,-3629.43 1020.52,-3617.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1021.84,-3614.04 1011.25,-3613.51 1019.2,-3620.52 1021.84,-3614.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415574297296 -->\n",
       "<g class=\"node\" id=\"node217\"><title>140415574297296</title>\n",
       "<polygon fill=\"none\" points=\"1093,-3577.5 1093,-3613.5 1223,-3613.5 1223,-3577.5 1093,-3577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1158\" y=\"-3591.8\">conv2d_68: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415332407120&#45;&gt;140415574297296 -->\n",
       "<g class=\"edge\" id=\"edge236\"><title>140415332407120-&gt;140415574297296</title>\n",
       "<path d=\"M1055,-3667.5C1070.56,-3647.25 1093.56,-3630.62 1113.88,-3618.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1115.76,-3621.52 1122.69,-3613.51 1112.28,-3615.45 1115.76,-3621.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330889168 -->\n",
       "<g class=\"node\" id=\"node199\"><title>140415330889168</title>\n",
       "<polygon fill=\"none\" points=\"439,-3577.5 439,-3613.5 705,-3613.5 705,-3577.5 439,-3577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"572\" y=\"-3591.8\">batch_normalization_65: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415330889232&#45;&gt;140415330889168 -->\n",
       "<g class=\"edge\" id=\"edge218\"><title>140415330889232-&gt;140415330889168</title>\n",
       "<path d=\"M587.916,-3650.31C585.518,-3642.2 582.599,-3632.34 579.92,-3623.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"583.229,-3622.13 577.037,-3613.53 576.517,-3624.11 583.229,-3622.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329948432 -->\n",
       "<g class=\"node\" id=\"node200\"><title>140415329948432</title>\n",
       "<polygon fill=\"none\" points=\"474.5,-3504.5 474.5,-3540.5 629.5,-3540.5 629.5,-3504.5 474.5,-3504.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552\" y=\"-3518.8\">activation_65: Activation</text>\n",
       "</g>\n",
       "<!-- 140415330889168&#45;&gt;140415329948432 -->\n",
       "<g class=\"edge\" id=\"edge219\"><title>140415330889168-&gt;140415329948432</title>\n",
       "<path d=\"M567.159,-3577.31C564.874,-3569.2 562.095,-3559.34 559.543,-3550.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"562.877,-3549.21 556.797,-3540.53 556.139,-3551.1 562.877,-3549.21\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330289424 -->\n",
       "<g class=\"node\" id=\"node201\"><title>140415330289424</title>\n",
       "<polygon fill=\"none\" points=\"508,-3431.5 508,-3467.5 638,-3467.5 638,-3431.5 508,-3431.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"573\" y=\"-3445.8\">conv2d_73: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415329948432&#45;&gt;140415330289424 -->\n",
       "<g class=\"edge\" id=\"edge220\"><title>140415329948432-&gt;140415330289424</title>\n",
       "<path d=\"M557.084,-3504.31C559.482,-3496.2 562.401,-3486.34 565.08,-3477.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"568.483,-3478.11 567.963,-3467.53 561.771,-3476.13 568.483,-3478.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330290960 -->\n",
       "<g class=\"node\" id=\"node202\"><title>140415330290960</title>\n",
       "<polygon fill=\"none\" points=\"444,-3358.5 444,-3394.5 710,-3394.5 710,-3358.5 444,-3358.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"577\" y=\"-3372.8\">batch_normalization_66: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415330289424&#45;&gt;140415330290960 -->\n",
       "<g class=\"edge\" id=\"edge221\"><title>140415330289424-&gt;140415330290960</title>\n",
       "<path d=\"M573.968,-3431.31C574.42,-3423.29 574.969,-3413.55 575.475,-3404.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578.973,-3404.71 576.041,-3394.53 571.984,-3404.32 578.973,-3404.71\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329357392 -->\n",
       "<g class=\"node\" id=\"node203\"><title>140415329357392</title>\n",
       "<polygon fill=\"none\" points=\"502.5,-3285.5 502.5,-3321.5 657.5,-3321.5 657.5,-3285.5 502.5,-3285.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"580\" y=\"-3299.8\">activation_66: Activation</text>\n",
       "</g>\n",
       "<!-- 140415330290960&#45;&gt;140415329357392 -->\n",
       "<g class=\"edge\" id=\"edge222\"><title>140415330290960-&gt;140415329357392</title>\n",
       "<path d=\"M577.726,-3358.31C578.065,-3350.29 578.477,-3340.55 578.856,-3331.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"582.355,-3331.67 579.28,-3321.53 575.361,-3331.37 582.355,-3331.67\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329828752 -->\n",
       "<g class=\"node\" id=\"node205\"><title>140415329828752</title>\n",
       "<polygon fill=\"none\" points=\"516,-3212.5 516,-3248.5 646,-3248.5 646,-3212.5 516,-3212.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"581\" y=\"-3226.8\">conv2d_74: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415329357392&#45;&gt;140415329828752 -->\n",
       "<g class=\"edge\" id=\"edge224\"><title>140415329357392-&gt;140415329828752</title>\n",
       "<path d=\"M580.242,-3285.31C580.355,-3277.29 580.492,-3267.55 580.619,-3258.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"584.119,-3258.58 580.76,-3248.53 577.12,-3258.48 584.119,-3258.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331897168 -->\n",
       "<g class=\"node\" id=\"node206\"><title>140415331897168</title>\n",
       "<polygon fill=\"none\" points=\"648,-3504.5 648,-3540.5 914,-3540.5 914,-3504.5 648,-3504.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"781\" y=\"-3518.8\">batch_normalization_62: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415331897296&#45;&gt;140415331897168 -->\n",
       "<g class=\"edge\" id=\"edge225\"><title>140415331897296-&gt;140415331897168</title>\n",
       "<path d=\"M781,-3650.42C781,-3625.84 781,-3580.25 781,-3550.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"784.5,-3550.56 781,-3540.56 777.5,-3550.56 784.5,-3550.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329651024 -->\n",
       "<g class=\"node\" id=\"node207\"><title>140415329651024</title>\n",
       "<polygon fill=\"none\" points=\"449,-3139.5 449,-3175.5 715,-3175.5 715,-3139.5 449,-3139.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-3153.8\">batch_normalization_67: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415329828752&#45;&gt;140415329651024 -->\n",
       "<g class=\"edge\" id=\"edge226\"><title>140415329828752-&gt;140415329651024</title>\n",
       "<path d=\"M581.242,-3212.31C581.355,-3204.29 581.492,-3194.55 581.619,-3185.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"585.119,-3185.58 581.76,-3175.53 578.12,-3185.48 585.119,-3185.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331536592 -->\n",
       "<g class=\"node\" id=\"node208\"><title>140415331536592</title>\n",
       "<polygon fill=\"none\" points=\"690.5,-3431.5 690.5,-3467.5 845.5,-3467.5 845.5,-3431.5 690.5,-3431.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"768\" y=\"-3445.8\">activation_62: Activation</text>\n",
       "</g>\n",
       "<!-- 140415331897168&#45;&gt;140415331536592 -->\n",
       "<g class=\"edge\" id=\"edge227\"><title>140415331897168-&gt;140415331536592</title>\n",
       "<path d=\"M777.853,-3504.31C776.384,-3496.29 774.6,-3486.55 772.956,-3477.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"776.362,-3476.73 771.118,-3467.53 769.476,-3478 776.362,-3476.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329582864 -->\n",
       "<g class=\"node\" id=\"node209\"><title>140415329582864</title>\n",
       "<polygon fill=\"none\" points=\"504.5,-3066.5 504.5,-3102.5 659.5,-3102.5 659.5,-3066.5 504.5,-3066.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-3080.8\">activation_67: Activation</text>\n",
       "</g>\n",
       "<!-- 140415329651024&#45;&gt;140415329582864 -->\n",
       "<g class=\"edge\" id=\"edge228\"><title>140415329651024-&gt;140415329582864</title>\n",
       "<path d=\"M582,-3139.31C582,-3131.29 582,-3121.55 582,-3112.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"585.5,-3112.53 582,-3102.53 578.5,-3112.53 585.5,-3112.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331614288 -->\n",
       "<g class=\"node\" id=\"node210\"><title>140415331614288</title>\n",
       "<polygon fill=\"none\" points=\"739,-3358.5 739,-3394.5 869,-3394.5 869,-3358.5 739,-3358.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804\" y=\"-3372.8\">conv2d_70: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415331536592&#45;&gt;140415331614288 -->\n",
       "<g class=\"edge\" id=\"edge229\"><title>140415331536592-&gt;140415331614288</title>\n",
       "<path d=\"M776.715,-3431.31C780.915,-3423.03 786.045,-3412.91 790.719,-3403.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"793.965,-3405.03 795.366,-3394.53 787.722,-3401.87 793.965,-3405.03\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329284432 -->\n",
       "<g class=\"node\" id=\"node211\"><title>140415329284432</title>\n",
       "<polygon fill=\"none\" points=\"517,-2993.5 517,-3029.5 647,-3029.5 647,-2993.5 517,-2993.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-3007.8\">conv2d_75: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415329582864&#45;&gt;140415329284432 -->\n",
       "<g class=\"edge\" id=\"edge230\"><title>140415329582864-&gt;140415329284432</title>\n",
       "<path d=\"M582,-3066.31C582,-3058.29 582,-3048.55 582,-3039.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"585.5,-3039.53 582,-3029.53 578.5,-3039.53 585.5,-3039.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331611344 -->\n",
       "<g class=\"node\" id=\"node212\"><title>140415331611344</title>\n",
       "<polygon fill=\"none\" points=\"677,-3285.5 677,-3321.5 943,-3321.5 943,-3285.5 677,-3285.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-3299.8\">batch_normalization_63: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415331614288&#45;&gt;140415331611344 -->\n",
       "<g class=\"edge\" id=\"edge231\"><title>140415331614288-&gt;140415331611344</title>\n",
       "<path d=\"M805.452,-3358.31C806.13,-3350.29 806.954,-3340.55 807.712,-3331.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"811.206,-3331.79 808.561,-3321.53 804.231,-3331.2 811.206,-3331.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415329284688 -->\n",
       "<g class=\"node\" id=\"node213\"><title>140415329284688</title>\n",
       "<polygon fill=\"none\" points=\"449,-2920.5 449,-2956.5 715,-2956.5 715,-2920.5 449,-2920.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582\" y=\"-2934.8\">batch_normalization_68: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415329284432&#45;&gt;140415329284688 -->\n",
       "<g class=\"edge\" id=\"edge232\"><title>140415329284432-&gt;140415329284688</title>\n",
       "<path d=\"M582,-2993.31C582,-2985.29 582,-2975.55 582,-2966.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"585.5,-2966.53 582,-2956.53 578.5,-2966.53 585.5,-2966.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330971408 -->\n",
       "<g class=\"node\" id=\"node214\"><title>140415330971408</title>\n",
       "<polygon fill=\"none\" points=\"732.5,-3212.5 732.5,-3248.5 887.5,-3248.5 887.5,-3212.5 732.5,-3212.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-3226.8\">activation_63: Activation</text>\n",
       "</g>\n",
       "<!-- 140415331611344&#45;&gt;140415330971408 -->\n",
       "<g class=\"edge\" id=\"edge233\"><title>140415331611344-&gt;140415330971408</title>\n",
       "<path d=\"M810,-3285.31C810,-3277.29 810,-3267.55 810,-3258.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"813.5,-3258.53 810,-3248.53 806.5,-3258.53 813.5,-3258.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328968336 -->\n",
       "<g class=\"node\" id=\"node215\"><title>140415328968336</title>\n",
       "<polygon fill=\"none\" points=\"569.5,-2847.5 569.5,-2883.5 724.5,-2883.5 724.5,-2847.5 569.5,-2847.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"647\" y=\"-2861.8\">activation_68: Activation</text>\n",
       "</g>\n",
       "<!-- 140415329284688&#45;&gt;140415328968336 -->\n",
       "<g class=\"edge\" id=\"edge234\"><title>140415329284688-&gt;140415328968336</title>\n",
       "<path d=\"M597.735,-2920.31C605.799,-2911.5 615.759,-2900.63 624.608,-2890.96\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"627.239,-2893.27 631.41,-2883.53 622.076,-2888.54 627.239,-2893.27\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330681168 -->\n",
       "<g class=\"node\" id=\"node218\"><title>140415330681168</title>\n",
       "<polygon fill=\"none\" points=\"745,-3139.5 745,-3175.5 875,-3175.5 875,-3139.5 745,-3139.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-3153.8\">conv2d_71: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415330971408&#45;&gt;140415330681168 -->\n",
       "<g class=\"edge\" id=\"edge237\"><title>140415330971408-&gt;140415330681168</title>\n",
       "<path d=\"M810,-3212.31C810,-3204.29 810,-3194.55 810,-3185.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"813.5,-3185.53 810,-3175.53 806.5,-3185.53 813.5,-3185.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328669904 -->\n",
       "<g class=\"node\" id=\"node219\"><title>140415328669904</title>\n",
       "<polygon fill=\"none\" points=\"638,-2774.5 638,-2810.5 768,-2810.5 768,-2774.5 638,-2774.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-2788.8\">conv2d_76: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415328968336&#45;&gt;140415328669904 -->\n",
       "<g class=\"edge\" id=\"edge238\"><title>140415328968336-&gt;140415328669904</title>\n",
       "<path d=\"M660.556,-2847.31C667.366,-2838.68 675.745,-2828.06 683.255,-2818.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"686.124,-2820.55 689.569,-2810.53 680.628,-2816.21 686.124,-2820.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328354384 -->\n",
       "<g class=\"node\" id=\"node220\"><title>140415328354384</title>\n",
       "<polygon fill=\"none\" points=\"932,-3504.5 932,-3540.5 1062,-3540.5 1062,-3504.5 932,-3504.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"997\" y=\"-3518.8\">conv2d_77: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415328357776&#45;&gt;140415328354384 -->\n",
       "<g class=\"edge\" id=\"edge239\"><title>140415328357776-&gt;140415328354384</title>\n",
       "<path d=\"M962.893,-3577.31C968.255,-3568.85 974.827,-3558.48 980.767,-3549.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"983.81,-3550.85 986.207,-3540.53 977.897,-3547.1 983.81,-3550.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415332407568 -->\n",
       "<g class=\"node\" id=\"node221\"><title>140415332407568</title>\n",
       "<polygon fill=\"none\" points=\"1015,-3358.5 1015,-3394.5 1281,-3394.5 1281,-3358.5 1015,-3358.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1148\" y=\"-3372.8\">batch_normalization_61: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415574297296&#45;&gt;140415332407568 -->\n",
       "<g class=\"edge\" id=\"edge240\"><title>140415574297296-&gt;140415332407568</title>\n",
       "<path d=\"M1157.22,-3577.47C1155.49,-3540.01 1151.35,-3450.18 1149.26,-3404.76\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1152.75,-3404.58 1148.79,-3394.75 1145.76,-3404.9 1152.75,-3404.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330681424 -->\n",
       "<g class=\"node\" id=\"node222\"><title>140415330681424</title>\n",
       "<polygon fill=\"none\" points=\"678,-3066.5 678,-3102.5 944,-3102.5 944,-3066.5 678,-3066.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-3080.8\">batch_normalization_64: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415330681168&#45;&gt;140415330681424 -->\n",
       "<g class=\"edge\" id=\"edge241\"><title>140415330681168-&gt;140415330681424</title>\n",
       "<path d=\"M810.242,-3139.31C810.355,-3131.29 810.492,-3121.55 810.619,-3112.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"814.119,-3112.58 810.76,-3102.53 807.12,-3112.48 814.119,-3112.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328673680 -->\n",
       "<g class=\"node\" id=\"node223\"><title>140415328673680</title>\n",
       "<polygon fill=\"none\" points=\"598,-2701.5 598,-2737.5 864,-2737.5 864,-2701.5 598,-2701.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731\" y=\"-2715.8\">batch_normalization_69: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415328669904&#45;&gt;140415328673680 -->\n",
       "<g class=\"edge\" id=\"edge242\"><title>140415328669904-&gt;140415328673680</title>\n",
       "<path d=\"M709.778,-2774.31C713.011,-2766.12 716.951,-2756.12 720.555,-2746.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"723.872,-2748.12 724.284,-2737.53 717.36,-2745.55 723.872,-2748.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328138384 -->\n",
       "<g class=\"node\" id=\"node224\"><title>140415328138384</title>\n",
       "<polygon fill=\"none\" points=\"864,-3431.5 864,-3467.5 1130,-3467.5 1130,-3431.5 864,-3431.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"997\" y=\"-3445.8\">batch_normalization_70: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415328354384&#45;&gt;140415328138384 -->\n",
       "<g class=\"edge\" id=\"edge243\"><title>140415328354384-&gt;140415328138384</title>\n",
       "<path d=\"M997,-3504.31C997,-3496.29 997,-3486.55 997,-3477.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1000.5,-3477.53 997,-3467.53 993.5,-3477.53 1000.5,-3477.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415331876752 -->\n",
       "<g class=\"node\" id=\"node225\"><title>140415331876752</title>\n",
       "<polygon fill=\"none\" points=\"1025.5,-2993.5 1025.5,-3029.5 1180.5,-3029.5 1180.5,-2993.5 1025.5,-2993.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1103\" y=\"-3007.8\">activation_61: Activation</text>\n",
       "</g>\n",
       "<!-- 140415332407568&#45;&gt;140415331876752 -->\n",
       "<g class=\"edge\" id=\"edge244\"><title>140415332407568-&gt;140415331876752</title>\n",
       "<path d=\"M1145.04,-3358.27C1140.67,-3331.32 1133,-3277.5 1133,-3231.5 1133,-3231.5 1133,-3231.5 1133,-3156.5 1133,-3114.88 1120.84,-3067.89 1112,-3039.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1115.29,-3038.02 1108.93,-3029.55 1108.62,-3040.14 1115.29,-3038.02\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330606288 -->\n",
       "<g class=\"node\" id=\"node226\"><title>140415330606288</title>\n",
       "<polygon fill=\"none\" points=\"733.5,-2920.5 733.5,-2956.5 888.5,-2956.5 888.5,-2920.5 733.5,-2920.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-2934.8\">activation_64: Activation</text>\n",
       "</g>\n",
       "<!-- 140415330681424&#45;&gt;140415330606288 -->\n",
       "<g class=\"edge\" id=\"edge245\"><title>140415330681424-&gt;140415330606288</title>\n",
       "<path d=\"M811,-3066.42C811,-3041.84 811,-2996.25 811,-2966.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"814.5,-2966.56 811,-2956.56 807.5,-2966.56 814.5,-2966.56\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328078928 -->\n",
       "<g class=\"node\" id=\"node227\"><title>140415328078928</title>\n",
       "<polygon fill=\"none\" points=\"695.5,-2628.5 695.5,-2664.5 850.5,-2664.5 850.5,-2628.5 695.5,-2628.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"773\" y=\"-2642.8\">activation_69: Activation</text>\n",
       "</g>\n",
       "<!-- 140415328673680&#45;&gt;140415328078928 -->\n",
       "<g class=\"edge\" id=\"edge246\"><title>140415328673680-&gt;140415328078928</title>\n",
       "<path d=\"M741.167,-2701.31C746.12,-2692.94 752.179,-2682.7 757.677,-2673.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"760.848,-2674.92 762.927,-2664.53 754.823,-2671.35 760.848,-2674.92\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327942288 -->\n",
       "<g class=\"node\" id=\"node228\"><title>140415327942288</title>\n",
       "<polygon fill=\"none\" points=\"842.5,-2847.5 842.5,-2883.5 997.5,-2883.5 997.5,-2847.5 842.5,-2847.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920\" y=\"-2861.8\">activation_70: Activation</text>\n",
       "</g>\n",
       "<!-- 140415328138384&#45;&gt;140415327942288 -->\n",
       "<g class=\"edge\" id=\"edge247\"><title>140415328138384-&gt;140415327942288</title>\n",
       "<path d=\"M994.036,-3431.27C989.665,-3404.32 982,-3350.5 982,-3304.5 982,-3304.5 982,-3304.5 982,-3010.5 982,-2966.05 956.764,-2919.7 938.493,-2891.97\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"941.357,-2889.96 932.846,-2883.65 935.565,-2893.89 941.357,-2889.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328279504 -->\n",
       "<g class=\"node\" id=\"node229\"><title>140415328279504</title>\n",
       "<polygon fill=\"none\" points=\"780,-2555.5 780,-2591.5 912,-2591.5 912,-2555.5 780,-2555.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"846\" y=\"-2569.8\">mixed7: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415331876752&#45;&gt;140415328279504 -->\n",
       "<g class=\"edge\" id=\"edge248\"><title>140415331876752-&gt;140415328279504</title>\n",
       "<path d=\"M1096.29,-2993.42C1083.12,-2961.54 1051.23,-2892.14 1007,-2847 975.232,-2814.58 924.526,-2838.67 920,-2793.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415330606288&#45;&gt;140415328279504 -->\n",
       "<g class=\"edge\" id=\"edge249\"><title>140415330606288-&gt;140415328279504</title>\n",
       "<path d=\"M811.368,-2920.21C812.592,-2900.69 817.114,-2868.84 833,-2847 859.705,-2810.29 920,-2838.89 920,-2793.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M920,-2791.5C918.187,-2719.27 881.735,-2640.36 860.682,-2600.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"863.644,-2598.69 855.816,-2591.55 857.483,-2602.01 863.644,-2598.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415328078928&#45;&gt;140415328279504 -->\n",
       "<g class=\"edge\" id=\"edge250\"><title>140415328078928-&gt;140415328279504</title>\n",
       "<path d=\"M790.671,-2628.31C799.817,-2619.42 811.135,-2608.41 821.147,-2598.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"823.763,-2601.01 828.491,-2591.53 818.883,-2595.99 823.763,-2601.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327942288&#45;&gt;140415328279504 -->\n",
       "<g class=\"edge\" id=\"edge251\"><title>140415327942288-&gt;140415328279504</title>\n",
       "<path d=\"M920,-2847.28C920,-2832.91 920,-2811.87 920,-2793.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327150608 -->\n",
       "<g class=\"node\" id=\"node230\"><title>140415327150608</title>\n",
       "<polygon fill=\"none\" points=\"586,-2482.5 586,-2518.5 716,-2518.5 716,-2482.5 586,-2482.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651\" y=\"-2496.8\">conv2d_80: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415328279504&#45;&gt;140415327150608 -->\n",
       "<g class=\"edge\" id=\"edge252\"><title>140415328279504-&gt;140415327150608</title>\n",
       "<path d=\"M799.293,-2555.49C771.758,-2545.47 736.725,-2532.71 707.624,-2522.12\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"708.596,-2518.75 698.002,-2518.61 706.201,-2525.32 708.596,-2518.75\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327871504 -->\n",
       "<g class=\"node\" id=\"node236\"><title>140415327871504</title>\n",
       "<polygon fill=\"none\" points=\"729,-2409.5 729,-2445.5 859,-2445.5 859,-2409.5 729,-2409.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"794\" y=\"-2423.8\">conv2d_78: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415328279504&#45;&gt;140415327871504 -->\n",
       "<g class=\"edge\" id=\"edge258\"><title>140415328279504-&gt;140415327871504</title>\n",
       "<path d=\"M881.052,-2555.31C900.109,-2543.28 920.018,-2525.15 921,-2501.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M921,-2499.5C922.196,-2470.7 897.096,-2453.47 868.904,-2443.22\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"869.852,-2439.84 859.258,-2440 867.637,-2446.48 869.852,-2439.84\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325221456 -->\n",
       "<g class=\"node\" id=\"node248\"><title>140415325221456</title>\n",
       "<polygon fill=\"none\" points=\"788.5,-1971.5 788.5,-2007.5 995.5,-2007.5 995.5,-1971.5 788.5,-1971.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"892\" y=\"-1985.8\">max_pooling2d_7: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140415328279504&#45;&gt;140415325221456 -->\n",
       "<g class=\"edge\" id=\"edge270\"><title>140415328279504-&gt;140415325221456</title>\n",
       "<path d=\"M921,-2499.5C923.655,-2435.56 921,-2419.5 921,-2355.5 921,-2355.5 921,-2355.5 921,-2134.5 921,-2092.94 909.245,-2045.93 900.703,-2017.26\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"904.01,-2016.1 897.729,-2007.57 897.318,-2018.15 904.01,-2016.1\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327152848 -->\n",
       "<g class=\"node\" id=\"node231\"><title>140415327152848</title>\n",
       "<polygon fill=\"none\" points=\"421,-2409.5 421,-2445.5 687,-2445.5 687,-2409.5 421,-2409.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554\" y=\"-2423.8\">batch_normalization_73: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415327150608&#45;&gt;140415327152848 -->\n",
       "<g class=\"edge\" id=\"edge253\"><title>140415327150608-&gt;140415327152848</title>\n",
       "<path d=\"M627.519,-2482.31C614.889,-2473.07 599.143,-2461.54 585.466,-2451.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"587.401,-2448.61 577.265,-2445.53 583.267,-2454.26 587.401,-2448.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326461264 -->\n",
       "<g class=\"node\" id=\"node232\"><title>140415326461264</title>\n",
       "<polygon fill=\"none\" points=\"453.5,-2336.5 453.5,-2372.5 608.5,-2372.5 608.5,-2336.5 453.5,-2336.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531\" y=\"-2350.8\">activation_73: Activation</text>\n",
       "</g>\n",
       "<!-- 140415327152848&#45;&gt;140415326461264 -->\n",
       "<g class=\"edge\" id=\"edge254\"><title>140415327152848-&gt;140415326461264</title>\n",
       "<path d=\"M548.432,-2409.31C545.805,-2401.2 542.609,-2391.34 539.674,-2382.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"542.928,-2380.96 536.516,-2372.53 536.269,-2383.12 542.928,-2380.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326551248 -->\n",
       "<g class=\"node\" id=\"node233\"><title>140415326551248</title>\n",
       "<polygon fill=\"none\" points=\"466,-2263.5 466,-2299.5 596,-2299.5 596,-2263.5 466,-2263.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531\" y=\"-2277.8\">conv2d_81: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415326461264&#45;&gt;140415326551248 -->\n",
       "<g class=\"edge\" id=\"edge255\"><title>140415326461264-&gt;140415326551248</title>\n",
       "<path d=\"M531,-2336.31C531,-2328.29 531,-2318.55 531,-2309.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"534.5,-2309.53 531,-2299.53 527.5,-2309.53 534.5,-2309.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326548304 -->\n",
       "<g class=\"node\" id=\"node234\"><title>140415326548304</title>\n",
       "<polygon fill=\"none\" points=\"398,-2190.5 398,-2226.5 664,-2226.5 664,-2190.5 398,-2190.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531\" y=\"-2204.8\">batch_normalization_74: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415326551248&#45;&gt;140415326548304 -->\n",
       "<g class=\"edge\" id=\"edge256\"><title>140415326551248-&gt;140415326548304</title>\n",
       "<path d=\"M531,-2263.31C531,-2255.29 531,-2245.55 531,-2236.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"534.5,-2236.53 531,-2226.53 527.5,-2236.53 534.5,-2236.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326101392 -->\n",
       "<g class=\"node\" id=\"node235\"><title>140415326101392</title>\n",
       "<polygon fill=\"none\" points=\"453.5,-2117.5 453.5,-2153.5 608.5,-2153.5 608.5,-2117.5 453.5,-2117.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531\" y=\"-2131.8\">activation_74: Activation</text>\n",
       "</g>\n",
       "<!-- 140415326548304&#45;&gt;140415326101392 -->\n",
       "<g class=\"edge\" id=\"edge257\"><title>140415326548304-&gt;140415326101392</title>\n",
       "<path d=\"M531,-2190.31C531,-2182.29 531,-2172.55 531,-2163.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"534.5,-2163.53 531,-2153.53 527.5,-2163.53 534.5,-2163.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326434960 -->\n",
       "<g class=\"node\" id=\"node237\"><title>140415326434960</title>\n",
       "<polygon fill=\"none\" points=\"483,-2044.5 483,-2080.5 613,-2080.5 613,-2044.5 483,-2044.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"548\" y=\"-2058.8\">conv2d_82: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415326101392&#45;&gt;140415326434960 -->\n",
       "<g class=\"edge\" id=\"edge259\"><title>140415326101392-&gt;140415326434960</title>\n",
       "<path d=\"M535.115,-2117.31C537.057,-2109.2 539.42,-2099.34 541.589,-2090.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"544.998,-2091.07 543.923,-2080.53 538.19,-2089.44 544.998,-2091.07\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327874896 -->\n",
       "<g class=\"node\" id=\"node238\"><title>140415327874896</title>\n",
       "<polygon fill=\"none\" points=\"627,-2336.5 627,-2372.5 893,-2372.5 893,-2336.5 627,-2336.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"760\" y=\"-2350.8\">batch_normalization_71: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415327871504&#45;&gt;140415327874896 -->\n",
       "<g class=\"edge\" id=\"edge260\"><title>140415327871504-&gt;140415327874896</title>\n",
       "<path d=\"M785.77,-2409.31C781.802,-2401.03 776.957,-2390.91 772.543,-2381.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"775.63,-2380.04 768.155,-2372.53 769.317,-2383.06 775.63,-2380.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415326437008 -->\n",
       "<g class=\"node\" id=\"node239\"><title>140415326437008</title>\n",
       "<polygon fill=\"none\" points=\"432,-1971.5 432,-2007.5 698,-2007.5 698,-1971.5 432,-1971.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"565\" y=\"-1985.8\">batch_normalization_75: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415326434960&#45;&gt;140415326437008 -->\n",
       "<g class=\"edge\" id=\"edge261\"><title>140415326434960-&gt;140415326437008</title>\n",
       "<path d=\"M552.115,-2044.31C554.057,-2036.2 556.42,-2026.34 558.589,-2017.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"561.998,-2018.07 560.923,-2007.53 555.19,-2016.44 561.998,-2018.07\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327640528 -->\n",
       "<g class=\"node\" id=\"node240\"><title>140415327640528</title>\n",
       "<polygon fill=\"none\" points=\"682.5,-2263.5 682.5,-2299.5 837.5,-2299.5 837.5,-2263.5 682.5,-2263.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"760\" y=\"-2277.8\">activation_71: Activation</text>\n",
       "</g>\n",
       "<!-- 140415327874896&#45;&gt;140415327640528 -->\n",
       "<g class=\"edge\" id=\"edge262\"><title>140415327874896-&gt;140415327640528</title>\n",
       "<path d=\"M760,-2336.31C760,-2328.29 760,-2318.55 760,-2309.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"763.5,-2309.53 760,-2299.53 756.5,-2309.53 763.5,-2309.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325761296 -->\n",
       "<g class=\"node\" id=\"node241\"><title>140415325761296</title>\n",
       "<polygon fill=\"none\" points=\"508.5,-1898.5 508.5,-1934.5 663.5,-1934.5 663.5,-1898.5 508.5,-1898.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"586\" y=\"-1912.8\">activation_75: Activation</text>\n",
       "</g>\n",
       "<!-- 140415326437008&#45;&gt;140415325761296 -->\n",
       "<g class=\"edge\" id=\"edge263\"><title>140415326437008-&gt;140415325761296</title>\n",
       "<path d=\"M570.084,-1971.31C572.482,-1963.2 575.401,-1953.34 578.08,-1944.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"581.483,-1945.11 580.963,-1934.53 574.771,-1943.13 581.483,-1945.11\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327416144 -->\n",
       "<g class=\"node\" id=\"node242\"><title>140415327416144</title>\n",
       "<polygon fill=\"none\" points=\"695,-2190.5 695,-2226.5 825,-2226.5 825,-2190.5 695,-2190.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"760\" y=\"-2204.8\">conv2d_79: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415327640528&#45;&gt;140415327416144 -->\n",
       "<g class=\"edge\" id=\"edge264\"><title>140415327640528-&gt;140415327416144</title>\n",
       "<path d=\"M760,-2263.31C760,-2255.29 760,-2245.55 760,-2236.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"763.5,-2236.53 760,-2226.53 756.5,-2236.53 763.5,-2236.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325836048 -->\n",
       "<g class=\"node\" id=\"node243\"><title>140415325836048</title>\n",
       "<polygon fill=\"none\" points=\"604,-1825.5 604,-1861.5 734,-1861.5 734,-1825.5 604,-1825.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"669\" y=\"-1839.8\">conv2d_83: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415325761296&#45;&gt;140415325836048 -->\n",
       "<g class=\"edge\" id=\"edge265\"><title>140415325761296-&gt;140415325836048</title>\n",
       "<path d=\"M606.092,-1898.31C616.695,-1889.24 629.865,-1877.98 641.411,-1868.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"643.769,-1870.69 649.093,-1861.53 639.219,-1865.37 643.769,-1870.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327415952 -->\n",
       "<g class=\"node\" id=\"node244\"><title>140415327415952</title>\n",
       "<polygon fill=\"none\" points=\"627,-2117.5 627,-2153.5 893,-2153.5 893,-2117.5 627,-2117.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"760\" y=\"-2131.8\">batch_normalization_72: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415327416144&#45;&gt;140415327415952 -->\n",
       "<g class=\"edge\" id=\"edge266\"><title>140415327416144-&gt;140415327415952</title>\n",
       "<path d=\"M760,-2190.31C760,-2182.29 760,-2172.55 760,-2163.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"763.5,-2163.53 760,-2153.53 756.5,-2163.53 763.5,-2163.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325838288 -->\n",
       "<g class=\"node\" id=\"node245\"><title>140415325838288</title>\n",
       "<polygon fill=\"none\" points=\"577,-1752.5 577,-1788.5 843,-1788.5 843,-1752.5 577,-1752.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"710\" y=\"-1766.8\">batch_normalization_76: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415325836048&#45;&gt;140415325838288 -->\n",
       "<g class=\"edge\" id=\"edge267\"><title>140415325836048-&gt;140415325838288</title>\n",
       "<path d=\"M678.925,-1825.31C683.76,-1816.94 689.675,-1806.7 695.042,-1797.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"698.197,-1798.94 700.166,-1788.53 692.135,-1795.44 698.197,-1798.94\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415327055376 -->\n",
       "<g class=\"node\" id=\"node246\"><title>140415327055376</title>\n",
       "<polygon fill=\"none\" points=\"682.5,-2044.5 682.5,-2080.5 837.5,-2080.5 837.5,-2044.5 682.5,-2044.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"760\" y=\"-2058.8\">activation_72: Activation</text>\n",
       "</g>\n",
       "<!-- 140415327415952&#45;&gt;140415327055376 -->\n",
       "<g class=\"edge\" id=\"edge268\"><title>140415327415952-&gt;140415327055376</title>\n",
       "<path d=\"M760,-2117.31C760,-2109.29 760,-2099.55 760,-2090.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"763.5,-2090.53 760,-2080.53 756.5,-2090.53 763.5,-2090.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325150800 -->\n",
       "<g class=\"node\" id=\"node247\"><title>140415325150800</title>\n",
       "<polygon fill=\"none\" points=\"670.5,-1679.5 670.5,-1715.5 825.5,-1715.5 825.5,-1679.5 670.5,-1679.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"748\" y=\"-1693.8\">activation_76: Activation</text>\n",
       "</g>\n",
       "<!-- 140415325838288&#45;&gt;140415325150800 -->\n",
       "<g class=\"edge\" id=\"edge269\"><title>140415325838288-&gt;140415325150800</title>\n",
       "<path d=\"M719.199,-1752.31C723.68,-1743.94 729.162,-1733.7 734.137,-1724.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"737.253,-1726 738.886,-1715.53 731.081,-1722.69 737.253,-1726\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325223824 -->\n",
       "<g class=\"node\" id=\"node249\"><title>140415325223824</title>\n",
       "<polygon fill=\"none\" points=\"754,-1606.5 754,-1642.5 886,-1642.5 886,-1606.5 754,-1606.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"820\" y=\"-1620.8\">mixed8: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415327055376&#45;&gt;140415325223824 -->\n",
       "<g class=\"edge\" id=\"edge271\"><title>140415327055376-&gt;140415325223824</title>\n",
       "<path d=\"M759.271,-2044.28C759.335,-2024.53 762.298,-1992.18 779,-1971 813.409,-1927.37 872.034,-1969.36 892,-1917.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M892,-1915.5C926.009,-1820.56 866.848,-1702.58 836.408,-1651.3\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"839.263,-1649.26 831.087,-1642.52 833.277,-1652.89 839.263,-1649.26\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325150800&#45;&gt;140415325223824 -->\n",
       "<g class=\"edge\" id=\"edge272\"><title>140415325150800-&gt;140415325223824</title>\n",
       "<path d=\"M765.429,-1679.31C774.45,-1670.42 785.613,-1659.41 795.488,-1649.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"798.068,-1652.04 802.731,-1642.53 793.153,-1647.06 798.068,-1652.04\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415325221456&#45;&gt;140415325223824 -->\n",
       "<g class=\"edge\" id=\"edge273\"><title>140415325221456-&gt;140415325223824</title>\n",
       "<path d=\"M888.625,-1971.23C886.514,-1956.45 885.312,-1934.87 892,-1917.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323436112 -->\n",
       "<g class=\"node\" id=\"node250\"><title>140415323436112</title>\n",
       "<polygon fill=\"none\" points=\"320,-1533.5 320,-1569.5 450,-1569.5 450,-1533.5 320,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"385\" y=\"-1547.8\">conv2d_88: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415325223824&#45;&gt;140415323436112 -->\n",
       "<g class=\"edge\" id=\"edge274\"><title>140415325223824-&gt;140415323436112</title>\n",
       "<path d=\"M753.933,-1612.72C674.99,-1599.83 543.209,-1578.32 460.367,-1564.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"460.697,-1561.31 450.264,-1563.15 459.569,-1568.22 460.697,-1561.31\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324364944 -->\n",
       "<g class=\"node\" id=\"node253\"><title>140415324364944</title>\n",
       "<polygon fill=\"none\" points=\"647,-1533.5 647,-1569.5 777,-1569.5 777,-1533.5 647,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"712\" y=\"-1547.8\">conv2d_85: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415325223824&#45;&gt;140415324364944 -->\n",
       "<g class=\"edge\" id=\"edge277\"><title>140415325223824-&gt;140415324364944</title>\n",
       "<path d=\"M794.131,-1606.49C779.942,-1597.17 762.158,-1585.47 746.77,-1575.36\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"748.311,-1572.18 738.032,-1569.61 744.465,-1578.03 748.311,-1572.18\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321475664 -->\n",
       "<g class=\"node\" id=\"node263\"><title>140415321475664</title>\n",
       "<polygon fill=\"none\" points=\"806.5,-1533.5 806.5,-1569.5 1051.5,-1569.5 1051.5,-1533.5 806.5,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"929\" y=\"-1547.8\">average_pooling2d_8: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415325223824&#45;&gt;140415321475664 -->\n",
       "<g class=\"edge\" id=\"edge287\"><title>140415325223824-&gt;140415321475664</title>\n",
       "<path d=\"M846.108,-1606.49C860.563,-1597.08 878.713,-1585.26 894.343,-1575.07\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"896.258,-1578 902.727,-1569.61 892.438,-1572.14 896.258,-1578\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324996048 -->\n",
       "<g class=\"node\" id=\"node264\"><title>140415324996048</title>\n",
       "<polygon fill=\"none\" points=\"1028,-1460.5 1028,-1496.5 1158,-1496.5 1158,-1460.5 1028,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093\" y=\"-1474.8\">conv2d_84: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415325223824&#45;&gt;140415324996048 -->\n",
       "<g class=\"edge\" id=\"edge288\"><title>140415325223824-&gt;140415324996048</title>\n",
       "<path d=\"M886.008,-1616.08C948.154,-1607.85 1035.41,-1592.66 1061,-1570 1079.17,-1553.91 1087.02,-1526.85 1090.42,-1506.49\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1093.89,-1506.95 1091.82,-1496.56 1086.96,-1505.97 1093.89,-1506.95\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323435216 -->\n",
       "<g class=\"node\" id=\"node251\"><title>140415323435216</title>\n",
       "<polygon fill=\"none\" points=\"182,-1460.5 182,-1496.5 448,-1496.5 448,-1460.5 182,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1474.8\">batch_normalization_81: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415323436112&#45;&gt;140415323435216 -->\n",
       "<g class=\"edge\" id=\"edge275\"><title>140415323436112-&gt;140415323435216</title>\n",
       "<path d=\"M368.055,-1533.31C359.285,-1524.42 348.432,-1513.41 338.831,-1503.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"341.302,-1501.19 331.789,-1496.53 336.317,-1506.11 341.302,-1501.19\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323214608 -->\n",
       "<g class=\"node\" id=\"node252\"><title>140415323214608</title>\n",
       "<polygon fill=\"none\" points=\"209.5,-1387.5 209.5,-1423.5 364.5,-1423.5 364.5,-1387.5 209.5,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287\" y=\"-1401.8\">activation_81: Activation</text>\n",
       "</g>\n",
       "<!-- 140415323435216&#45;&gt;140415323214608 -->\n",
       "<g class=\"edge\" id=\"edge276\"><title>140415323435216-&gt;140415323214608</title>\n",
       "<path d=\"M308.222,-1460.31C304.989,-1452.12 301.049,-1442.12 297.445,-1432.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"300.64,-1431.55 293.716,-1423.53 294.128,-1434.12 300.64,-1431.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322878352 -->\n",
       "<g class=\"node\" id=\"node254\"><title>140415322878352</title>\n",
       "<polygon fill=\"none\" points=\"167,-1314.5 167,-1350.5 297,-1350.5 297,-1314.5 167,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232\" y=\"-1328.8\">conv2d_89: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415323214608&#45;&gt;140415322878352 -->\n",
       "<g class=\"edge\" id=\"edge278\"><title>140415323214608-&gt;140415322878352</title>\n",
       "<path d=\"M273.686,-1387.31C266.998,-1378.68 258.768,-1368.06 251.393,-1358.53\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254.082,-1356.29 245.191,-1350.53 248.548,-1360.58 254.082,-1356.29\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324368784 -->\n",
       "<g class=\"node\" id=\"node255\"><title>140415324368784</title>\n",
       "<polygon fill=\"none\" points=\"574,-1460.5 574,-1496.5 840,-1496.5 840,-1460.5 574,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"707\" y=\"-1474.8\">batch_normalization_78: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415324364944&#45;&gt;140415324368784 -->\n",
       "<g class=\"edge\" id=\"edge279\"><title>140415324364944-&gt;140415324368784</title>\n",
       "<path d=\"M710.79,-1533.31C710.225,-1525.29 709.539,-1515.55 708.906,-1506.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"712.393,-1506.26 708.199,-1496.53 705.41,-1506.75 712.393,-1506.26\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322879824 -->\n",
       "<g class=\"node\" id=\"node256\"><title>140415322879824</title>\n",
       "<polygon fill=\"none\" points=\"71,-1241.5 71,-1277.5 337,-1277.5 337,-1241.5 71,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204\" y=\"-1255.8\">batch_normalization_82: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415322878352&#45;&gt;140415322879824 -->\n",
       "<g class=\"edge\" id=\"edge280\"><title>140415322878352-&gt;140415322879824</title>\n",
       "<path d=\"M225.222,-1314.31C221.989,-1306.12 218.049,-1296.12 214.445,-1286.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"217.64,-1285.55 210.716,-1277.53 211.128,-1288.12 217.64,-1285.55\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324218064 -->\n",
       "<g class=\"node\" id=\"node257\"><title>140415324218064</title>\n",
       "<polygon fill=\"none\" points=\"626.5,-1387.5 626.5,-1423.5 781.5,-1423.5 781.5,-1387.5 626.5,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704\" y=\"-1401.8\">activation_78: Activation</text>\n",
       "</g>\n",
       "<!-- 140415324368784&#45;&gt;140415324218064 -->\n",
       "<g class=\"edge\" id=\"edge281\"><title>140415324368784-&gt;140415324218064</title>\n",
       "<path d=\"M706.274,-1460.31C705.935,-1452.29 705.523,-1442.55 705.144,-1433.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"708.639,-1433.37 704.72,-1423.53 701.645,-1433.67 708.639,-1433.37\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322598736 -->\n",
       "<g class=\"node\" id=\"node258\"><title>140415322598736</title>\n",
       "<polygon fill=\"none\" points=\"265.5,-1168.5 265.5,-1204.5 420.5,-1204.5 420.5,-1168.5 265.5,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"343\" y=\"-1182.8\">activation_82: Activation</text>\n",
       "</g>\n",
       "<!-- 140415322879824&#45;&gt;140415322598736 -->\n",
       "<g class=\"edge\" id=\"edge282\"><title>140415322879824-&gt;140415322598736</title>\n",
       "<path d=\"M237.294,-1241.49C256.239,-1231.82 280.163,-1219.6 300.458,-1209.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"302.183,-1212.28 309.496,-1204.61 298.998,-1206.05 302.183,-1212.28\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324563152 -->\n",
       "<g class=\"node\" id=\"node259\"><title>140415324563152</title>\n",
       "<polygon fill=\"none\" points=\"474,-1314.5 474,-1350.5 604,-1350.5 604,-1314.5 474,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"539\" y=\"-1328.8\">conv2d_86: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415324218064&#45;&gt;140415324563152 -->\n",
       "<g class=\"edge\" id=\"edge283\"><title>140415324218064-&gt;140415324563152</title>\n",
       "<path d=\"M664.478,-1387.49C641.585,-1377.64 612.566,-1365.16 588.202,-1354.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"589.34,-1351.35 578.771,-1350.61 586.573,-1357.78 589.34,-1351.35\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323935120 -->\n",
       "<g class=\"node\" id=\"node260\"><title>140415323935120</title>\n",
       "<polygon fill=\"none\" points=\"673,-1314.5 673,-1350.5 803,-1350.5 803,-1314.5 673,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"738\" y=\"-1328.8\">conv2d_87: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415324218064&#45;&gt;140415323935120 -->\n",
       "<g class=\"edge\" id=\"edge284\"><title>140415324218064-&gt;140415323935120</title>\n",
       "<path d=\"M712.23,-1387.31C716.198,-1379.03 721.043,-1368.91 725.457,-1359.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"728.683,-1361.06 729.845,-1350.53 722.37,-1358.04 728.683,-1361.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322761488 -->\n",
       "<g class=\"node\" id=\"node261\"><title>140415322761488</title>\n",
       "<polygon fill=\"none\" points=\"278,-1095.5 278,-1131.5 408,-1131.5 408,-1095.5 278,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"343\" y=\"-1109.8\">conv2d_90: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415322598736&#45;&gt;140415322761488 -->\n",
       "<g class=\"edge\" id=\"edge285\"><title>140415322598736-&gt;140415322761488</title>\n",
       "<path d=\"M343,-1168.31C343,-1160.29 343,-1150.55 343,-1141.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"346.5,-1141.53 343,-1131.53 339.5,-1141.53 346.5,-1141.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322159248 -->\n",
       "<g class=\"node\" id=\"node262\"><title>140415322159248</title>\n",
       "<polygon fill=\"none\" points=\"549,-1095.5 549,-1131.5 679,-1131.5 679,-1095.5 549,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614\" y=\"-1109.8\">conv2d_91: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415322598736&#45;&gt;140415322159248 -->\n",
       "<g class=\"edge\" id=\"edge286\"><title>140415322598736-&gt;140415322159248</title>\n",
       "<path d=\"M407.911,-1168.49C447.342,-1158.16 497.837,-1144.93 538.976,-1134.16\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"539.893,-1137.53 548.679,-1131.61 538.119,-1130.76 539.893,-1137.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324564688 -->\n",
       "<g class=\"node\" id=\"node265\"><title>140415324564688</title>\n",
       "<polygon fill=\"none\" points=\"355,-1241.5 355,-1277.5 621,-1277.5 621,-1241.5 355,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-1255.8\">batch_normalization_79: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415324563152&#45;&gt;140415324564688 -->\n",
       "<g class=\"edge\" id=\"edge289\"><title>140415324563152-&gt;140415324564688</title>\n",
       "<path d=\"M526.654,-1314.31C520.515,-1305.77 512.975,-1295.27 506.19,-1285.82\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"508.909,-1283.61 500.232,-1277.53 503.223,-1287.69 508.909,-1283.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323937232 -->\n",
       "<g class=\"node\" id=\"node266\"><title>140415323937232</title>\n",
       "<polygon fill=\"none\" points=\"639,-1241.5 639,-1277.5 905,-1277.5 905,-1241.5 639,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772\" y=\"-1255.8\">batch_normalization_80: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415323935120&#45;&gt;140415323937232 -->\n",
       "<g class=\"edge\" id=\"edge290\"><title>140415323935120-&gt;140415323937232</title>\n",
       "<path d=\"M746.23,-1314.31C750.198,-1306.03 755.043,-1295.91 759.457,-1286.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"762.683,-1288.06 763.845,-1277.53 756.37,-1285.04 762.683,-1288.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322670672 -->\n",
       "<g class=\"node\" id=\"node267\"><title>140415322670672</title>\n",
       "<polygon fill=\"none\" points=\"210,-1022.5 210,-1058.5 476,-1058.5 476,-1022.5 210,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"343\" y=\"-1036.8\">batch_normalization_83: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415322761488&#45;&gt;140415322670672 -->\n",
       "<g class=\"edge\" id=\"edge291\"><title>140415322761488-&gt;140415322670672</title>\n",
       "<path d=\"M343,-1095.31C343,-1087.29 343,-1077.55 343,-1068.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"346.5,-1068.53 343,-1058.53 339.5,-1068.53 346.5,-1068.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322064336 -->\n",
       "<g class=\"node\" id=\"node268\"><title>140415322064336</title>\n",
       "<polygon fill=\"none\" points=\"494,-1022.5 494,-1058.5 760,-1058.5 760,-1022.5 494,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-1036.8\">batch_normalization_84: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415322159248&#45;&gt;140415322064336 -->\n",
       "<g class=\"edge\" id=\"edge292\"><title>140415322159248-&gt;140415322064336</title>\n",
       "<path d=\"M617.147,-1095.31C618.616,-1087.29 620.4,-1077.55 622.044,-1068.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"625.524,-1069 623.882,-1058.53 618.638,-1067.73 625.524,-1069\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321661392 -->\n",
       "<g class=\"node\" id=\"node269\"><title>140415321661392</title>\n",
       "<polygon fill=\"none\" points=\"866,-1460.5 866,-1496.5 996,-1496.5 996,-1460.5 866,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"931\" y=\"-1474.8\">conv2d_92: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415321475664&#45;&gt;140415321661392 -->\n",
       "<g class=\"edge\" id=\"edge293\"><title>140415321475664-&gt;140415321661392</title>\n",
       "<path d=\"M929.484,-1533.31C929.71,-1525.29 929.985,-1515.55 930.237,-1506.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"933.737,-1506.62 930.52,-1496.53 926.74,-1506.43 933.737,-1506.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324996304 -->\n",
       "<g class=\"node\" id=\"node270\"><title>140415324996304</title>\n",
       "<polygon fill=\"none\" points=\"961,-1022.5 961,-1058.5 1227,-1058.5 1227,-1022.5 961,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-1036.8\">batch_normalization_77: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415324996048&#45;&gt;140415324996304 -->\n",
       "<g class=\"edge\" id=\"edge294\"><title>140415324996048-&gt;140415324996304</title>\n",
       "<path d=\"M1093.2,-1460.21C1093.49,-1433.18 1094,-1379.25 1094,-1333.5 1094,-1333.5 1094,-1333.5 1094,-1185.5 1094,-1144.93 1094,-1097.94 1094,-1068.96\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1097.5,-1068.79 1094,-1058.79 1090.5,-1068.79 1097.5,-1068.79\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323648912 -->\n",
       "<g class=\"node\" id=\"node271\"><title>140415323648912</title>\n",
       "<polygon fill=\"none\" points=\"485.5,-1168.5 485.5,-1204.5 640.5,-1204.5 640.5,-1168.5 485.5,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"563\" y=\"-1182.8\">activation_79: Activation</text>\n",
       "</g>\n",
       "<!-- 140415324564688&#45;&gt;140415323648912 -->\n",
       "<g class=\"edge\" id=\"edge295\"><title>140415324564688-&gt;140415323648912</title>\n",
       "<path d=\"M506.155,-1241.31C515.552,-1232.42 527.18,-1221.41 537.467,-1211.67\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"540.156,-1213.95 545.012,-1204.53 535.344,-1208.86 540.156,-1213.95\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323510032 -->\n",
       "<g class=\"node\" id=\"node272\"><title>140415323510032</title>\n",
       "<polygon fill=\"none\" points=\"694.5,-1168.5 694.5,-1204.5 849.5,-1204.5 849.5,-1168.5 694.5,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772\" y=\"-1182.8\">activation_80: Activation</text>\n",
       "</g>\n",
       "<!-- 140415323937232&#45;&gt;140415323510032 -->\n",
       "<g class=\"edge\" id=\"edge296\"><title>140415323937232-&gt;140415323510032</title>\n",
       "<path d=\"M772,-1241.31C772,-1233.29 772,-1223.55 772,-1214.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"775.5,-1214.53 772,-1204.53 768.5,-1214.53 775.5,-1214.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415322516688 -->\n",
       "<g class=\"node\" id=\"node273\"><title>140415322516688</title>\n",
       "<polygon fill=\"none\" points=\"321.5,-949.5 321.5,-985.5 476.5,-985.5 476.5,-949.5 321.5,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-963.8\">activation_83: Activation</text>\n",
       "</g>\n",
       "<!-- 140415322670672&#45;&gt;140415322516688 -->\n",
       "<g class=\"edge\" id=\"edge297\"><title>140415322670672-&gt;140415322516688</title>\n",
       "<path d=\"M356.556,-1022.31C363.366,-1013.68 371.745,-1003.06 379.255,-993.534\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"382.124,-995.548 385.569,-985.529 376.628,-991.213 382.124,-995.548\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321918480 -->\n",
       "<g class=\"node\" id=\"node274\"><title>140415321918480</title>\n",
       "<polygon fill=\"none\" points=\"549.5,-949.5 549.5,-985.5 704.5,-985.5 704.5,-949.5 549.5,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-963.8\">activation_84: Activation</text>\n",
       "</g>\n",
       "<!-- 140415322064336&#45;&gt;140415321918480 -->\n",
       "<g class=\"edge\" id=\"edge298\"><title>140415322064336-&gt;140415321918480</title>\n",
       "<path d=\"M627,-1022.31C627,-1014.29 627,-1004.55 627,-995.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"630.5,-995.529 627,-985.529 623.5,-995.529 630.5,-995.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321661136 -->\n",
       "<g class=\"node\" id=\"node275\"><title>140415321661136</title>\n",
       "<polygon fill=\"none\" points=\"800,-1387.5 800,-1423.5 1066,-1423.5 1066,-1387.5 800,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"933\" y=\"-1401.8\">batch_normalization_85: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415321661392&#45;&gt;140415321661136 -->\n",
       "<g class=\"edge\" id=\"edge299\"><title>140415321661392-&gt;140415321661136</title>\n",
       "<path d=\"M931.484,-1460.31C931.71,-1452.29 931.985,-1442.55 932.237,-1433.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"935.737,-1433.62 932.52,-1423.53 928.74,-1433.43 935.737,-1433.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415324622736 -->\n",
       "<g class=\"node\" id=\"node276\"><title>140415324622736</title>\n",
       "<polygon fill=\"none\" points=\"912.5,-876.5 912.5,-912.5 1067.5,-912.5 1067.5,-876.5 912.5,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"990\" y=\"-890.8\">activation_77: Activation</text>\n",
       "</g>\n",
       "<!-- 140415324996304&#45;&gt;140415324622736 -->\n",
       "<g class=\"edge\" id=\"edge300\"><title>140415324996304-&gt;140415324622736</title>\n",
       "<path d=\"M1081.66,-1022.42C1063.52,-997.3 1029.53,-950.234 1008.43,-921.014\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1011.01,-918.62 1002.32,-912.562 1005.34,-922.718 1011.01,-918.62\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323339024 -->\n",
       "<g class=\"node\" id=\"node277\"><title>140415323339024</title>\n",
       "<polygon fill=\"none\" points=\"699,-1095.5 699,-1131.5 845,-1131.5 845,-1095.5 699,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772\" y=\"-1109.8\">mixed9_0: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415323648912&#45;&gt;140415323339024 -->\n",
       "<g class=\"edge\" id=\"edge301\"><title>140415323648912-&gt;140415323339024</title>\n",
       "<path d=\"M613.061,-1168.49C642.829,-1158.38 680.774,-1145.49 712.123,-1134.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"713.281,-1138.14 721.623,-1131.61 711.029,-1131.52 713.281,-1138.14\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323510032&#45;&gt;140415323339024 -->\n",
       "<g class=\"edge\" id=\"edge302\"><title>140415323510032-&gt;140415323339024</title>\n",
       "<path d=\"M772,-1168.31C772,-1160.29 772,-1150.55 772,-1141.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"775.5,-1141.53 772,-1131.53 768.5,-1141.53 775.5,-1141.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321474320 -->\n",
       "<g class=\"node\" id=\"node278\"><title>140415321474320</title>\n",
       "<polygon fill=\"none\" points=\"543,-876.5 543,-912.5 711,-912.5 711,-876.5 543,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-890.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415322516688&#45;&gt;140415321474320 -->\n",
       "<g class=\"edge\" id=\"edge303\"><title>140415322516688-&gt;140415321474320</title>\n",
       "<path d=\"M453.612,-949.494C486.226,-939.338 527.838,-926.379 562.121,-915.704\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"563.537,-918.929 572.044,-912.614 561.455,-912.245 563.537,-918.929\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321918480&#45;&gt;140415321474320 -->\n",
       "<g class=\"edge\" id=\"edge304\"><title>140415321918480-&gt;140415321474320</title>\n",
       "<path d=\"M627,-949.313C627,-941.289 627,-931.547 627,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"630.5,-922.529 627,-912.529 623.5,-922.529 630.5,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321352080 -->\n",
       "<g class=\"node\" id=\"node279\"><title>140415321352080</title>\n",
       "<polygon fill=\"none\" points=\"842.5,-949.5 842.5,-985.5 997.5,-985.5 997.5,-949.5 842.5,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920\" y=\"-963.8\">activation_85: Activation</text>\n",
       "</g>\n",
       "<!-- 140415321661136&#45;&gt;140415321352080 -->\n",
       "<g class=\"edge\" id=\"edge305\"><title>140415321661136-&gt;140415321352080</title>\n",
       "<path d=\"M933,-1387.21C933,-1360.18 933,-1306.25 933,-1260.5 933,-1260.5 933,-1260.5 933,-1112.5 933,-1071.77 927.777,-1024.82 923.952,-995.895\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"927.387,-995.18 922.568,-985.744 920.451,-996.125 927.387,-995.18\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321278672 -->\n",
       "<g class=\"node\" id=\"node280\"><title>140415321278672</title>\n",
       "<polygon fill=\"none\" points=\"748,-803.5 748,-839.5 880,-839.5 880,-803.5 748,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"814\" y=\"-817.8\">mixed9: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415324622736&#45;&gt;140415321278672 -->\n",
       "<g class=\"edge\" id=\"edge306\"><title>140415324622736-&gt;140415321278672</title>\n",
       "<path d=\"M947.844,-876.494C923.208,-866.555 891.922,-853.934 865.793,-843.394\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"867.006,-840.109 856.422,-839.614 864.387,-846.601 867.006,-840.109\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415323339024&#45;&gt;140415321278672 -->\n",
       "<g class=\"edge\" id=\"edge307\"><title>140415323339024-&gt;140415321278672</title>\n",
       "<path d=\"M781.104,-1095.26C799.509,-1058.05 837.466,-967.763 814,-895.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M814,-893.5C809.28,-879.592 808.954,-863.251 809.953,-849.753\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"813.452,-849.942 810.981,-839.639 806.488,-849.234 813.452,-849.942\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321474320&#45;&gt;140415321278672 -->\n",
       "<g class=\"edge\" id=\"edge308\"><title>140415321474320-&gt;140415321278672</title>\n",
       "<path d=\"M671.791,-876.494C698.081,-866.512 731.5,-853.824 759.335,-843.255\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"760.82,-846.435 768.926,-839.614 758.335,-839.891 760.82,-846.435\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321352080&#45;&gt;140415321278672 -->\n",
       "<g class=\"edge\" id=\"edge309\"><title>140415321352080-&gt;140415321278672</title>\n",
       "<path d=\"M869.849,-949.494C846.611,-938.389 822.312,-921.096 814,-895.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319563216 -->\n",
       "<g class=\"node\" id=\"node281\"><title>140415319563216</title>\n",
       "<polygon fill=\"none\" points=\"314,-730.5 314,-766.5 444,-766.5 444,-730.5 314,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379\" y=\"-744.8\">conv2d_97: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415321278672&#45;&gt;140415319563216 -->\n",
       "<g class=\"edge\" id=\"edge310\"><title>140415321278672-&gt;140415319563216</title>\n",
       "<path d=\"M747.933,-809.717C668.99,-796.832 537.209,-775.323 454.367,-761.801\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"454.697,-758.309 444.264,-760.152 453.569,-765.217 454.697,-758.309\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320520912 -->\n",
       "<g class=\"node\" id=\"node284\"><title>140415320520912</title>\n",
       "<polygon fill=\"none\" points=\"641,-730.5 641,-766.5 771,-766.5 771,-730.5 641,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"706\" y=\"-744.8\">conv2d_94: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415321278672&#45;&gt;140415320520912 -->\n",
       "<g class=\"edge\" id=\"edge313\"><title>140415321278672-&gt;140415320520912</title>\n",
       "<path d=\"M788.131,-803.494C773.942,-794.166 756.158,-782.474 740.77,-772.358\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"742.311,-769.182 732.032,-766.614 738.465,-775.032 742.311,-769.182\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317584976 -->\n",
       "<g class=\"node\" id=\"node294\"><title>140415317584976</title>\n",
       "<polygon fill=\"none\" points=\"800.5,-730.5 800.5,-766.5 1045.5,-766.5 1045.5,-730.5 800.5,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923\" y=\"-744.8\">average_pooling2d_9: AveragePooling2D</text>\n",
       "</g>\n",
       "<!-- 140415321278672&#45;&gt;140415317584976 -->\n",
       "<g class=\"edge\" id=\"edge323\"><title>140415321278672-&gt;140415317584976</title>\n",
       "<path d=\"M840.108,-803.494C854.563,-794.079 872.713,-782.255 888.343,-772.075\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"890.258,-775.004 896.727,-766.614 886.438,-769.139 890.258,-775.004\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321451728 -->\n",
       "<g class=\"node\" id=\"node295\"><title>140415321451728</title>\n",
       "<polygon fill=\"none\" points=\"1022,-657.5 1022,-693.5 1152,-693.5 1152,-657.5 1022,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1087\" y=\"-671.8\">conv2d_93: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415321278672&#45;&gt;140415321451728 -->\n",
       "<g class=\"edge\" id=\"edge324\"><title>140415321278672-&gt;140415321451728</title>\n",
       "<path d=\"M880.008,-813.084C942.154,-804.85 1029.41,-789.658 1055,-767 1073.17,-750.913 1081.02,-723.846 1084.42,-703.495\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1087.89,-703.949 1085.82,-693.558 1080.96,-702.972 1087.89,-703.949\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319562576 -->\n",
       "<g class=\"node\" id=\"node282\"><title>140415319562576</title>\n",
       "<polygon fill=\"none\" points=\"176,-657.5 176,-693.5 442,-693.5 442,-657.5 176,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-671.8\">batch_normalization_90: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415319563216&#45;&gt;140415319562576 -->\n",
       "<g class=\"edge\" id=\"edge311\"><title>140415319563216-&gt;140415319562576</title>\n",
       "<path d=\"M362.055,-730.313C353.285,-721.417 342.432,-710.409 332.831,-700.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"335.302,-698.193 325.789,-693.529 330.317,-703.107 335.302,-698.193\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415318911312 -->\n",
       "<g class=\"node\" id=\"node283\"><title>140415318911312</title>\n",
       "<polygon fill=\"none\" points=\"176.5,-584.5 176.5,-620.5 331.5,-620.5 331.5,-584.5 176.5,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-598.8\">activation_90: Activation</text>\n",
       "</g>\n",
       "<!-- 140415319562576&#45;&gt;140415318911312 -->\n",
       "<g class=\"edge\" id=\"edge312\"><title>140415319562576-&gt;140415318911312</title>\n",
       "<path d=\"M295.686,-657.313C288.998,-648.679 280.768,-638.055 273.393,-628.534\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"276.082,-626.291 267.191,-620.529 270.548,-630.578 276.082,-626.291\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319109584 -->\n",
       "<g class=\"node\" id=\"node285\"><title>140415319109584</title>\n",
       "<polygon fill=\"none\" points=\"161,-511.5 161,-547.5 291,-547.5 291,-511.5 161,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"226\" y=\"-525.8\">conv2d_98: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415318911312&#45;&gt;140415319109584 -->\n",
       "<g class=\"edge\" id=\"edge314\"><title>140415318911312-&gt;140415319109584</title>\n",
       "<path d=\"M247.222,-584.313C243.989,-576.115 240.049,-566.123 236.445,-556.985\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"239.64,-555.548 232.716,-547.529 233.128,-558.116 239.64,-555.548\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320521360 -->\n",
       "<g class=\"node\" id=\"node286\"><title>140415320521360</title>\n",
       "<polygon fill=\"none\" points=\"568,-657.5 568,-693.5 834,-693.5 834,-657.5 568,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"701\" y=\"-671.8\">batch_normalization_87: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415320520912&#45;&gt;140415320521360 -->\n",
       "<g class=\"edge\" id=\"edge315\"><title>140415320520912-&gt;140415320521360</title>\n",
       "<path d=\"M704.79,-730.313C704.225,-722.289 703.539,-712.547 702.906,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"706.393,-703.258 702.199,-693.529 699.41,-703.75 706.393,-703.258\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319009424 -->\n",
       "<g class=\"node\" id=\"node287\"><title>140415319009424</title>\n",
       "<polygon fill=\"none\" points=\"65,-438.5 65,-474.5 331,-474.5 331,-438.5 65,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198\" y=\"-452.8\">batch_normalization_91: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415319109584&#45;&gt;140415319009424 -->\n",
       "<g class=\"edge\" id=\"edge316\"><title>140415319109584-&gt;140415319009424</title>\n",
       "<path d=\"M219.222,-511.313C215.989,-503.115 212.049,-493.123 208.445,-483.985\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"211.64,-482.548 204.716,-474.529 205.128,-485.116 211.64,-482.548\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320193872 -->\n",
       "<g class=\"node\" id=\"node288\"><title>140415320193872</title>\n",
       "<polygon fill=\"none\" points=\"620.5,-584.5 620.5,-620.5 775.5,-620.5 775.5,-584.5 620.5,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"698\" y=\"-598.8\">activation_87: Activation</text>\n",
       "</g>\n",
       "<!-- 140415320521360&#45;&gt;140415320193872 -->\n",
       "<g class=\"edge\" id=\"edge317\"><title>140415320521360-&gt;140415320193872</title>\n",
       "<path d=\"M700.274,-657.313C699.935,-649.289 699.523,-639.547 699.144,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"702.639,-630.372 698.72,-620.529 695.645,-630.668 702.639,-630.372\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415318518928 -->\n",
       "<g class=\"node\" id=\"node289\"><title>140415318518928</title>\n",
       "<polygon fill=\"none\" points=\"213.5,-365.5 213.5,-401.5 368.5,-401.5 368.5,-365.5 213.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"291\" y=\"-379.8\">activation_91: Activation</text>\n",
       "</g>\n",
       "<!-- 140415319009424&#45;&gt;140415318518928 -->\n",
       "<g class=\"edge\" id=\"edge318\"><title>140415319009424-&gt;140415318518928</title>\n",
       "<path d=\"M220.513,-438.313C232.507,-429.156 247.434,-417.76 260.46,-407.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"262.87,-410.379 268.695,-401.529 258.622,-404.815 262.87,-410.379\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320278096 -->\n",
       "<g class=\"node\" id=\"node290\"><title>140415320278096</title>\n",
       "<polygon fill=\"none\" points=\"468,-511.5 468,-547.5 598,-547.5 598,-511.5 468,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-525.8\">conv2d_95: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415320193872&#45;&gt;140415320278096 -->\n",
       "<g class=\"edge\" id=\"edge319\"><title>140415320193872-&gt;140415320278096</title>\n",
       "<path d=\"M658.478,-584.494C635.585,-574.643 606.566,-562.156 582.202,-551.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"583.34,-548.351 572.771,-547.614 580.573,-554.781 583.34,-548.351\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319726608 -->\n",
       "<g class=\"node\" id=\"node291\"><title>140415319726608</title>\n",
       "<polygon fill=\"none\" points=\"667,-511.5 667,-547.5 797,-547.5 797,-511.5 667,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"732\" y=\"-525.8\">conv2d_96: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415320193872&#45;&gt;140415319726608 -->\n",
       "<g class=\"edge\" id=\"edge320\"><title>140415320193872-&gt;140415319726608</title>\n",
       "<path d=\"M706.23,-584.313C710.198,-576.028 715.043,-565.91 719.457,-556.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"722.683,-558.06 723.845,-547.529 716.37,-555.036 722.683,-558.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415318797776 -->\n",
       "<g class=\"node\" id=\"node292\"><title>140415318797776</title>\n",
       "<polygon fill=\"none\" points=\"226,-292.5 226,-328.5 356,-328.5 356,-292.5 226,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"291\" y=\"-306.8\">conv2d_99: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415318518928&#45;&gt;140415318797776 -->\n",
       "<g class=\"edge\" id=\"edge321\"><title>140415318518928-&gt;140415318797776</title>\n",
       "<path d=\"M291,-365.313C291,-357.289 291,-347.547 291,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"294.5,-338.529 291,-328.529 287.5,-338.529 294.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317847376 -->\n",
       "<g class=\"node\" id=\"node293\"><title>140415317847376</title>\n",
       "<polygon fill=\"none\" points=\"440.5,-292.5 440.5,-328.5 577.5,-328.5 577.5,-292.5 440.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"509\" y=\"-306.8\">conv2d_100: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415318518928&#45;&gt;140415317847376 -->\n",
       "<g class=\"edge\" id=\"edge322\"><title>140415318518928-&gt;140415317847376</title>\n",
       "<path d=\"M343.216,-365.494C374.266,-355.381 413.846,-342.491 446.544,-331.841\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"448.03,-335.038 456.454,-328.614 445.862,-328.382 448.03,-335.038\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320279440 -->\n",
       "<g class=\"node\" id=\"node296\"><title>140415320279440</title>\n",
       "<polygon fill=\"none\" points=\"349,-438.5 349,-474.5 615,-474.5 615,-438.5 349,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482\" y=\"-452.8\">batch_normalization_88: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415320278096&#45;&gt;140415320279440 -->\n",
       "<g class=\"edge\" id=\"edge325\"><title>140415320278096-&gt;140415320279440</title>\n",
       "<path d=\"M520.654,-511.313C514.515,-502.766 506.975,-492.269 500.19,-482.823\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"502.909,-480.609 494.232,-474.529 497.223,-484.693 502.909,-480.609\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319639888 -->\n",
       "<g class=\"node\" id=\"node297\"><title>140415319639888</title>\n",
       "<polygon fill=\"none\" points=\"633,-438.5 633,-474.5 899,-474.5 899,-438.5 633,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-452.8\">batch_normalization_89: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415319726608&#45;&gt;140415319639888 -->\n",
       "<g class=\"edge\" id=\"edge326\"><title>140415319726608-&gt;140415319639888</title>\n",
       "<path d=\"M740.23,-511.313C744.198,-503.028 749.043,-492.91 753.457,-483.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"756.683,-485.06 757.845,-474.529 750.37,-482.036 756.683,-485.06\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415318797712 -->\n",
       "<g class=\"node\" id=\"node298\"><title>140415318797712</title>\n",
       "<polygon fill=\"none\" points=\"158,-219.5 158,-255.5 424,-255.5 424,-219.5 158,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"291\" y=\"-233.8\">batch_normalization_92: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415318797776&#45;&gt;140415318797712 -->\n",
       "<g class=\"edge\" id=\"edge327\"><title>140415318797776-&gt;140415318797712</title>\n",
       "<path d=\"M291,-292.313C291,-284.289 291,-274.547 291,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"294.5,-265.529 291,-255.529 287.5,-265.529 294.5,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317904400 -->\n",
       "<g class=\"node\" id=\"node299\"><title>140415317904400</title>\n",
       "<polygon fill=\"none\" points=\"442,-219.5 442,-255.5 708,-255.5 708,-219.5 442,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575\" y=\"-233.8\">batch_normalization_93: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415317847376&#45;&gt;140415317904400 -->\n",
       "<g class=\"edge\" id=\"edge328\"><title>140415317847376-&gt;140415317904400</title>\n",
       "<path d=\"M524.977,-292.313C533.165,-283.505 543.278,-272.625 552.264,-262.958\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"554.925,-265.236 559.17,-255.529 549.798,-260.47 554.925,-265.236\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317321552 -->\n",
       "<g class=\"node\" id=\"node300\"><title>140415317321552</title>\n",
       "<polygon fill=\"none\" points=\"856.5,-657.5 856.5,-693.5 993.5,-693.5 993.5,-657.5 856.5,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-671.8\">conv2d_101: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415317584976&#45;&gt;140415317321552 -->\n",
       "<g class=\"edge\" id=\"edge329\"><title>140415317584976-&gt;140415317321552</title>\n",
       "<path d=\"M923.484,-730.313C923.71,-722.289 923.985,-712.547 924.237,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"927.737,-703.623 924.52,-693.529 920.74,-703.426 927.737,-703.623\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415321451920 -->\n",
       "<g class=\"node\" id=\"node301\"><title>140415321451920</title>\n",
       "<polygon fill=\"none\" points=\"955,-219.5 955,-255.5 1221,-255.5 1221,-219.5 955,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1088\" y=\"-233.8\">batch_normalization_86: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415321451728&#45;&gt;140415321451920 -->\n",
       "<g class=\"edge\" id=\"edge330\"><title>140415321451728-&gt;140415321451920</title>\n",
       "<path d=\"M1087.2,-657.211C1087.49,-630.177 1088,-576.251 1088,-530.5 1088,-530.5 1088,-530.5 1088,-382.5 1088,-341.933 1088,-294.937 1088,-265.961\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1091.5,-265.789 1088,-255.789 1084.5,-265.789 1091.5,-265.789\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320088144 -->\n",
       "<g class=\"node\" id=\"node302\"><title>140415320088144</title>\n",
       "<polygon fill=\"none\" points=\"435.5,-365.5 435.5,-401.5 590.5,-401.5 590.5,-365.5 435.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"513\" y=\"-379.8\">activation_88: Activation</text>\n",
       "</g>\n",
       "<!-- 140415320279440&#45;&gt;140415320088144 -->\n",
       "<g class=\"edge\" id=\"edge331\"><title>140415320279440-&gt;140415320088144</title>\n",
       "<path d=\"M489.504,-438.313C493.084,-430.115 497.446,-420.123 501.436,-410.985\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"504.771,-412.094 505.565,-401.529 498.356,-409.293 504.771,-412.094\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319477648 -->\n",
       "<g class=\"node\" id=\"node303\"><title>140415319477648</title>\n",
       "<polygon fill=\"none\" points=\"688.5,-365.5 688.5,-401.5 843.5,-401.5 843.5,-365.5 688.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-379.8\">activation_89: Activation</text>\n",
       "</g>\n",
       "<!-- 140415319639888&#45;&gt;140415319477648 -->\n",
       "<g class=\"edge\" id=\"edge332\"><title>140415319639888-&gt;140415319477648</title>\n",
       "<path d=\"M766,-438.313C766,-430.289 766,-420.547 766,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"769.5,-411.529 766,-401.529 762.5,-411.529 769.5,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415318188304 -->\n",
       "<g class=\"node\" id=\"node304\"><title>140415318188304</title>\n",
       "<polygon fill=\"none\" points=\"268.5,-146.5 268.5,-182.5 423.5,-182.5 423.5,-146.5 268.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-160.8\">activation_92: Activation</text>\n",
       "</g>\n",
       "<!-- 140415318797712&#45;&gt;140415318188304 -->\n",
       "<g class=\"edge\" id=\"edge333\"><title>140415318797712-&gt;140415318188304</title>\n",
       "<path d=\"M304.314,-219.313C311.002,-210.679 319.232,-200.055 326.607,-190.534\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"329.452,-192.578 332.809,-182.529 323.918,-188.291 329.452,-192.578\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317762640 -->\n",
       "<g class=\"node\" id=\"node305\"><title>140415317762640</title>\n",
       "<polygon fill=\"none\" points=\"497.5,-146.5 497.5,-182.5 652.5,-182.5 652.5,-146.5 497.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575\" y=\"-160.8\">activation_93: Activation</text>\n",
       "</g>\n",
       "<!-- 140415317904400&#45;&gt;140415317762640 -->\n",
       "<g class=\"edge\" id=\"edge334\"><title>140415317904400-&gt;140415317762640</title>\n",
       "<path d=\"M575,-219.313C575,-211.289 575,-201.547 575,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578.5,-192.529 575,-182.529 571.5,-192.529 578.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317321296 -->\n",
       "<g class=\"node\" id=\"node306\"><title>140415317321296</title>\n",
       "<polygon fill=\"none\" points=\"794,-584.5 794,-620.5 1060,-620.5 1060,-584.5 794,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"927\" y=\"-598.8\">batch_normalization_94: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140415317321552&#45;&gt;140415317321296 -->\n",
       "<g class=\"edge\" id=\"edge335\"><title>140415317321552-&gt;140415317321296</title>\n",
       "<path d=\"M925.484,-657.313C925.71,-649.289 925.985,-639.547 926.237,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"929.737,-630.623 926.52,-620.529 922.74,-630.426 929.737,-630.623\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415320712784 -->\n",
       "<g class=\"node\" id=\"node307\"><title>140415320712784</title>\n",
       "<polygon fill=\"none\" points=\"874.5,-73.5 874.5,-109.5 1029.5,-109.5 1029.5,-73.5 874.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"952\" y=\"-87.8\">activation_86: Activation</text>\n",
       "</g>\n",
       "<!-- 140415321451920&#45;&gt;140415320712784 -->\n",
       "<g class=\"edge\" id=\"edge336\"><title>140415321451920-&gt;140415320712784</title>\n",
       "<path d=\"M1071.86,-219.416C1047.84,-193.975 1002.54,-146.013 975.034,-116.889\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"977.525,-114.429 968.114,-109.562 972.436,-119.235 977.525,-114.429\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319561872 -->\n",
       "<g class=\"node\" id=\"node308\"><title>140415319561872</title>\n",
       "<polygon fill=\"none\" points=\"693,-292.5 693,-328.5 839,-328.5 839,-292.5 693,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-306.8\">mixed9_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415320088144&#45;&gt;140415319561872 -->\n",
       "<g class=\"edge\" id=\"edge337\"><title>140415320088144-&gt;140415319561872</title>\n",
       "<path d=\"M573.6,-365.494C610.101,-355.25 656.758,-342.157 694.985,-331.429\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"696.335,-334.685 705.018,-328.614 694.444,-327.946 696.335,-334.685\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319477648&#45;&gt;140415319561872 -->\n",
       "<g class=\"edge\" id=\"edge338\"><title>140415319477648-&gt;140415319561872</title>\n",
       "<path d=\"M766,-365.313C766,-357.289 766,-347.547 766,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"769.5,-338.529 766,-328.529 762.5,-338.529 769.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317583440 -->\n",
       "<g class=\"node\" id=\"node309\"><title>140415317583440</title>\n",
       "<polygon fill=\"none\" points=\"491,-73.5 491,-109.5 659,-109.5 659,-73.5 491,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575\" y=\"-87.8\">concatenate_2: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415318188304&#45;&gt;140415317583440 -->\n",
       "<g class=\"edge\" id=\"edge339\"><title>140415318188304-&gt;140415317583440</title>\n",
       "<path d=\"M400.851,-146.494C433.609,-136.338 475.403,-123.379 509.837,-112.704\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"511.288,-115.918 519.803,-109.614 509.215,-109.232 511.288,-115.918\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317762640&#45;&gt;140415317583440 -->\n",
       "<g class=\"edge\" id=\"edge340\"><title>140415317762640-&gt;140415317583440</title>\n",
       "<path d=\"M575,-146.313C575,-138.289 575,-128.547 575,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578.5,-119.529 575,-109.529 571.5,-119.529 578.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317045328 -->\n",
       "<g class=\"node\" id=\"node310\"><title>140415317045328</title>\n",
       "<polygon fill=\"none\" points=\"794.5,-146.5 794.5,-182.5 949.5,-182.5 949.5,-146.5 794.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872\" y=\"-160.8\">activation_94: Activation</text>\n",
       "</g>\n",
       "<!-- 140415317321296&#45;&gt;140415317045328 -->\n",
       "<g class=\"edge\" id=\"edge341\"><title>140415317321296-&gt;140415317045328</title>\n",
       "<path d=\"M927,-584.211C927,-557.176 927,-503.25 927,-457.5 927,-457.5 927,-457.5 927,-309.5 927,-265.981 904.807,-219.646 888.616,-191.632\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"891.519,-189.666 883.396,-182.864 885.505,-193.247 891.519,-189.666\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317132368 -->\n",
       "<g class=\"node\" id=\"node311\"><title>140415317132368</title>\n",
       "<polygon fill=\"none\" points=\"696.5,-0.5 696.5,-36.5 835.5,-36.5 835.5,-0.5 696.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-14.8\">mixed10: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140415320712784&#45;&gt;140415317132368 -->\n",
       "<g class=\"edge\" id=\"edge342\"><title>140415320712784-&gt;140415317132368</title>\n",
       "<path d=\"M907.448,-73.4937C881.299,-63.5119 848.059,-50.8236 820.373,-40.2554\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"821.423,-36.91 810.833,-36.6136 818.927,-43.4497 821.423,-36.91\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415319561872&#45;&gt;140415317132368 -->\n",
       "<g class=\"edge\" id=\"edge343\"><title>140415319561872-&gt;140415317132368</title>\n",
       "<path d=\"M769.496,-292.34C776.376,-254.918 789.097,-163.626 766,-92.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<path d=\"M766,-90.5C761.075,-76.6635 760.735,-60.3267 761.778,-46.8143\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"765.278,-46.9977 762.85,-36.6848 758.317,-46.2608 765.278,-46.9977\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317583440&#45;&gt;140415317132368 -->\n",
       "<g class=\"edge\" id=\"edge344\"><title>140415317583440-&gt;140415317132368</title>\n",
       "<path d=\"M620.749,-73.4937C647.719,-63.4683 682.033,-50.7127 710.537,-40.117\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"711.808,-43.3786 719.962,-36.6136 709.369,-36.8173 711.808,-43.3786\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415317045328&#45;&gt;140415317132368 -->\n",
       "<g class=\"edge\" id=\"edge345\"><title>140415317045328-&gt;140415317132368</title>\n",
       "<path d=\"M821.849,-146.494C798.611,-135.389 774.312,-118.096 766,-92.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "SVG(model_to_dot(inception_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'input_5')\n",
      "(1, 'conv2d_8')\n",
      "(2, 'batch_normalization_1')\n",
      "(3, 'activation_1')\n",
      "(4, 'conv2d_9')\n",
      "(5, 'batch_normalization_2')\n",
      "(6, 'activation_2')\n",
      "(7, 'conv2d_10')\n",
      "(8, 'batch_normalization_3')\n",
      "(9, 'activation_3')\n",
      "(10, 'max_pooling2d_4')\n",
      "(11, 'conv2d_11')\n",
      "(12, 'batch_normalization_4')\n",
      "(13, 'activation_4')\n",
      "(14, 'conv2d_12')\n",
      "(15, 'batch_normalization_5')\n",
      "(16, 'activation_5')\n",
      "(17, 'max_pooling2d_5')\n",
      "(18, 'conv2d_16')\n",
      "(19, 'batch_normalization_9')\n",
      "(20, 'activation_9')\n",
      "(21, 'conv2d_14')\n",
      "(22, 'conv2d_17')\n",
      "(23, 'batch_normalization_7')\n",
      "(24, 'batch_normalization_10')\n",
      "(25, 'activation_7')\n",
      "(26, 'activation_10')\n",
      "(27, 'average_pooling2d_1')\n",
      "(28, 'conv2d_13')\n",
      "(29, 'conv2d_15')\n",
      "(30, 'conv2d_18')\n",
      "(31, 'conv2d_19')\n",
      "(32, 'batch_normalization_6')\n",
      "(33, 'batch_normalization_8')\n",
      "(34, 'batch_normalization_11')\n",
      "(35, 'batch_normalization_12')\n",
      "(36, 'activation_6')\n",
      "(37, 'activation_8')\n",
      "(38, 'activation_11')\n",
      "(39, 'activation_12')\n",
      "(40, 'mixed0')\n",
      "(41, 'conv2d_23')\n",
      "(42, 'batch_normalization_16')\n",
      "(43, 'activation_16')\n",
      "(44, 'conv2d_21')\n",
      "(45, 'conv2d_24')\n",
      "(46, 'batch_normalization_14')\n",
      "(47, 'batch_normalization_17')\n",
      "(48, 'activation_14')\n",
      "(49, 'activation_17')\n",
      "(50, 'average_pooling2d_2')\n",
      "(51, 'conv2d_20')\n",
      "(52, 'conv2d_22')\n",
      "(53, 'conv2d_25')\n",
      "(54, 'conv2d_26')\n",
      "(55, 'batch_normalization_13')\n",
      "(56, 'batch_normalization_15')\n",
      "(57, 'batch_normalization_18')\n",
      "(58, 'batch_normalization_19')\n",
      "(59, 'activation_13')\n",
      "(60, 'activation_15')\n",
      "(61, 'activation_18')\n",
      "(62, 'activation_19')\n",
      "(63, 'mixed1')\n",
      "(64, 'conv2d_30')\n",
      "(65, 'batch_normalization_23')\n",
      "(66, 'activation_23')\n",
      "(67, 'conv2d_28')\n",
      "(68, 'conv2d_31')\n",
      "(69, 'batch_normalization_21')\n",
      "(70, 'batch_normalization_24')\n",
      "(71, 'activation_21')\n",
      "(72, 'activation_24')\n",
      "(73, 'average_pooling2d_3')\n",
      "(74, 'conv2d_27')\n",
      "(75, 'conv2d_29')\n",
      "(76, 'conv2d_32')\n",
      "(77, 'conv2d_33')\n",
      "(78, 'batch_normalization_20')\n",
      "(79, 'batch_normalization_22')\n",
      "(80, 'batch_normalization_25')\n",
      "(81, 'batch_normalization_26')\n",
      "(82, 'activation_20')\n",
      "(83, 'activation_22')\n",
      "(84, 'activation_25')\n",
      "(85, 'activation_26')\n",
      "(86, 'mixed2')\n",
      "(87, 'conv2d_35')\n",
      "(88, 'batch_normalization_28')\n",
      "(89, 'activation_28')\n",
      "(90, 'conv2d_36')\n",
      "(91, 'batch_normalization_29')\n",
      "(92, 'activation_29')\n",
      "(93, 'conv2d_34')\n",
      "(94, 'conv2d_37')\n",
      "(95, 'batch_normalization_27')\n",
      "(96, 'batch_normalization_30')\n",
      "(97, 'activation_27')\n",
      "(98, 'activation_30')\n",
      "(99, 'max_pooling2d_6')\n",
      "(100, 'mixed3')\n",
      "(101, 'conv2d_42')\n",
      "(102, 'batch_normalization_35')\n",
      "(103, 'activation_35')\n",
      "(104, 'conv2d_43')\n",
      "(105, 'batch_normalization_36')\n",
      "(106, 'activation_36')\n",
      "(107, 'conv2d_39')\n",
      "(108, 'conv2d_44')\n",
      "(109, 'batch_normalization_32')\n",
      "(110, 'batch_normalization_37')\n",
      "(111, 'activation_32')\n",
      "(112, 'activation_37')\n",
      "(113, 'conv2d_40')\n",
      "(114, 'conv2d_45')\n",
      "(115, 'batch_normalization_33')\n",
      "(116, 'batch_normalization_38')\n",
      "(117, 'activation_33')\n",
      "(118, 'activation_38')\n",
      "(119, 'average_pooling2d_4')\n",
      "(120, 'conv2d_38')\n",
      "(121, 'conv2d_41')\n",
      "(122, 'conv2d_46')\n",
      "(123, 'conv2d_47')\n",
      "(124, 'batch_normalization_31')\n",
      "(125, 'batch_normalization_34')\n",
      "(126, 'batch_normalization_39')\n",
      "(127, 'batch_normalization_40')\n",
      "(128, 'activation_31')\n",
      "(129, 'activation_34')\n",
      "(130, 'activation_39')\n",
      "(131, 'activation_40')\n",
      "(132, 'mixed4')\n",
      "(133, 'conv2d_52')\n",
      "(134, 'batch_normalization_45')\n",
      "(135, 'activation_45')\n",
      "(136, 'conv2d_53')\n",
      "(137, 'batch_normalization_46')\n",
      "(138, 'activation_46')\n",
      "(139, 'conv2d_49')\n",
      "(140, 'conv2d_54')\n",
      "(141, 'batch_normalization_42')\n",
      "(142, 'batch_normalization_47')\n",
      "(143, 'activation_42')\n",
      "(144, 'activation_47')\n",
      "(145, 'conv2d_50')\n",
      "(146, 'conv2d_55')\n",
      "(147, 'batch_normalization_43')\n",
      "(148, 'batch_normalization_48')\n",
      "(149, 'activation_43')\n",
      "(150, 'activation_48')\n",
      "(151, 'average_pooling2d_5')\n",
      "(152, 'conv2d_48')\n",
      "(153, 'conv2d_51')\n",
      "(154, 'conv2d_56')\n",
      "(155, 'conv2d_57')\n",
      "(156, 'batch_normalization_41')\n",
      "(157, 'batch_normalization_44')\n",
      "(158, 'batch_normalization_49')\n",
      "(159, 'batch_normalization_50')\n",
      "(160, 'activation_41')\n",
      "(161, 'activation_44')\n",
      "(162, 'activation_49')\n",
      "(163, 'activation_50')\n",
      "(164, 'mixed5')\n",
      "(165, 'conv2d_62')\n",
      "(166, 'batch_normalization_55')\n",
      "(167, 'activation_55')\n",
      "(168, 'conv2d_63')\n",
      "(169, 'batch_normalization_56')\n",
      "(170, 'activation_56')\n",
      "(171, 'conv2d_59')\n",
      "(172, 'conv2d_64')\n",
      "(173, 'batch_normalization_52')\n",
      "(174, 'batch_normalization_57')\n",
      "(175, 'activation_52')\n",
      "(176, 'activation_57')\n",
      "(177, 'conv2d_60')\n",
      "(178, 'conv2d_65')\n",
      "(179, 'batch_normalization_53')\n",
      "(180, 'batch_normalization_58')\n",
      "(181, 'activation_53')\n",
      "(182, 'activation_58')\n",
      "(183, 'average_pooling2d_6')\n",
      "(184, 'conv2d_58')\n",
      "(185, 'conv2d_61')\n",
      "(186, 'conv2d_66')\n",
      "(187, 'conv2d_67')\n",
      "(188, 'batch_normalization_51')\n",
      "(189, 'batch_normalization_54')\n",
      "(190, 'batch_normalization_59')\n",
      "(191, 'batch_normalization_60')\n",
      "(192, 'activation_51')\n",
      "(193, 'activation_54')\n",
      "(194, 'activation_59')\n",
      "(195, 'activation_60')\n",
      "(196, 'mixed6')\n",
      "(197, 'conv2d_72')\n",
      "(198, 'batch_normalization_65')\n",
      "(199, 'activation_65')\n",
      "(200, 'conv2d_73')\n",
      "(201, 'batch_normalization_66')\n",
      "(202, 'activation_66')\n",
      "(203, 'conv2d_69')\n",
      "(204, 'conv2d_74')\n",
      "(205, 'batch_normalization_62')\n",
      "(206, 'batch_normalization_67')\n",
      "(207, 'activation_62')\n",
      "(208, 'activation_67')\n",
      "(209, 'conv2d_70')\n",
      "(210, 'conv2d_75')\n",
      "(211, 'batch_normalization_63')\n",
      "(212, 'batch_normalization_68')\n",
      "(213, 'activation_63')\n",
      "(214, 'activation_68')\n",
      "(215, 'average_pooling2d_7')\n",
      "(216, 'conv2d_68')\n",
      "(217, 'conv2d_71')\n",
      "(218, 'conv2d_76')\n",
      "(219, 'conv2d_77')\n",
      "(220, 'batch_normalization_61')\n",
      "(221, 'batch_normalization_64')\n",
      "(222, 'batch_normalization_69')\n",
      "(223, 'batch_normalization_70')\n",
      "(224, 'activation_61')\n",
      "(225, 'activation_64')\n",
      "(226, 'activation_69')\n",
      "(227, 'activation_70')\n",
      "(228, 'mixed7')\n",
      "(229, 'conv2d_80')\n",
      "(230, 'batch_normalization_73')\n",
      "(231, 'activation_73')\n",
      "(232, 'conv2d_81')\n",
      "(233, 'batch_normalization_74')\n",
      "(234, 'activation_74')\n",
      "(235, 'conv2d_78')\n",
      "(236, 'conv2d_82')\n",
      "(237, 'batch_normalization_71')\n",
      "(238, 'batch_normalization_75')\n",
      "(239, 'activation_71')\n",
      "(240, 'activation_75')\n",
      "(241, 'conv2d_79')\n",
      "(242, 'conv2d_83')\n",
      "(243, 'batch_normalization_72')\n",
      "(244, 'batch_normalization_76')\n",
      "(245, 'activation_72')\n",
      "(246, 'activation_76')\n",
      "(247, 'max_pooling2d_7')\n",
      "(248, 'mixed8')\n",
      "(249, 'conv2d_88')\n",
      "(250, 'batch_normalization_81')\n",
      "(251, 'activation_81')\n",
      "(252, 'conv2d_85')\n",
      "(253, 'conv2d_89')\n",
      "(254, 'batch_normalization_78')\n",
      "(255, 'batch_normalization_82')\n",
      "(256, 'activation_78')\n",
      "(257, 'activation_82')\n",
      "(258, 'conv2d_86')\n",
      "(259, 'conv2d_87')\n",
      "(260, 'conv2d_90')\n",
      "(261, 'conv2d_91')\n",
      "(262, 'average_pooling2d_8')\n",
      "(263, 'conv2d_84')\n",
      "(264, 'batch_normalization_79')\n",
      "(265, 'batch_normalization_80')\n",
      "(266, 'batch_normalization_83')\n",
      "(267, 'batch_normalization_84')\n",
      "(268, 'conv2d_92')\n",
      "(269, 'batch_normalization_77')\n",
      "(270, 'activation_79')\n",
      "(271, 'activation_80')\n",
      "(272, 'activation_83')\n",
      "(273, 'activation_84')\n",
      "(274, 'batch_normalization_85')\n",
      "(275, 'activation_77')\n",
      "(276, 'mixed9_0')\n",
      "(277, 'concatenate_1')\n",
      "(278, 'activation_85')\n",
      "(279, 'mixed9')\n",
      "(280, 'conv2d_97')\n",
      "(281, 'batch_normalization_90')\n",
      "(282, 'activation_90')\n",
      "(283, 'conv2d_94')\n",
      "(284, 'conv2d_98')\n",
      "(285, 'batch_normalization_87')\n",
      "(286, 'batch_normalization_91')\n",
      "(287, 'activation_87')\n",
      "(288, 'activation_91')\n",
      "(289, 'conv2d_95')\n",
      "(290, 'conv2d_96')\n",
      "(291, 'conv2d_99')\n",
      "(292, 'conv2d_100')\n",
      "(293, 'average_pooling2d_9')\n",
      "(294, 'conv2d_93')\n",
      "(295, 'batch_normalization_88')\n",
      "(296, 'batch_normalization_89')\n",
      "(297, 'batch_normalization_92')\n",
      "(298, 'batch_normalization_93')\n",
      "(299, 'conv2d_101')\n",
      "(300, 'batch_normalization_86')\n",
      "(301, 'activation_88')\n",
      "(302, 'activation_89')\n",
      "(303, 'activation_92')\n",
      "(304, 'activation_93')\n",
      "(305, 'batch_normalization_94')\n",
      "(306, 'activation_86')\n",
      "(307, 'mixed9_1')\n",
      "(308, 'concatenate_2')\n",
      "(309, 'activation_94')\n",
      "(310, 'mixed10')\n"
     ]
    }
   ],
   "source": [
    "# check the layers by name\n",
    "for i,layer in enumerate(inception_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 3 864         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 6 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 8 5120        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 1 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 6 76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 3 6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 3 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 3 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 6 192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 6 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 9 55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 4 144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 4 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 6 76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 9 82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 6 192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 9 288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 6 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 9 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 6 192         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 9 55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 4 144         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 4 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 6 76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 9 82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 6 192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 9 288         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 6 192         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 6 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 9 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 6 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 6 192         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 6 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 9 55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 9 82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 3 1152        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 9 288         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 1 384         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 1 384         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 1 576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 480         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 1 576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 1 179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 1 179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 480         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 480         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 1 215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 1 215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 1 576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 1 258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 1 258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 1 576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 1 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 3 552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 1 331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 3 960         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 1 576         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 3 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 4 1344        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 3 1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 3 960         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 3 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 4 1344        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, None, 4 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, None, 3 1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, None, 3 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 3 1152        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 3 1152        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, None, 1 393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 3 960         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, None, 3 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, None, 3 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 1 576         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"994pt\" viewBox=\"0.00 0.00 215.00 994.00\" width=\"215pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 990)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-990 211,-990 211,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140416251690704 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140416251690704</title>\n",
       "<polygon fill=\"none\" points=\"41,-949.5 41,-985.5 166,-985.5 166,-949.5 41,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-963.8\">input_4: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140416251690512 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140416251690512</title>\n",
       "<polygon fill=\"none\" points=\"42,-876.5 42,-912.5 165,-912.5 165,-876.5 42,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-890.8\">conv2d_1: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416251690704&#45;&gt;140416251690512 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140416251690704-&gt;140416251690512</title>\n",
       "<path d=\"M103.5,-949.313C103.5,-941.289 103.5,-931.547 103.5,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-922.529 103.5,-912.529 100,-922.529 107,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251690640 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140416251690640</title>\n",
       "<polygon fill=\"none\" points=\"0,-803.5 0,-839.5 207,-839.5 207,-803.5 0,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-817.8\">max_pooling2d_1: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140416251690512&#45;&gt;140416251690640 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140416251690512-&gt;140416251690640</title>\n",
       "<path d=\"M103.5,-876.313C103.5,-868.289 103.5,-858.547 103.5,-849.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-849.529 103.5,-839.529 100,-849.529 107,-849.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251690768 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140416251690768</title>\n",
       "<polygon fill=\"none\" points=\"42,-730.5 42,-766.5 165,-766.5 165,-730.5 42,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-744.8\">conv2d_2: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416251690640&#45;&gt;140416251690768 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140416251690640-&gt;140416251690768</title>\n",
       "<path d=\"M103.5,-803.313C103.5,-795.289 103.5,-785.547 103.5,-776.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-776.529 103.5,-766.529 100,-776.529 107,-776.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140415901072016 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140415901072016</title>\n",
       "<polygon fill=\"none\" points=\"0,-657.5 0,-693.5 207,-693.5 207,-657.5 0,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-671.8\">max_pooling2d_2: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140416251690768&#45;&gt;140415901072016 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140416251690768-&gt;140415901072016</title>\n",
       "<path d=\"M103.5,-730.313C103.5,-722.289 103.5,-712.547 103.5,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-703.529 103.5,-693.529 100,-703.529 107,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251797904 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140416251797904</title>\n",
       "<polygon fill=\"none\" points=\"42,-584.5 42,-620.5 165,-620.5 165,-584.5 42,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-598.8\">conv2d_3: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140415901072016&#45;&gt;140416251797904 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140415901072016-&gt;140416251797904</title>\n",
       "<path d=\"M103.5,-657.313C103.5,-649.289 103.5,-639.547 103.5,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-630.529 103.5,-620.529 100,-630.529 107,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251379536 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140416251379536</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 207,-547.5 207,-511.5 0,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-525.8\">max_pooling2d_3: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 140416251797904&#45;&gt;140416251379536 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140416251797904-&gt;140416251379536</title>\n",
       "<path d=\"M103.5,-584.313C103.5,-576.289 103.5,-566.547 103.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-557.529 103.5,-547.529 100,-557.529 107,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251379408 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140416251379408</title>\n",
       "<polygon fill=\"none\" points=\"42,-438.5 42,-474.5 165,-474.5 165,-438.5 42,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-452.8\">conv2d_4: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416251379536&#45;&gt;140416251379408 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140416251379536-&gt;140416251379408</title>\n",
       "<path d=\"M103.5,-511.313C103.5,-503.289 103.5,-493.547 103.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-484.529 103.5,-474.529 100,-484.529 107,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251110608 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140416251110608</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 207,-401.5 207,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-379.8\">up_sampling2d_1: UpSampling2D</text>\n",
       "</g>\n",
       "<!-- 140416251379408&#45;&gt;140416251110608 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140416251379408-&gt;140416251110608</title>\n",
       "<path d=\"M103.5,-438.313C103.5,-430.289 103.5,-420.547 103.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-411.529 103.5,-401.529 100,-411.529 107,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251523472 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140416251523472</title>\n",
       "<polygon fill=\"none\" points=\"42,-292.5 42,-328.5 165,-328.5 165,-292.5 42,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-306.8\">conv2d_5: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416251110608&#45;&gt;140416251523472 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140416251110608-&gt;140416251523472</title>\n",
       "<path d=\"M103.5,-365.313C103.5,-357.289 103.5,-347.547 103.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-338.529 103.5,-328.529 100,-338.529 107,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416250837392 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140416250837392</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 207,-255.5 207,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-233.8\">up_sampling2d_2: UpSampling2D</text>\n",
       "</g>\n",
       "<!-- 140416251523472&#45;&gt;140416250837392 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140416251523472-&gt;140416250837392</title>\n",
       "<path d=\"M103.5,-292.313C103.5,-284.289 103.5,-274.547 103.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-265.529 103.5,-255.529 100,-265.529 107,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251111376 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140416251111376</title>\n",
       "<polygon fill=\"none\" points=\"42,-146.5 42,-182.5 165,-182.5 165,-146.5 42,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-160.8\">conv2d_6: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416250837392&#45;&gt;140416251111376 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140416250837392-&gt;140416251111376</title>\n",
       "<path d=\"M103.5,-219.313C103.5,-211.289 103.5,-201.547 103.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-192.529 103.5,-182.529 100,-192.529 107,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416250963088 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140416250963088</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 207,-109.5 207,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-87.8\">up_sampling2d_3: UpSampling2D</text>\n",
       "</g>\n",
       "<!-- 140416251111376&#45;&gt;140416250963088 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140416251111376-&gt;140416250963088</title>\n",
       "<path d=\"M103.5,-146.313C103.5,-138.289 103.5,-128.547 103.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-119.529 103.5,-109.529 100,-119.529 107,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140416251283600 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140416251283600</title>\n",
       "<polygon fill=\"none\" points=\"42,-0.5 42,-36.5 165,-36.5 165,-0.5 42,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-14.8\">conv2d_7: Conv2D</text>\n",
       "</g>\n",
       "<!-- 140416250963088&#45;&gt;140416251283600 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140416250963088-&gt;140416251283600</title>\n",
       "<path d=\"M103.5,-73.3129C103.5,-65.2895 103.5,-55.5475 103.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"107,-46.5288 103.5,-36.5288 100,-46.5289 107,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(autoencoder).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
